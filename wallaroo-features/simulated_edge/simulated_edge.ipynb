{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/wallaroo-features/simulated_edge).\n",
    "\n",
    "# Simulated Edge Demo\n",
    "\n",
    "This notebook will explore \"Edge ML\", meaning deploying a model intended to be run on \"the edge\". What is \"the edge\"?  This is typically defined as a resource (CPU, memory, and/or bandwidth) constrained environment or where a combination of latency requirements and bandwidth available requires the models to run locally.\n",
    "\n",
    "Wallaroo provides two key capabilities when it comes to deploying models to edge devices:\n",
    "\n",
    "1. Since the same engine is used in both environments, the model behavior can often be simulated accurately using Wallaroo in a data center for testing prior to deployment.\n",
    "2. Wallaroo makes edge deployments \"observable\" so the same tools used to monitor model performance can be used in both kinds of deployments. \n",
    "\n",
    "This notebook closely parallels the [Aloha tutorial](https://docs.wallaroo.ai/wallaroo-tutorials/wallaroo-quick-start-aloha/).  The primary difference is instead of provide ample resources to a pipeline to allow high-throughput operation we will specify a resource budget matching what is expected in the final deployment. Then we can apply the expected load to the model and observe how it behaves given the available resources.\n",
    "\n",
    "This example uses the open source [Aloha CNN LSTM model](https://www.researchgate.net/publication/348920204_Using_Auxiliary_Inputs_in_Deep_Learning_Models_for_Detecting_DGA-based_Domain_Names) for classifying Domain names as being either legitimate or being used for nefarious purposes such as malware distribution. This could be deployed on a network router to detect suspicious domains in real-time. Of course, it is important to monitor the behavior of the model across all of the deployments so we can see if the detect rate starts to drift over time.\n",
    "\n",
    "Note that this example is not intended for production use and is meant of an example of running Wallaroo in a restrained environment.  The environment is based on the [Wallaroo AWS EC2 Setup guide](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-install-guides/YOUR SUFFIX-install-guides/wallaroo-setup-environment-community/wallaroo-aws-vm-community-setup/).\n",
    "\n",
    "Full details on how to configure a deployment through the SDK, see the [Wallaroo SDK guides](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/).\n",
    "\n",
    "For our example, we will perform the following:\n",
    "\n",
    "* Create a workspace for our work.\n",
    "* Upload the Aloha model.\n",
    "* Define a resource budget for our inference pipeline.\n",
    "* Create a pipeline that can ingest our submitted data, submit it to the model, and export the results\n",
    "* Run a sample inference through our pipeline by loading a file\n",
    "* Run a batch inference through our pipeline's URL and store the results in a file and find that the original memory\n",
    "  allocation is too small.\n",
    "* Redeploy the pipeline with a larger memory budget and attempt sending the same batch of requests through again.\n",
    "\n",
    "All sample data and models are available through the [Wallaroo Quick Start Guide Samples repository](https://github.com/WallarooLabs/quickstartguide_samples)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a Connection to Wallaroo\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  If logging in externally, update the `wallarooPrefix` and `wallarooSuffix` variables with the proper DNS information.  For more information on Wallaroo DNS settings, see the [Wallaroo DNS Integration Guide](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-dns-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "import pandas as pd\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "# used to display dataframe information without truncating\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Client connection from local Wallaroo instance\n",
    "\n",
    "# wl = wallaroo.Client()\n",
    "\n",
    "# SSO login through keycloak\n",
    "\n",
    "wallarooPrefix = \"YOUR PREFIX\"\n",
    "wallarooSuffix = \"YOUR SUFFIX\"\n",
    "\n",
    "wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n",
    "                    auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n",
    "                    auth_type=\"sso\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrow Support\n",
    "\n",
    "As of the 2023.1 release, Wallaroo provides support for dataframe and Arrow for inference inputs.  This tutorial allows users to adjust their experience based on whether they have enabled Arrow support in their Wallaroo instance or not.\n",
    "\n",
    "If Arrow support has been enabled, `arrowEnabled=True`. If disabled or you're not sure, set it to `arrowEnabled=False`\n",
    "\n",
    "The examples below will be shown in an arrow enabled environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "arrowEnabled=True\n",
    "os.environ[\"ARROW_ENABLED\"]=f\"{arrowEnabled}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful variables\n",
    "\n",
    "The following variables and methods are used to create a workspace, the pipeline in the example workspace and upload models into it.\n",
    "\n",
    "To allow this tutorial to be run multiple times or by multiple users in the same Wallaroo instance, a random 4 character prefix will be added to the workspace, pipeline, and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "# make a random 4 character prefix\n",
    "prefix= ''.join(random.choice(string.ascii_lowercase) for i in range(4))\n",
    "\n",
    "pipeline_name = f'{prefix}edgepipelineexample'\n",
    "workspace_name = f'{prefix}edgeworkspaceexample'\n",
    "model_name = f'{prefix}alohamodel'\n",
    "model_file_name = './alohacnnlstm.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workspace(name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "def get_pipeline(name):\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(pipeline_name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(pipeline_name)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Set the Workspace\n",
    "\n",
    "Create the workspace and set it as our default workspace.  If a workspace by the same name already exists, then that workspace will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'nxtledgeworkspaceexample', 'id': 67, 'archived': False, 'created_by': '39bf22e5-bb55-40ce-b1ab-36eed7a29150', 'created_at': '2023-02-16T21:34:51.091099+00:00', 'models': [], 'pipelines': []}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = get_workspace(workspace_name)\n",
    "\n",
    "wl.set_current_workspace(workspace)\n",
    "workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Models\n",
    "\n",
    "Now we will upload our models.  Note that for this example we are applying the model from a .ZIP file.  The Aloha model is a [protobuf](https://developers.google.com/protocol-buffers) file that has been defined for evaluating web pages, and we will configure it to use data in the `tensorflow` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wl.upload_model(model_name, model_file_name).configure(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the resource budget\n",
    "The DeploymentConfig object specifies the resources to allocate for a model pipeline. In this case, we're going to set a very small budget, one that is too small for this model and then expand it based on testing. To start with, we'll use 1 CPU and 150 MB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(1).memory(\"150Mi\").build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model\n",
    "Now that we have a model that we want to use we will create a deployment for it using the resource limits defined above. \n",
    "\n",
    "We will tell the deployment we are using a tensorflow model and give the deployment name and the configuration we want for the deployment.\n",
    "\n",
    "To do this, we'll create our pipeline that can ingest the data, pass the data to our Aloha model, and give us a final output.  We'll call our pipeline `edgepipeline`, then deploy it so it's ready to receive data.  The deployment process usually takes about 45 seconds.\n",
    "\n",
    "* **Note**:  If you receive an error that the pipeline could not be deployed because there are not enough resources, undeploy any other pipelines and deploy this one again.  This command can quickly undeploy all pipelines to regain resources.  We recommend **not** running this command in a production environment since it will cancel any running pipelines:\n",
    "\n",
    "```python\n",
    "for p in wl.list_pipelines(): p.undeploy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>nxtledgepipelineexample</td></tr><tr><th>created</th> <td>2023-02-16 21:34:53.913174+00:00</td></tr><tr><th>last_updated</th> <td>2023-02-16 21:34:54.545012+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>e02d066b-b8d9-4fab-ad71-e00645276727, 6afaefc1-1d83-4850-8e25-1b5d725fa4b3</td></tr><tr><th>steps</th> <td>nxtlalohamodel</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'nxtledgepipelineexample', 'create_time': datetime.datetime(2023, 2, 16, 21, 34, 53, 913174, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'nxtlalohamodel', 'version': 'ff764f70-5469-4022-9a7f-1e4b371c32c3', 'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8'}]}}]\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = get_pipeline(pipeline_name)\n",
    "pipeline.add_model_step(model)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the pipeline is running and list what models are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.48.1.152',\n",
       "   'name': 'engine-ddd5577c8-gjmnr',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'nxtledgepipelineexample',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'nxtlalohamodel',\n",
       "      'version': 'ff764f70-5469-4022-9a7f-1e4b371c32c3',\n",
       "      'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.48.1.151',\n",
       "   'name': 'engine-lb-74b4969486-psvgm',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "\n",
    "### Infer 1 row\n",
    "\n",
    "Now that the pipeline is deployed and our model is in place, we'll perform a smoke test to verify the pipeline is up and running properly.  We'll use the `infer_from_file` command to load a single encoded URL into the inference engine and print the results back out.\n",
    "\n",
    "The result should tell us that the tokenized URL is legitimate (0) or fraud (1).  This sample data should return close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.text_input</th>\n",
       "      <th>out.gozi</th>\n",
       "      <th>out.matsnu</th>\n",
       "      <th>out.simda</th>\n",
       "      <th>out.corebot</th>\n",
       "      <th>out.ramdo</th>\n",
       "      <th>out.suppobox</th>\n",
       "      <th>out.qakbot</th>\n",
       "      <th>out.pykspa</th>\n",
       "      <th>out.dircrypt</th>\n",
       "      <th>out.cryptolocker</th>\n",
       "      <th>out.banjori</th>\n",
       "      <th>out.kraken</th>\n",
       "      <th>out.ramnit</th>\n",
       "      <th>out.main</th>\n",
       "      <th>out.locky</th>\n",
       "      <th>check_failures</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-16 21:35:11.100</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 16, 32, 23, 29, 32, 30, 19, 26, 17]</td>\n",
       "      <td>[2.0289332e-05]</td>\n",
       "      <td>[0.010341614]</td>\n",
       "      <td>[1.7933435e-26]</td>\n",
       "      <td>[0.98291475]</td>\n",
       "      <td>[0.0062362333]</td>\n",
       "      <td>[1.3889844e-27]</td>\n",
       "      <td>[0.016155045]</td>\n",
       "      <td>[0.008038961]</td>\n",
       "      <td>[4.7591206e-05]</td>\n",
       "      <td>[0.012099553]</td>\n",
       "      <td>[0.0015195842]</td>\n",
       "      <td>[0.00031977228]</td>\n",
       "      <td>[0.0009985747]</td>\n",
       "      <td>[0.997564]</td>\n",
       "      <td>[0.011029261]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  \\\n",
       "0 2023-02-16 21:35:11.100   \n",
       "\n",
       "                                                                                                                                                      in.text_input  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 16, 32, 23, 29, 32, 30, 19, 26, 17]   \n",
       "\n",
       "          out.gozi     out.matsnu        out.simda   out.corebot  \\\n",
       "0  [2.0289332e-05]  [0.010341614]  [1.7933435e-26]  [0.98291475]   \n",
       "\n",
       "        out.ramdo     out.suppobox     out.qakbot     out.pykspa  \\\n",
       "0  [0.0062362333]  [1.3889844e-27]  [0.016155045]  [0.008038961]   \n",
       "\n",
       "      out.dircrypt out.cryptolocker     out.banjori       out.kraken  \\\n",
       "0  [4.7591206e-05]    [0.012099553]  [0.0015195842]  [0.00031977228]   \n",
       "\n",
       "       out.ramnit    out.main      out.locky  check_failures  \n",
       "0  [0.0009985747]  [0.997564]  [0.011029261]               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if arrowEnabled is True:\n",
    "    result = pipeline.infer_from_file('./data/data_1.df.json')\n",
    "else:\n",
    "    result = pipeline.infer_from_file(\"./data/data_1.json\")\n",
    "display(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **IMPORTANT NOTE**:  The `_deployment._url()` method will return an **internal** URL when using Python commands from within the Wallaroo instance - for example, the Wallaroo JupyterHub service.  When connecting via an external connection, `_deployment._url()` returns an **external** URL.  External URL connections requires [the authentication be included in the HTTP request](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-api-guide/), and that [Model Endpoints Guide](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-model-endpoints-guide/) external endpoints are enabled in the Wallaroo configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://sparkly-apple-3026.api.wallaroo.community/v1/api/pipelines/infer/nxtledgepipelineexample-76\n"
     ]
    }
   ],
   "source": [
    "inference_url = pipeline._deployment._url()\n",
    "print(inference_url)\n",
    "connection =wl.mlops().__dict__\n",
    "token = connection['token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arrowEnabled is True:\n",
    "    dataFile=\"./data/data_1k.df.json\"\n",
    "    contentType=\"application/json; format=pandas-records\"\n",
    "else:\n",
    "    dataFile=\"./data/data_1k.json\"\n",
    "    contentType=\"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  735k  100    95  100  735k     74   573k  0:00:01  0:00:01 --:--:--  577k\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST {inference_url} -H \"Authorization: Bearer {token}\" -H \"Content-Type:{contentType}\" --data @{dataFile} > curl_response.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redeploy with a little larger budget \n",
    "If you look in the file curl_response.txt, you will see that the inference failed:\n",
    "> upstream connect error or disconnect/reset before headers. reset reason: connection termination\n",
    "\n",
    "Even though a single inference passed, submitted a larger batch of work did not. If this is an expected usage case for\n",
    "this model, we need to add more memory. Let's do that now.\n",
    "\n",
    "The following DeploymentConfig is the same as the original, but increases the memory from 150MB to 300MB. This sort\n",
    "of budget would be available on some network routers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>nxtledgepipelineexample</td></tr><tr><th>created</th> <td>2023-02-16 21:34:53.913174+00:00</td></tr><tr><th>last_updated</th> <td>2023-02-16 21:35:52.382536+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>01db2afd-05d9-47b0-954d-0bde74d4a8fe, 0084c8f0-f8fc-421c-9b24-724bf9a5e1a5, e02d066b-b8d9-4fab-ad71-e00645276727, 6afaefc1-1d83-4850-8e25-1b5d725fa4b3</td></tr><tr><th>steps</th> <td>nxtlalohamodel</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'nxtledgepipelineexample', 'create_time': datetime.datetime(2023, 2, 16, 21, 34, 53, 913174, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'nxtlalohamodel', 'version': 'ff764f70-5469-4022-9a7f-1e4b371c32c3', 'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8'}]}}]\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()\n",
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(1).memory(\"300Mi\").build()\n",
    "pipeline = wl.build_pipeline(pipeline_name)\n",
    "pipeline.add_model_step(model)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-run inference\n",
    "Running the same curl command again should now produce a curl_response.txt file containing the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1402k  100  666k  100  735k   243k   268k  0:00:02  0:00:02 --:--:--  515k\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST {inference_url} -H \"Authorization: Bearer {token}\" -H \"Content-Type:{contentType}\" --data @{dataFile} > curl_response.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that increasing the memory was necessary to run a batch of 1,000 inferences at once. If this is not a design\n",
    "use case for your system, running with the smaller memory budget may be acceptable. Wallaroo allows you to easily test difference\n",
    "loading patterns to get a sense for what resources are required with sufficient buffer to allow for robust operation of your system\n",
    "while not over-provisioning scarce resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undeploy Pipeline\n",
    "\n",
    "When finished with our tests, we will undeploy the pipeline so we have the Kubernetes resources back for other tasks.  Note that if the deployment variable is unchanged aloha_pipeline.deploy() will restart the inference engine in the same configuration as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>nxtledgepipelineexample</td></tr><tr><th>created</th> <td>2023-02-16 21:34:53.913174+00:00</td></tr><tr><th>last_updated</th> <td>2023-02-16 21:35:52.382536+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>01db2afd-05d9-47b0-954d-0bde74d4a8fe, 0084c8f0-f8fc-421c-9b24-724bf9a5e1a5, e02d066b-b8d9-4fab-ad71-e00645276727, 6afaefc1-1d83-4850-8e25-1b5d725fa4b3</td></tr><tr><th>steps</th> <td>nxtlalohamodel</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'nxtledgepipelineexample', 'create_time': datetime.datetime(2023, 2, 16, 21, 34, 53, 913174, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'nxtlalohamodel', 'version': 'ff764f70-5469-4022-9a7f-1e4b371c32c3', 'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8'}]}}]\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrowtests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dda4bf3640b7fafcd1648658b879b4cc9f6ba6084e8fb356fdaaa1a461d1690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
