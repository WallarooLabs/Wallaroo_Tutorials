{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/wallaroo-model-cookbooks/aloha).\n",
    "\n",
    "## Aloha Demo\n",
    "\n",
    "In this notebook we will walk through a simple pipeline deployment to inference on a model. For this example we will be using an open source model that uses an [Aloha CNN LSTM model](https://www.researchgate.net/publication/348920204_Using_Auxiliary_Inputs_in_Deep_Learning_Models_for_Detecting_DGA-based_Domain_Names) for classifying Domain names as being either legitimate or being used for nefarious purposes such as malware distribution.  \n",
    "\n",
    "For our example, we will perform the following:\n",
    "\n",
    "* Create a workspace for our work.\n",
    "* Upload the Aloha model.\n",
    "* Create a pipeline that can ingest our submitted data, submit it to the model, and export the results\n",
    "* Run a sample inference through our pipeline by loading a file\n",
    "* Run a sample inference through our pipeline's URL and store the results in a file.\n",
    "\n",
    "All sample data and models are available through the [Wallaroo Quick Start Guide Samples repository](https://github.com/WallarooLabs/quickstartguide_samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a Connection to Wallaroo\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "import asyncio \n",
    "import datetime\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "\n",
    "# used to display dataframe information without truncating\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# to display dataframe tables\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log into the following URL in a web browser:\n",
      "\n",
      "\thttps://sparkly-apple-3026.keycloak.wallaroo.community/auth/realms/master/device?user_code=GHMU-YILH\n",
      "\n",
      "Login successful!\n"
     ]
    }
   ],
   "source": [
    "# Client connection from local Wallaroo instance\n",
    "\n",
    "wl = wallaroo.Client()\n",
    "\n",
    "# SSO login through keycloak\n",
    "\n",
    "# wallarooPrefix = \"YOUR PREFIX\"\n",
    "# wallarooSuffix = \"YOUR SUFFIX\"\n",
    "\n",
    "# wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n",
    "#                     auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n",
    "#                     auth_type=\"sso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrow Support\n",
    "\n",
    "As of the 2023.1 release, Wallaroo provides support for dataframe and Arrow for inference inputs.  This tutorial allows users to adjust their experience based on whether they have enabled Arrow support in their Wallaroo instance or not.\n",
    "\n",
    "If Arrow support has been enabled, `arrowEnabled=True`. If disabled or you're not sure, set it to `arrowEnabled=False`\n",
    "\n",
    "The examples below will be shown in an arrow enabled environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Only set the below to make the OS environment ARROW_ENABLED to TRUE.  Otherwise, leave as is.\n",
    "# os.environ[\"ARROW_ENABLED\"]=\"True\"\n",
    "\n",
    "if \"ARROW_ENABLED\" not in os.environ or os.environ[\"ARROW_ENABLED\"].casefold() == \"False\".casefold():\n",
    "    arrowEnabled = False\n",
    "else:\n",
    "    arrowEnabled = True\n",
    "print(arrowEnabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Workspace\n",
    "\n",
    "We will create a workspace to work in and call it the \"alohaworkspace\", then set it as current workspace environment.  We'll also create our pipeline in advance as `alohapipeline`.  The model name and the model file will be specified for use in later steps.\n",
    "\n",
    "To allow this tutorial to be run multiple times or by multiple users in the same Wallaroo instance, a random 4 character prefix will be added to the workspace, pipeline, and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "# make a random 4 character prefix\n",
    "prefix= ''.join(random.choice(string.ascii_lowercase) for i in range(4))\n",
    "workspace_name = f\"{prefix}alohaworkspace\"\n",
    "pipeline_name = f\"{prefix}alohapipeline\"\n",
    "model_name = f\"{prefix}alohamodel\"\n",
    "model_file_name = './alohacnnlstm.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workspace(name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "def get_pipeline(name):\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(pipeline_name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(pipeline_name)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aloha_pipeline = wl.list_pipelines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aloha_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>voomalohapipeline</td></tr><tr><th>created</th> <td>2023-06-26 19:33:33.632845+00:00</td></tr><tr><th>last_updated</th> <td>2023-06-26 19:33:33.632845+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>c963b069-8fc8-4562-98c0-b05980b3b10e</td></tr><tr><th>steps</th> <td></td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'voomalohapipeline', 'create_time': datetime.datetime(2023, 6, 26, 19, 33, 33, 632845, tzinfo=tzutc()), 'definition': '[]'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = get_workspace(workspace_name)\n",
    "\n",
    "wl.set_current_workspace(workspace)\n",
    "\n",
    "aloha_pipeline = get_pipeline(pipeline_name)\n",
    "aloha_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the workspace is created the current default workspace with the `get_current_workspace()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wl.get_current_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Models\n",
    "\n",
    "Now we will upload our models.  Note that for this example we are applying the model from a .ZIP file.  The Aloha model is a [protobuf](https://developers.google.com/protocol-buffers) file that has been defined for evaluating web pages, and we will configure it to use data in the `tensorflow` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wl.upload_model(model_name, model_file_name).configure(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model\n",
    "\n",
    "Now that we have a model that we want to use we will create a deployment for it. \n",
    "\n",
    "We will tell the deployment we are using a tensorflow model and give the deployment name and the configuration we want for the deployment.\n",
    "\n",
    "To do this, we'll create our pipeline that can ingest the data, pass the data to our Aloha model, and give us a final output.  We'll call our pipeline `aloha-test-demo`, then deploy it so it's ready to receive data.  The deployment process usually takes about 45 seconds.\n",
    "\n",
    "* **Note**:  If you receive an error that the pipeline could not be deployed because there are not enough resources, undeploy any other pipelines and deploy this one again.  This command can quickly undeploy all pipelines to regain resources.  We recommend **not** running this command in a production environment since it will cancel any running pipelines:\n",
    "\n",
    "```python\n",
    "for p in wl.list_pipelines(): p.undeploy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>voomalohapipeline</td></tr><tr><th>created</th> <td>2023-06-26 19:33:33.632845+00:00</td></tr><tr><th>last_updated</th> <td>2023-06-26 19:33:33.632845+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>c963b069-8fc8-4562-98c0-b05980b3b10e</td></tr><tr><th>steps</th> <td></td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'voomalohapipeline', 'create_time': datetime.datetime(2023, 6, 26, 19, 33, 33, 632845, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'voomalohamodel', 'version': 'a83f7c41-5b69-4485-92d4-8c6c326db3f6', 'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8'}]}}]\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.add_model_step(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLICAS = 8\n",
    "deployment_config = (wallaroo.DeploymentConfigBuilder()\n",
    "    .cpus(1)\n",
    "    .memory(\"0.5Gi\")\n",
    "    .replica_count(REPLICAS)\n",
    "    .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 45s .............. ok\n"
     ]
    }
   ],
   "source": [
    "aloha_pipeline = aloha_pipeline.deploy(deployment_config =deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the pipeline is running and list what models are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aloha_pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interferences\n",
    "\n",
    "### Infer 1 row\n",
    "\n",
    "Now that the pipeline is deployed and our Aloha model is in place, we'll perform a smoke test to verify the pipeline is up and running properly.  We'll use the `infer_from_file` command to load a single encoded URL into the inference engine and print the results back out.\n",
    "\n",
    "The result should tell us that the tokenized URL is legitimate (0) or fraud (1).  This sample data should return close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if arrowEnabled is True:\n",
    "    result = aloha_pipeline.infer_from_file('./data/data_1.df.json')\n",
    "else:\n",
    "    result = aloha_pipeline.infer_from_file(\"\n",
    "                                            ./data/data_1.json\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for parallel inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, ABC, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 16, 32, 23, 29, 32, 30, 19, 26, 17]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                           text_input\n",
       "0  [0, ABC, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 28, 16, 32, 23, 29, 32, 30, 19, 26, 17]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = pd.read_json(\"./data/data_1.df.json\")\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(200):\n",
    "    dataset.append(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for index, row in input_df.head(200).iterrows():\n",
    "    dataset.append(row.to_frame('text_input').reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.append(input_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential inference - how it is done today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed_in_parallel = 0.313088 : 1\n"
     ]
    }
   ],
   "source": [
    "timeout_secs=1200\n",
    "now = datetime.datetime.now()\n",
    "##########\n",
    "results = aloha_pipeline.infer(tensor=input_df, timeout=10)\n",
    "total = datetime.datetime.now() - now\n",
    "print(f\"Elapsed_in_parallel = {total.total_seconds()} : {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed = 12.768011 : 200\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Run the inference sequentially to establish a baseline\n",
    "#\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "results = []\n",
    "for df in dataset:\n",
    "    results.append(aloha_pipeline.infer(tensor=df, timeout=10))\n",
    "\n",
    "total = datetime.datetime.now() - now\n",
    "\n",
    "print(f\"Elapsed = {total.total_seconds()} : {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel inference using SDK's new pipeline.parallel_infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed_in_parallel = 1.72374 : 201\n"
     ]
    }
   ],
   "source": [
    "timeout_secs=1200\n",
    "now = datetime.datetime.now()\n",
    "##########\n",
    "parallel_results = await aloha_pipeline.parallel_infer(tensor_list=dataset, timeout=timeout_secs, num_parallel=2*8, retries=3)\n",
    "##########\n",
    "total = datetime.datetime.now() - now\n",
    "print(f\"Elapsed_in_parallel = {total.total_seconds()} : {len(parallel_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "httpx.HTTPStatusError(\"Server error '500 Internal Server Error' for url 'http://engine-lb.voomalohapipeline-51:29502/pipelines/voomalohapipeline?dataset%5B%5D=%2A&dataset.exclude%5B%5D=metadata&dataset.separator=.'\\nFor more information check: https://httpstatuses.com/500\")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_results[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_infer():\n",
    "    timeout_secs=1200\n",
    "    now = datetime.datetime.now()\n",
    "    ##########\n",
    "    parallel_results = asyncio.run(aloha_pipeline.parallel_infer(tensor_list=dataset, timeout=timeout_secs, num_parallel=8, retries=3))\n",
    "    ##########\n",
    "    total = datetime.datetime.now() - now\n",
    "    print(f\"Elapsed_in_parallel = {total.total_seconds()} : {len(parallel_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "threads = []\n",
    "for i in range(3):\n",
    "    thread = threading.Thread(target=wrapper_infer)\n",
    "    threads.append(thread)\n",
    "    print(\"starting new thread...............\")\n",
    "    thread.start()\n",
    "    \n",
    "\n",
    "# Wait for all threads to complete\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.datetime.now()\n",
    "# results_df = pd.concat((r for r in parallel_results if not None))\n",
    "# total = datetime.datetime.now() - now\n",
    "# print(f\"Elapsed_in_parallel = {total.total_seconds()} : {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIY parallel infer using pipeline.async_infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# PYTHON PARALLEL VERSION\n",
    "#\n",
    "\n",
    "import asyncio\n",
    "import httpx\n",
    "\n",
    "async def parallel_infer(pipe, dataset, timeout_secs, num_parallel):\n",
    "    # do more \n",
    "    \n",
    "    results = []\n",
    "    async with httpx.AsyncClient(timeout=timeout_secs) as client:\n",
    "        # get data from db async\n",
    "        tasks = [pipe.async_infer(async_client=client, tensor=df, timeout=timeout_secs) for idx, df in enumerate(dataset)]\n",
    "        results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "        # analysis async\n",
    "        \n",
    "    # do more\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed_in_parallel = 1.885742 : 200\n"
     ]
    }
   ],
   "source": [
    "timeout_secs=1200\n",
    "now = datetime.datetime.now()\n",
    "diy_results = await parallel_infer(aloha_pipeline, dataset, timeout_secs, 8)\n",
    "total = datetime.datetime.now() - now\n",
    "print(f\"Elapsed_in_parallel = {total.total_seconds()} : {len(diy_results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I don't like using await - what are my options?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeout_secs=10\n",
    "def sync_parallel_infer():\n",
    "    return asyncio.run(aloha_pipeline.parallel_infer(dataset, timeout_secs, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "sync_results = sync_parallel_infer()\n",
    "total = datetime.datetime.now() - now\n",
    "print(f\"Elapsed_in_parallel = {total.total_seconds()} : {len(sync_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sync_results[199]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Gevent parallel inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent.pool\n",
    "import json\n",
    "\n",
    "from geventhttpclient import HTTPClient\n",
    "from geventhttpclient.url import URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = aloha_pipeline._deployment._url()\n",
    "inference_url = URL(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_async_infer():\n",
    "    greenlets = []\n",
    "    # Create a gevent pool with a maximum concurrency level\n",
    "    pool = gevent.pool.Pool(8)\n",
    "    for data in dataset:\n",
    "        greenlet = pool.spawn(aloha_pipeline.infer, data)\n",
    "        greenlets.append(greenlet)\n",
    "    gevent.joinall(greenlets)\n",
    "    \n",
    "    results = [greenlet.value for greenlet in greenlets]\n",
    "    # combined_df = pd.concat(results, ignore_index=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "results = aloha_pipeline.parallel_infer(dataset, num_parallel)\n",
    "total = datetime.datetime.now() - now\n",
    "print(f\"Elapsed_in_parallel = {total.total_seconds()} : {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undeploy Pipeline\n",
    "\n",
    "When finished with our tests, we will undeploy the pipeline so we have the Kubernetes resources back for other tasks.  Note that if the deployment variable is unchanged aloha_pipeline.deploy() will restart the inference engine in the same configuration as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aloha_pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "lock = asyncio.Lock()\n",
    "\n",
    "async def async_task(i):\n",
    "    print(f\"Async {i} task starts\")\n",
    "    await sync_task_wrapper(i)\n",
    "    await asyncio.sleep(1)\n",
    "    print(f\"Async {i} task finishes\")\n",
    "    \n",
    "async def sync_task_wrapper(i):\n",
    "    async with lock:\n",
    "        await asyncio.get_event_loop().run_in_executor(sync_task(i))\n",
    "\n",
    "def sync_task(n):\n",
    "    print(f\"Sync task {n} starts\")\n",
    "    # Simulating a blocking operation\n",
    "    for i in range(100):\n",
    "        pass\n",
    "    print(f\"Sync task {n} finishes\")\n",
    "\n",
    "async def main():\n",
    "    print(\"Main task starts\")\n",
    "    tasks = [\n",
    "        async_task(i)\n",
    "        for i in range(5)\n",
    "    ]\n",
    "    await asyncio.gather(*tasks, return_exceptions=True)\n",
    "    await asyncio.sleep(2)  # Simulating other async operations\n",
    "    print(\"Main task finishes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_infer():\n",
    "    asyncio.run(parallel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "threads = []\n",
    "for i in range(3):\n",
    "    thread = threading.Thread(target=wrapper_main)\n",
    "    threads.append(thread)\n",
    "    print(\"starting new thread...............\")\n",
    "    thread.start()\n",
    "    \n",
    "\n",
    "# Wait for all threads to complete\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dda4bf3640b7fafcd1648658b879b4cc9f6ba6084e8fb356fdaaa1a461d1690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
