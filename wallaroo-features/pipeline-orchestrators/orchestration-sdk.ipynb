{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d6daff",
   "metadata": {},
   "source": [
    "## Pipeline Orchestrations Tutorial\n",
    "\n",
    "Wallaroo provides data connections, orchestrations, and tasks to provide organizations with a method of creating and managing automated tasks that can either be run on demand, on a regular schedule, or as a service so they respond to requests.\n",
    "\n",
    "| Object | Description |\n",
    "|---|---|\n",
    "| Orchestration | A set of instructions written as a python script with a requirements library.  Orchestrations are uploaded to the Wallaroo instance |\n",
    "| Task | An implementation of an orchestration.  Tasks are run either once when requested, on a repeating schedule, or as a service. |\n",
    "| Connector | Definitions set by MLOps engineers that are used by other Wallaroo users for connection information to a data source.  Usually paired with orchestrations. |\n",
    "\n",
    "A typical flow in the orchestration, task and connector life cycle is:\n",
    "\n",
    "1. (Optional) A connector is defined with information such as username, connection URL, tokens, etc.\n",
    "1. One or more connectors are applied to a workspace for users to implement in their code or orchestrations.\n",
    "1. An orchestration is created to perform some set instructions.  For example:\n",
    "    1. Deploy a pipeline, request data from an external service, store the results in an external database, then undeploy the pipeline.\n",
    "    1. Download a ML Model then replace a current pipeline step with the new version.\n",
    "    1. Collect log files from a deployed pipeline once every hour and submit it to a Kafka or other service.\n",
    "1. A task is created that specifies the orchestration to perform and the schedule:\n",
    "    1. Run once.\n",
    "    1. Run on a schedule (based on `cron` like settings).\n",
    "    1. Run as a service to be run whenever requested.\n",
    "1. Once the use for a task is complete, it is killed and its schedule or service removed.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "The tutorial will demonstrate the following:\n",
    "\n",
    "1. Create a simple connection to retrieve an Apache Arrow table file from a GitHub registry.\n",
    "1. Create an orchestration that retrieves the Apache Arrow table file from the location defined by the connection, deploy a pipeline, perform an inference, then undeploys the pipeline.\n",
    "1. Implement the orchestration as a task that runs every minute.\n",
    "1. Display the logs from the pipeline after 5 minutes to verify the task is running."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67e1bb26",
   "metadata": {},
   "source": [
    "## Requires Libraries\n",
    "\n",
    "The following libraries are required, and included by default in a Wallaroo instance's JupyterHub service.\n",
    "\n",
    "* [wallaroo](https://pypi.org/project/wallaroo/):  The Wallaroo SDK.\n",
    "* [pandas](https://pypi.org/project/pandas/): The pandas data analysis library.\n",
    "* [pyarrow](https://pypi.org/project/pyarrow/): The Apache Arrow Python library.\n",
    "\n",
    "The specific versions used are set in the file `./resources/requirements.txt`.  Supported libraries are automatically installed with the `pypi` or `conda` commands.  For example, from the root of this tutorials folder:\n",
    "\n",
    "```python\n",
    "pip install -r ./resources/requirements.txt\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae5103a-ce97-4d57-8bca-708150127610",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Using pipeline orchestrations consist of these steps: \n",
    "1. [Write orchestration code](#1.-Write-orchestration-code)\n",
    "2. [Create archive](#2.-Create-archive)\n",
    "3. [Upload archive](#3.-Upload-archive)\n",
    "4. [Run task](#4.-Run-task)\n",
    "\n",
    "[Task Management](#Task-Management)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2b09330-0408-45eb-b321-b48b65041789",
   "metadata": {},
   "source": [
    "## Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to a Wallaroo instance.  We'll load the libraries and set our client connection settings\n",
    "\n",
    "### Workspace, Model and Pipeline Setup\n",
    "\n",
    "For this tutorial, we'll create a workspace, upload our sample model and deploy a pipeline.  We'll perform some quick sample inferences to verify that everything it working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "\n",
    "# to display dataframe tables\n",
    "from IPython.display import display\n",
    "# used to display dataframe information without truncating\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import pyarrow as pa\n",
    "\n",
    "import os\n",
    "# Used for the Wallaroo SDK version 2023.1\n",
    "os.environ[\"ARROW_ENABLED\"]=\"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d19a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login through local Wallaroo instance\n",
    "\n",
    "# wl = wallaroo.Client()\n",
    "\n",
    "# # SSO login through keycloak\n",
    "\n",
    "# wallarooPrefix = \"YOUR PREFIX\"\n",
    "# wallarooSuffix = \"YOUR PREFIX\"\n",
    "\n",
    "\n",
    "# wallarooPrefix = \"doc-test\"\n",
    "# wallarooSuffix = \"wallaroocommunity.ninja\"\n",
    "\n",
    "\n",
    "# wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n",
    "#                     auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n",
    "#                     auth_type=\"sso\")\n",
    "\n",
    "# os.environ[\"WALLAROO_SDK_CREDENTIALS\"] = './creds.json.example'\n",
    "\n",
    "wallarooPrefix=\"doc-test\"\n",
    "wallarooSuffix=\"wallaroocommunity.ninja\"\n",
    "\n",
    "# wallarooPrefix=\"product-uat-ee\"\n",
    "# wallarooSuffix=\"wallaroocommunity.ninja\"\n",
    "\n",
    "# wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n",
    "#                     auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n",
    "#                     auth_type=\"sso\")\n",
    "\n",
    "wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n",
    "                    auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n",
    "                    auth_type=\"sso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a981ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables for later steps\n",
    "\n",
    "workspace_name = 'orchestrationworkspace'\n",
    "pipeline_name = 'orchestrationpipeline'\n",
    "model_name = 'orchestrationmodel'\n",
    "model_file_name = './models/rf_model.onnx'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c46c722",
   "metadata": {},
   "source": [
    "### Helper Methods\n",
    "\n",
    "The following helper methods are used to either create or get workspaces and pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to retrieve workspaces and pipelines\n",
    "\n",
    "def get_workspace(name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "def get_pipeline(name):\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(name)\n",
    "    return pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98b78cf9",
   "metadata": {},
   "source": [
    "### Create the Workspace and Pipeline\n",
    "\n",
    "We'll now create our workspace and pipeline for the tutorial.  If this tutorial has been run previously, then this will retrieve the existing ones with the assumption they're for us with this tutorial.\n",
    "\n",
    "We'll set the retrieved workspace as the current workspace in the SDK, so all commands will default to that workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = get_workspace(workspace_name)\n",
    "wl.set_current_workspace(workspace)\n",
    "\n",
    "pipeline = get_pipeline(pipeline_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9fc274",
   "metadata": {},
   "source": [
    "### Upload the Model and Deploy Pipeline\n",
    "\n",
    "We'll upload our model into our sample workspace, then add it as a pipeline step before deploying the pipeline to it's ready to accept inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a69610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model\n",
    "\n",
    "housing_model_control = wl.upload_model(model_name, model_file_name).configure()\n",
    "\n",
    "# Add the model as a pipeline step\n",
    "\n",
    "pipeline.add_model_step(housing_model_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy the pipeline\n",
    "pipeline.deploy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "573bc347",
   "metadata": {},
   "source": [
    "### Sample Inferences\n",
    "\n",
    "We'll perform some quick sample inferences using an Apache Arrow table as the input.  Once that's finished, we'll undeploy the pipeline and return the resources back to the Wallaroo instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a9e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample inferences\n",
    "\n",
    "batch_inferences = pipeline.infer_from_file('./data/xtest-1k.arrow')\n",
    "\n",
    "large_inference_result =  batch_inferences.to_pandas()\n",
    "display(large_inference_result.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf77c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# undeploy the pipeline\n",
    "\n",
    "pipeline.undeploy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7fc12cb",
   "metadata": {},
   "source": [
    "## Create Wallaroo Connection\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f37027c1",
   "metadata": {},
   "source": [
    "\n",
    "Connections are created at the Wallaroo instance level, typically by a MLOps or DevOps engineer, then applied to a workspace.\n",
    "\n",
    "For this section:\n",
    "\n",
    "1. We will create a sample connection that just has a URL to the same Arrow table file we used in the previous step.\n",
    "1. We'll apply the data connection to the workspace above.\n",
    "1. For a quick demonstration, we'll use the connection to retrieve the Arrow table file and use it for a quick sample inference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da10ec00",
   "metadata": {},
   "source": [
    "### Create Connection\n",
    "\n",
    "Connections are created with the Wallaroo client command [`add_connection`](https://staging.docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-dataconnectors/#create-data-connection) with the following parameters.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| --- | --- | ---|\n",
    "| **name** | string (Required) | The name of the connector. |\n",
    "| **type** | string (Required) | The user defined type of connector. |\n",
    "| **details** | Dict (Requires) | User defined configuration details for the data connection.  These can be `{'username':'dataperson', 'password':'datapassword', 'port': 3339}`, or `{'token':'abcde123==', 'host':'example.com', 'port:1234'}`, or other user defined combinations.  |\n",
    "\n",
    "We'll create the connection named `houseprice_arrow_table`, set it to the type `HTTPFILE`, and provide the details as `'host':'https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/20230314_2023.2_updates/wallaroo-testing-tutorials/houseprice-saga/data/xtest-1k.arrow?raw=true'` - the location for our sample Arrow table inference input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.add_connection(\"houseprice_arrow_table\", \n",
    "                  \"HTTPFILE\", \n",
    "                  {'host':'https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/20230314_2023.2_updates/wallaroo-testing-tutorials/houseprice-saga/data/xtest-1k.arrow?raw=true'}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7cd12-3e81-4221-8161-47a9e2c71cfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "The orchestration code will run in an environment very similar to the Wallaroo Jupyter Lab experience. The idea is code should work the same in a Lab notebook as it does in an orchestrator task.\n",
    "\n",
    "* Same Python version\n",
    "* Same Wallaroo SDK version\n",
    "* Code may assume it will run in an empty `/home/jovyan` directory (any desired code or artifacts must be included explicitly)\n",
    "* Pip dependencies can be specified\n",
    "* `wallaroo.Client` constructor `auth_type` argument is ignored - it's okay to pass nothing\n",
    "* New functions:\n",
    "    * `wallaroo.in_task()` returns `True` if the code is running in an Orchestrator task\n",
    "    * `wallaroo.task_args()` returns a `Dict` of invocation-specific arguments passed to the `run_` calls\n",
    "\n",
    "Example `requirements.txt`\n",
    "--------------------------\n",
    "```python\n",
    "dbt-bigquery==1.4.3\n",
    "dbt-core==1.4.5\n",
    "dbt-extractor==0.4.1\n",
    "dbt-postgres==1.4.5\n",
    "google-api-core==2.8.2\n",
    "google-auth==2.11.0\n",
    "google-auth-oauthlib==0.4.6\n",
    "google-cloud-bigquery==3.3.2\n",
    "google-cloud-bigquery-storage==2.15.0\n",
    "google-cloud-core==2.3.2\n",
    "google-cloud-storage==2.5.0\n",
    "google-crc32c==1.5.0\n",
    "google-pasta==0.2.0\n",
    "google-resumable-media==2.3.3\n",
    "googleapis-common-protos==1.56.4\n",
    "```\n",
    "\n",
    "Example orchestrator\n",
    "--------------------\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "import wallaroo\n",
    "\n",
    "wl = wallaroo.Client()\n",
    "\n",
    "if wl.in_task():\n",
    "    conn_name = wl.task_args()[\"connection-name\"]\n",
    "    print(f\"I am running in a task and I will read from {conn_name}\")\n",
    "          \n",
    "# Get a database connection\n",
    "workspace = wl.get_current_workspace()\n",
    "prodcon = workspace.get_connection(conn_name)\n",
    "\n",
    "# Deploy a pipeline\n",
    "\n",
    "# Do some queries and inferences\n",
    "etc...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7dedcc-898f-44d2-8fca-f5cf2793caa2",
   "metadata": {},
   "source": [
    "Here are some other SDK functions useful inside an orchestration task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e235f8c-4817-402f-b8a9-6f52b51f4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will always return False in a notebook, because we are not in a task\n",
    "wl.in_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76feff31-f92e-43f2-a829-d091e6c76a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we were in a task, we could get our arguments like this\n",
    "wl.task_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f320d1c2-4edc-4c76-b3b7-32d3510b26a3",
   "metadata": {},
   "source": [
    "## 2. Create archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5711b658-b16a-45c4-9d2b-0bc45a1bd222",
   "metadata": {},
   "source": [
    "The uploaded artifact must be a ZIP file which contains:\n",
    "\n",
    "* User code. If `main.py` exists, then that will be used as the task entrypoint. Otherwise, if only one .py exists, then that will be the entrypoint.\n",
    "* Optional: A standard Python `requirements.txt` for any dependencies to be provided in the task environment. The Wallaroo SDK will already be present and should not be mentioned.\n",
    "* Optional: Any other artifacts desired for runtime, including data or code.\n",
    "\n",
    "The ZIP file should not contain any directories: only files at the top level.\n",
    "\n",
    "**Note** - In future versions SDK may help with packaging. In this version, our Jupyter Lab includes the `zip` program for use in a terminal tab, or the zip file can be created elsewhere.\n",
    "\n",
    "### Procedure\n",
    "\n",
    "In a terminal, whether in a Jupyter Lab or desktop, assemble artifacts as above and then create the archive.\n",
    "```shell\n",
    "$ zip hello.zip main.py requirements.txt \n",
    "  adding: main.py (deflated 47%)\n",
    "  adding: requirements.txt (deflated 52%)\n",
    "```\n",
    "\n",
    "The first argument to `zip` is the name of the archive you want, while the rest are all of the contents to add."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406aa68a-bd96-42a5-b598-2cdfff5f5270",
   "metadata": {},
   "source": [
    "## 3. Upload archive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284928be-b2af-459b-8470-8deb1ba68f5f",
   "metadata": {},
   "source": [
    "With zip archive in hand, we can use the SDK to upload it to Wallaroo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb6e9e-25bb-4f96-8e1c-a2a4455d0ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "import os\n",
    "os.environ[\"WALLAROO_SDK_CREDENTIALS\"] = \"creds.json\"\n",
    "wl = wallaroo.Client(auth_type=\"user_password\")\n",
    "orc1 = wl.upload_orchestration(path=\"hello.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cbf818-f772-4cc7-a31c-fd6b82f7f9d7",
   "metadata": {},
   "source": [
    "At this point, Wallaroo will perform a packaging step where it downloads and installs all the dependencies listed in `requirements.txt`. The status can be observed either in the orchestration list or by examining an individual orchestration object. The state will transition from `packaging pending` through `ready`. Also, the `orchestration.status()` method can be used to pause until packaging is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd96e5-672f-4304-847a-20625cd4071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ae1b8f-2e07-4213-bd1e-6b92deb97454",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.list_orchestrations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540a253-7294-4aa5-90cf-28cde358dd39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while orc1.status() != 'ready': \n",
    "    print(\"waiting\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc05481-4378-499e-8cea-60d37150f41b",
   "metadata": {},
   "source": [
    "## 4. Run task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a54d710-01c0-462c-8994-a677501fd121",
   "metadata": {},
   "source": [
    "At this point, given a `ready` orchestration, it's ready to run as a task.  There are three alternatives here.\n",
    "\n",
    "With an orchestration in hand we can launch it as a task in three ways.\n",
    "\n",
    "| Type       | SDK Call |  How triggered                                                               | Purpose                                                       |\n",
    "|------------|----------|:------------------------------------------------------------------------------|:---------------------------------------------------------------|\n",
    "| Once       | `orc.run_once()` | User makes one api call. Task runs once and exits                      | Single batch, experimentation                                 |\n",
    "| Scheduled  | `orc.run_scheduled()` | User provides schedule. Task runs exits whenever schedule dictates                                  | Recurrent batch ETL                                           |\n",
    "| Continuous | `orc.run_continuously()` | User provides a listen port. Task runs forever. It can listen on that port if it wants and we help. | User defined network service, continuous ETL, queue processor |\n",
    "\n",
    "All take a `json_args` parameter, where the user can pass an invocation-specific `Dict` of arguments which will be available in the running task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbb364-463e-487f-93d9-33ed7af58c4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b73d28-7169-41da-a434-afafde39af3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: run once\n",
    "task = orc1.run_once({\"a\":\"b\"})\n",
    "task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72243e1a-a379-4b23-b8b9-34d01ca1fbf8",
   "metadata": {},
   "source": [
    "# Task Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204597e-ccae-4801-acd3-a516455ea745",
   "metadata": {},
   "source": [
    "## Under the hood - Kubernetes log retrieval\n",
    "\n",
    "In upcoming releases we will be able to retrieve task logs in friendly manners. In the meantime, some kubectl magic will be necessary. The cloud host's Kuberenetes log console can also be used.\n",
    "\n",
    "When a task is launched, the ID will be shown in its object as above.  With that in hand, list workspaces\n",
    "\n",
    "```shell\n",
    "$ kubectl get namespace\n",
    "NAME                       STATUS   AGE\n",
    "default                    Active   530d\n",
    "kube-system                Active   530d\n",
    "wallaroo                   Active   16d\n",
    "tasks-00d86236-d0fe-4a-1   Active   26s```\n",
    "```\n",
    "\n",
    "Notice the last namespace with matching `task-ID`.  There will be one task in that namespace:\n",
    "\n",
    "```shell\n",
    "$ kubectl -n tasks-00d86236-d0fe-4a-1 get pod\n",
    "NAME                                                READY   STATUS             RESTARTS       AGE\n",
    "00d-exec-orch-oneshot-arb-one-exe-6f95c84ff-99586   0/1     Running            5 (107s ago)   6m19s\n",
    "```\n",
    "\n",
    "You can now list logs from that pod.\n",
    "\n",
    "```shell\n",
    "$ kubectl -n tasks-00d86236-d0fe-4a-1 logs 00d-exec-orch-oneshot-arb-one-exe-6f95c84ff-99586\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008070ae-7d5c-4c8d-afc0-96a4ffdd3ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the status of all tasks\n",
    "wl.list_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a83c59-21e1-4144-bbd2-3e37aa606567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill one task\n",
    "task.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b523e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill one task\n",
    "task.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4aee0-18e6-49b5-b8a6-f45e78059dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill all tasks\n",
    "for t in wl.list_tasks(): t.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b50b6b9-7c79-4ca8-b684-59abcc4f5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Orchestration\n",
    "\n",
    "url = f\"{wl.api_endpoint}/v1/api/orchestration/upload\"\n",
    "\n",
    "fp = open(\"hello.zip\", \"rb\")\n",
    "resp = requests.post(\n",
    "    url,\n",
    "    headers={\n",
    "    \"Authorization\": wl.auth._bearer_token_str(), },\n",
    "    files=[(\"file\", (\"hello2.zip\", fp, \"application/octet-stream\")), \n",
    "         (\"metadata\", (\"metadata\", '{\"workspace_id\": 1}', \"application/json\"))],\n",
    ")\n",
    "\n",
    "assert resp.status_code == 202\n",
    "(resp.status_code, resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e84ec-3911-40f1-8248-bec180b525a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=f\"{wl.api_endpoint}/v1/api/orchestration/list\"\n",
    "resp=requests.post(url, headers=headers, json={'workspace_id':wsid})\n",
    "assert resp.status_code == 200\n",
    "assert resp.text == '[]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8969e6-7716-4ccc-b9cc-9654ba94a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=f\"{wl.api_endpoint}/v1/api/orchestration/list\"\n",
    "resp=requests.post(url, headers=headers)\n",
    "#assert resp.status_code == 422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dde79b3-3801-41bc-be23-626ccbcfa52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa134725-b12e-4804-b980-ee0c3625c777",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"workspace_id\": wsid, \"orch_id\": orchid, \"json\": {\"aa\":\"bb\"}}\n",
    "url=f\"{wl.api_endpoint}/v1/api/orchestration/task/run_once\"\n",
    "resp=requests.post(url, headers=headers, json=data)\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da71e08e-001a-4cbd-8e9a-405ecf2613b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.list_workspaces()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37837d76-1f25-4238-8cd0-49be8f64d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.get_current_workspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76b34c-cf7d-4bc7-b2cd-53f2ff24dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.list_orchestrations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25433090-aed9-484b-add3-a988a2e8c830",
   "metadata": {},
   "outputs": [],
   "source": [
    "orch = wl.list_orchestrations()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ed23b7-c7de-4eb6-8e51-57e203f48ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "orch.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e268fb-48ed-4494-82b5-e722f762979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = orch.run_once(json={\"hello\": 34})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
