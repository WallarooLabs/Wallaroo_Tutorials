{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Edge Demo\n",
    "\n",
    "This notebook will explore \"Edge ML\", meaning deploying a model intended to be run on \"the edge\". What is \"the edge\"?\n",
    "The meaning varies depending upon whom you ask, but very broadly it means \"not in a large data center\" and typically in a\n",
    "somewhat resource (CPU, memory, and/or bandwidth) constrained environment or where a combination of latency requirements\n",
    "and bandwidth available requires the models to run locally. \n",
    "\n",
    "Wallaroo provides two key capabilities when it comes to deploying models to edge devices:\n",
    "1. Since the same engine is used in both environments, the model behavior can often be simulated accurately using Wallaroo\n",
    "in a data center for testing prior to deployment. This notebook demonstrates how.\n",
    "2. Wallaroo makes edge deployments \"observable\" so the same tools used to monitor model performance can be used in both\n",
    "kinds of deployments. \n",
    "\n",
    "This notebook closely parallels the Aloha tutorial. That is intentional: we want testing an edge deployment to be nearly\n",
    "the same as anything else. The primary difference is instead of provide ample resources to a pipeline to allow high-throughput\n",
    "operation we will specify a resource budget matching what is expected in the final deployment. Then we can apply the expected load\n",
    "to the model and observe how it behaves given the available resources.\n",
    "\n",
    "We will be using an open source model that uses an [Aloha CNN LSTM model](https://www.researchgate.net/publication/348920204_Using_Auxiliary_Inputs_in_Deep_Learning_Models_for_Detecting_DGA-based_Domain_Names) for classifiying Domain names as being either legitimate or being used for nefarious purposes such as malware distribution. This could be deployed on a network\n",
    "router to detect suspicious domains in real-time. Of course, it is important to monitor the behavior of the model across\n",
    "all of the deployments so we can see if the detect rate starts to drift over time.\n",
    "\n",
    "For our example, we will perform the following:\n",
    "\n",
    "* Create a workspace for our work.\n",
    "* Upload the Aloha model.\n",
    "* Define a resource budget for our inference pipeline.\n",
    "* Create a pipeline that can ingest our submitted data, submit it to the model, and export the results\n",
    "* Run a sample inference through our pipeline by loading a file\n",
    "* Run a sample inference through our pipeline's URL and store the results in a file.\n",
    "\n",
    "All sample data and models are available through the [Wallaroo Quick Start Guide Samples repository](https://github.com/WallarooLabs/quickstartguide_samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open a Connection to Wallaroo\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log into the following URL in a web browser:\n",
      "\n",
      "\thttps://yellow-platypus-9801.keycloak.wallaroo.community/auth/realms/master/device?user_code=NACK-RXGV\n",
      "\n",
      "Login successful!\n"
     ]
    }
   ],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Workspace\n",
    "\n",
    "We will create a workspace to work in and call it the \"alohaworkspace\", then set it as current workspace environment.  We'll also create our pipeline in advance as `alohapipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_name = 'alohaworkspace'\n",
    "pipeline_name = 'alohapipeline'\n",
    "model_name = 'alohamodel'\n",
    "model_file_name = './aloha-cnn-lstm.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workspace(name):\n",
    "    wl = wallaroo.Client()\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "def get_pipeline(name):\n",
    "    wl = wallaroo.Client()\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(pipeline_name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(pipeline_name)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = get_workspace(workspace_name)\n",
    "\n",
    "wl.set_current_workspace(workspace)\n",
    "\n",
    "aloha_pipeline = get_pipeline(pipeline_name)\n",
    "aloha_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the workspace is created the current default workspace with the `get_current_workspace()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'aloha-workspace', 'id': 6, 'archived': False, 'created_by': '7dbb3754-4c14-4730-8b77-33caeea7a2a0', 'created_at': '2022-03-29T16:14:08.85824+00:00', 'models': [], 'pipelines': []}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.get_current_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Models\n",
    "\n",
    "Now we will upload our models.  Note that for this example we are applying the model from a .ZIP file.  The Aloha model is a [protobuf](https://developers.google.com/protocol-buffers) file that has been defined for evaluating web pages, and we will configure it to use data in the `tensorflow` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wl.upload_model(model_name, model_file_name).configure(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the resource budget\n",
    "The DeploymentConfig object specifies the resources to allocate for a model pipeline. In this case, we're going to\n",
    "model a fairly small edge device where we've allocated 300 MB of RAM and a max of 1 CPU core to the model as might\n",
    "be available in some network routers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(4).memory(\"7Gi\").build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model\n",
    "Now that we have a model that we want to use we will create a deployment for it using the resource limits defined above. \n",
    "\n",
    "We will tell the deployment we are using a tensorflow model and give the deployment name and the configuration we want for the deployment.\n",
    "\n",
    "To do this, we'll create our pipeline that can ingest the data, pass the data to our Aloha model, and give us a final output.  We'll call our pipeline `aloha-test-demo`, then deploy it so it's ready to receive data.  The deployment process usually takes about 45 seconds.\n",
    "\n",
    "* **Note**:  If you receive an error that the pipeline could not be deployed because there are not enough resources, undeploy any other pipelines and deploy this one again.  This command can quickly undeploy all pipelines to regain resources.  We recommend **not** running this command in a production environment since it will cancel any running pipelines:\n",
    "\n",
    "```python\n",
    "for p in wl.list_pipelines(): p.undeploy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 45s ....... ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'aloha-test-demo', 'create_time': datetime.datetime(2022, 3, 29, 16, 15, 31, 638290, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'aloha-2', 'version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde', 'sha': 'fd998cd5e4964bbbb4f8d29d245a8ac67df81b62be767afbceb96a03d1a01520'}]}}]\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.add_model_step(model)\n",
    "aloha_pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the pipeline is running and list what models are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': None,\n",
       " 'engines': [{'ip': '10.12.1.236',\n",
       "   'name': 'engine-864d86d898-k26hv',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'aloha-test-demo',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'aloha-2',\n",
       "      'version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde',\n",
       "      'sha': 'fd998cd5e4964bbbb4f8d29d245a8ac67df81b62be767afbceb96a03d1a01520',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.12.1.235',\n",
       "   'name': 'engine-lb-85846c64f8-dcj4f',\n",
       "   'status': 'Running',\n",
       "   'reason': None}]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interferences\n",
    "\n",
    "### Infer 1 row\n",
    "\n",
    "Now that the pipeline is deployed and our Aloha model is in place, we'll perform a smoke test to verify the pipeline is up and running properly.  We'll use the `infer_from_file` command to load a single encoded URL into the inference engine and print the results back out.\n",
    "\n",
    "The result should tell us that the tokenized URL is legitimate (0) or fraud (1).  This sample data should return close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InferenceResult({'check_failures': [],\n",
       "  'elapsed': 631348351,\n",
       "  'model_name': 'aloha-2',\n",
       "  'model_version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde',\n",
       "  'original_data': {'text_input': [[0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    0,\n",
       "                                    28,\n",
       "                                    16,\n",
       "                                    32,\n",
       "                                    23,\n",
       "                                    29,\n",
       "                                    32,\n",
       "                                    30,\n",
       "                                    19,\n",
       "                                    26,\n",
       "                                    17]]},\n",
       "  'outputs': [{'Float': {'data': [0.001519620418548584], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.9829147458076477], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.012099534273147583], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [4.7593468480044976e-05],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [2.0289742678869516e-05],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [0.0003197789192199707],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [0.011029303073883057], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.9975639581680298], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.010341644287109375], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.008038878440856934], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.016155093908309937], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.006236225366592407], 'dim': [1, 1], 'v': 1}},\n",
       "              {'Float': {'data': [0.0009985864162445068],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [1.7933435344117743e-26],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}},\n",
       "              {'Float': {'data': [1.388984431455466e-27],\n",
       "                         'dim': [1, 1],\n",
       "                         'v': 1}}],\n",
       "  'pipeline_name': 'aloha-test-demo',\n",
       "  'time': 1648570552486})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.infer_from_file(\"data-1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Inference\n",
    "\n",
    "Now that our smoke test is successfully, let's really give it some data.  We have two inference files we can use:\n",
    "\n",
    "* `data-1k.json`:  Contains 1,0000 inferences\n",
    "* `data-25k.json`: Contains 25,000 inferences\n",
    "\n",
    "We'll pipe the `data-25k.json` file through the `aloha_pipeline` deployment URL, and place the results in a file named `response.txt`.  We'll also display the time this takes.  Note that for larger batches of 50,000 inferences or more can be difficult to view in Juypter Hub because of its size.\n",
    "\n",
    "When running this example, replace the URL from the `_deployment._url()` command into the `curl` command below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://engine-lb.aloha-test-demo-5:29502/pipelines/aloha-test-demo'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline._deployment._url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12.9M  100 10.1M  100 2886k   539k   149k  0:00:19  0:00:19 --:--:-- 2570k\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://engine-lb.aloha-test-demo-5:29502/pipelines/aloha-test-demo -H \"Content-Type:application/json\" --data @data-25k.json > curl_response.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undeploy Pipeline\n",
    "\n",
    "When finished with our tests, we will undeploy the pipeline so we have the Kubernetes resources back for other tasks.  Note that if the deployment variable is unchanged aloha_pipeline.deploy() will restart the inference engine in the same configuration as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'aloha-test-demo', 'create_time': datetime.datetime(2022, 3, 29, 16, 15, 31, 638290, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'aloha-2', 'version': '496e6860-a658-4d35-8b55-0f8cc6ad6fde', 'sha': 'fd998cd5e4964bbbb4f8d29d245a8ac67df81b62be767afbceb96a03d1a01520'}]}}]\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
