{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b8d897-d813-45eb-ac1e-133c5a4f67a4",
   "metadata": {},
   "source": [
    "This tutorial demonstrates deploying the [LLamav2 Large Language Model (LLM)](https://ai.meta.com/llama/) model to Wallaroo and performing inferences through it.\n",
    "\n",
    "This demonstrations takes the Llama V2 model and wraps it in a [Wallaroo BYOP (Bring Your Own Predict)](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/) framework.  This allows ML models outside of the standard [Wallaroo Native and Wallaroo Containerized Runtimes](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) to be deployed in a Wallaroo pipeline.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* A Wallaroo Ops instance Version 2023.4 and above.\n",
    "* A nodepool with at least 1 GPU.  See [Create GPU Nodepools for Kubernetes Clusters](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-install-guides/wallaroo-install-configurations/wallaroo-gpu-nodepools/) for instructions on setting up a nodepool with GPU virtual machines.\n",
    "* The BYOP version of the LLamav2 model.  The total size of this model is 20 GB.  If needed, review the [Manage Minio Storage for Models Storage](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-minio-storage/) guide for instructions on increasing the model storage capacity in your Wallaroo Ops instance.  The model is available through the following link.  Store this model in the `./models` directory.\n",
    "  * [https://storage.googleapis.com/wallaroo-public-data/llm-models/model-auto-conversion_BYOP_llama_byop_llamav2_new2.zip]https://storage.googleapis.com/wallaroo-public-data/llm-models/model-auto-conversion_BYOP_llama_byop_llamav2_new2.zip\n",
    "\n",
    "## Tutorial Steps\n",
    "\n",
    "This tutorial follows this process:\n",
    "\n",
    "* Connect to the Wallaroo Ops instance.\n",
    "* Create a workspace.\n",
    "* Upload the model.\n",
    "* Create a pipeline and deploy it.\n",
    "* Perform a sample inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50de070e",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "The first step will be to import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79633d11-605e-47ca-99df-bdf7c08c1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import wallaroo\n",
    "\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff919888-7071-45e2-ac3d-c9c36dcc52fa",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f5dc229-67f8-4bea-bc7d-75b9e6fe616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client(auth_type=\"sso\", interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abd8196",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "We will create a workspace to manage our pipeline and models.  The following variables will set the name of our sample workspace then set it as the current workspace.\n",
    "\n",
    "Workspace names must be unique.  The following helper function will either create a new workspace, or retrieve an existing one with the same name.  Verify that a pre-existing workspace has been shared with the targeted user.\n",
    "\n",
    "Set the variables `workspace_name` to ensure a unique workspace name if required.\n",
    "\n",
    "The workspace will then be set as the Current Workspace.  Model uploads and pipeline creation through the SDK are set in the current workspace.\n",
    "\n",
    "* References\n",
    "  * [Wallaroo SDK Essentials Guide: Workspace Management](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-workspace/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26ad71b3-eede-4a61-aae4-33f40cd1546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWorkspace(wl, ws_name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == ws_name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(ws_name)\n",
    "    return workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2ca0590-c2f3-4a76-8e31-598c12500737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'llama-models', 'id': 8, 'archived': False, 'created_by': 'e3c9f02f-988a-4097-8cc0-370fd3d629fa', 'created_at': '2024-01-16T02:45:53.745255+00:00', 'models': [{'name': 'llama-chat', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 1, 16, 2, 56, 32, 639430, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 1, 16, 2, 56, 32, 639430, tzinfo=tzutc())}, {'name': 'llama-rag', 'versions': 2, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 1, 17, 0, 25, 7, 661947, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 1, 16, 13, 30, 8, 549780, tzinfo=tzutc())}], 'pipelines': [{'name': 'llamav2-pipe', 'create_time': datetime.datetime(2024, 1, 16, 3, 5, 59, 758313, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'rag-llamav2-pipe', 'create_time': datetime.datetime(2024, 1, 16, 13, 42, 39, 879093, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llamav2-chat-1', 'create_time': datetime.datetime(2024, 1, 16, 14, 24, 40, 398877, tzinfo=tzutc()), 'definition': '[]'}]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace_name = \"llama-models\"\n",
    "ws = getWorkspace(wl, workspace_name)\n",
    "wl.set_current_workspace(ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_id = workspace.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2029ba9b",
   "metadata": {},
   "source": [
    "### Upload Model\n",
    "\n",
    "The model is uploaded as a BYOP model, where the model, Python script and other artifacts are included in a .zip file.  This requires the input and output schemas for the model specified in Apache Arrow Schema format.\n",
    "\n",
    "The following method will use the Wallaroo API to upload the model with its relevant input/output schemas.  Because of the size of this model, it may take anywhere from 15 to 45 minutes to upload, depending on the speed of your connection.\n",
    "\n",
    "* References\n",
    "  * [Wallaroo SDK Essentials Guide: Model Uploads and Registrations: Arbitrary Python](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683fb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using curl to upload the file.\n",
    "\n",
    "metadata = { \n",
    "    \"name\": \"llama-chat\",\n",
    "    \"visibility\": \"private\",\n",
    "    \"workspace_id\": workspace_id,\n",
    "    \"conversion\": {\n",
    "        \"framework\": \"custom\", \n",
    "        \"python_version\": \"3.8\", \n",
    "        \"requirements\": [], \n",
    "        \"input_schema\": \"/////3AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAQAAAB0ZXh0AAAAAAQABAAEAAAA\", \n",
    "        \"output_schema\": \"/////7AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAEAAAAwP///wAAARAUAAAALAAAAAQAAAABAAAAOAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAAAAAGAAgABAAGAAAAAQAAABAAFAAIAAYABwAMAAAAEAAQAAAAAAABBRAAAAAcAAAABAAAAAAAAAAEAAAAaXRlbQAAAAAEAAQABAAAAA==\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# save metadata to a file\n",
    "with open(\"./data/file_upload.json\", \"w\") as outfile:\n",
    "    json.dump(metadata, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1bb46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use once to upload the model\n",
    "\n",
    "!curl {wl.api_endpoint}/v1/api/models/upload_and_convert \\\n",
    "  -H \"Authorization: {wl.auth.auth_header()['Authorization']}\" \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"metadata=@./data/file_upload.json;type=application/json\" \\\n",
    "  -F \"file=@models/model-auto-conversion_BYOP_llama_byop_llamav2_new2.zip;type=application/octet-stream\" \\\n",
    "  --progress-bar | cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4817b",
   "metadata": {},
   "source": [
    "### Retrieve Model Version\n",
    "\n",
    "We now retrieve the model version using the Wallaroo SDK.  This reference is used for the deployment steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653dfdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = workspace.models()[-1].versions()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b5e43",
   "metadata": {},
   "source": [
    "### Deploy Pipeline\n",
    "\n",
    "Next we configure the hardware we want to use for deployment. If we plan on eventually deploying to edge, this is a good way to simulate edge hardware conditions.  The BYOP model is deployed as a Wallaroo Containerized Runtime, so the hardware allocation is performed through the `sidekick` options.\n",
    "\n",
    "* References\n",
    "  * [Wallaroo SDK Essentials Guide: Pipeline Deployment Configuration](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline-deployment-config/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e94f07-f8c1-4494-9649-a9bc1019d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('1Gi') \\\n",
    "    .sidekick_gpus(model, 1) \\\n",
    "    .deployment_label('wallaroo.ai/gpu: a100') \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e9da0-8fb1-4d1b-b1a8-bf855ad9026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"llamav2-chat-1\"\n",
    "llamav2_pipe = wl.build_pipeline(pipeline_name)\n",
    "llamav2_pipe.add_model_step(model)\n",
    "\n",
    "llamav2_pipe.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9930f992-90ef-4076-8e40-77f253dd2b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.244.4.17',\n",
       "   'name': 'engine-b44578ccb-mcvth',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'llamav2-chat-1',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'llama-chat',\n",
       "      'version': '12c993cb-2b6c-4eaf-9cd7-099f4164d68c',\n",
       "      'sha': '23c11e89fb3d3fe6e48f8817754a64e326d6d9ed9cd3cbdc0784cd48e684d4cc',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.244.0.203',\n",
       "   'name': 'engine-lb-5df9b487cf-z62kz',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.244.4.16',\n",
       "   'name': 'engine-sidekick-llama-chat-1-87c574d49-f6d2q',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llamav2_pipe.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52c34e3-2b76-4ec3-829c-fe986e21805e",
   "metadata": {},
   "source": [
    "### Testing Llama\n",
    "\n",
    "We will now test our LlamaV2 model with a simple request, and display the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ee2a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.DataFrame({'text': ['Describe Virgin Australia']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d6b3619-0084-4954-812c-4b4362c71d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Describe Virgin Australia's (VA) Fleet Structure\\n\\nVirgin Australia (VA) operates a diverse fleet of aircraft, with a mix of narrow-body, wide-body, and regional jets. Here is a brief overview of VA's current fleet structure:\\n\\n1. Narrow-body aircraft:\\n\\t* Airbus A320-200: 35 aircraft (used for short-haul flights within Australia and to nearby countries)\\n\\t* Airbus A321-200: 10 aircraft (used for long-haul flights within Australia and to nearby countries)\\n2. Wide-body aircraft:\\n\\t* Boeing 777-300ER: 15 aircraft (used for long-haul flights to destinations in Asia, Europe, and the United States)\\n\\t* Airbus A330-200: 5 aircraft (used for long-haul flights to destinations in Asia and the Pacific)\\n3. Regional jets:\\n\\t* Bombardier Q400: 10 aircraft (used for short-haul flights within Australia)\\n\\t* Fokker 100: 5 aircraft (used for short-haul flights within Australia)\\n4. Future fleet plans:\\n\\t* Virgin Australia has announced plans to retire its Boeing 737-800 and 737-400 aircraft and replace them with new Airbus A320neos and A330neos.\\n\\t* The airline has also ordered 15 Airbus A220-300 aircraft, which will be delivered from 2020.\\n\\nVA's fleet structure is designed to meet the demand for domestic and international travel within Australia and to nearby countries. The airline's narrow-body aircraft are used for short-haul flights, while its wide-body aircraft are used for long-haul flights to more distant destinations. The regional jets are used for shorter flights within Australia.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = llamav2_pipe.infer(input_df)\n",
    "out[\"out.generated_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d75af0",
   "metadata": {},
   "source": [
    "### Undeploy Pipeline\n",
    "\n",
    "With the demonstartion complete, we undeploy the pipeline and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169b96b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
