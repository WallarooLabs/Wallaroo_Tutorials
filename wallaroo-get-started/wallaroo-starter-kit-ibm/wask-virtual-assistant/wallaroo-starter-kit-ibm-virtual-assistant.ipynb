{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b935cdb3",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.2.2_tutorials/wallaroo-get-started/wallaroo-starter-kit-ibm/wask-virtual-assistant).\n",
    "\n",
    "The following is a demonstration of downloading, deploying, and inferring from a RAG based virtual assistant from the [Wallaroo AI Starter Kit for IBM Power](https://docs.wallaroo.ai/wallaroo-free/wallaroo-ai-start-kit-ibm-power/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b82c831",
   "metadata": {},
   "source": [
    "## Wallaroo AI Starter Kit for IBM: Virtual Assistant Deployment Guide\n",
    "\n",
    "This tutorial functions as a practical example of using the [Wallaroo AI Starter Kit for IBM Power](https://docs.wallaroo.ai/wallaroo-free/wallaroo-ai-start-kit-ibm-power/) to deploy the Virtual Assistant model in an IBM Logical Partition (LPAR).\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting, verify that the [Wallaroo AI Start Kit LPAR (Logical Partition) Prerequisites](https://docs.wallaroo.ai/wallaroo-free/wallaroo-ai-start-kit-ibm-power/wallaroo-start-kit-ibm-lpar/) are complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b2ba2",
   "metadata": {},
   "source": [
    "## Procedure\n",
    "\n",
    "### Retrieve the Deployment Command\n",
    "\n",
    "1. Proceed to the [Wallaroo AI Starter Kit URL](https://ai-catalog-kit.lovable.app/).\n",
    "1. Select the **Model Card** for the **Wallaroo AI Starter Kit - Virtual Assistant** and copy the **Deployment Command**.\n",
    "\n",
    "   {{<figure src=\"/images/2025.2/wallaroo-start-here/wask-ibm/wask-ibm-card.png\" width=\"800\" label=\"Sample Start Kit Model Card\">}}\n",
    "\n",
    "   The following shows an example of this command.\n",
    "\n",
    "   ```bash\n",
    "   podman run \\\n",
    "       -p $EDGE_PORT:8080 \\\n",
    "       -e OCI_USERNAME=\"wallarooai+wallaroo_model_catalog\" \\\n",
    "       -e OCI_PASSWORD=\"$GENERATED_TOKEN\" \\\n",
    "       -e SYSTEM_PROMPT=\"$SYSTEM_PROMPT\" \\\n",
    "       -e MODEL_CONTEXT_FILENAME=\"/$(basename $MODEL_CONTEXT_FILENAME)\" \\\n",
    "       -v $MODEL_CONTEXT_FILENAME:/$(basename $MODEL_CONTEXT_FILENAME) \\\n",
    "       -e PIPELINE_URL=quay.io/wallarooai/wask/wask-va-pipeline:bfb42a37-45ea-4d99-8ebf-c37a1de7dd70 \\\n",
    "       -e CONFIG_CPUS=1.0 --cpus=15.0 --memory=20g \\\n",
    "       quay.io/wallarooai/wask/fitzroy-mini-ppc64le:v2025.2.0-6480\n",
    "   ```\n",
    "\n",
    "1. Retrieve the Generated Token by selecting **Generate Token** next to the **Deploy Command**.  If this token is lost, return to the Wallaroo AI Starter Kit page and generate a new token.\n",
    "\n",
    "### Set the Deployment Command\n",
    "\n",
    "Login to the LPAR through a terminal shell - for example, `ssh`.\n",
    "\n",
    "1. Set the following variables:\n",
    "   1. `OCI_USERNAME`:  The Quay.io username, set to `wallarooai+wallaroo_model_catalog`.\n",
    "   1. `OCI_PASSWORD`:  This is set to the generated token retrieved from the previous step.\n",
    "   2. `EDGE_PORT`:  The external IP port used to make inference requests.  Verify this port is open and accessible from the requesting systems.\n",
    "   3. `SYSTEM_PROMPT`:  This is a system prompt, for example: \"Answer the question in 10 words or less using the following context\".\n",
    "   4. `MODEL_CONTEXT_FILENAME`: This is a reference to a JSON file.\n",
    "2. Create the `MODEL_CONTEXT_FILENAME` file in the same path specified (for example:  `context.json`).  This file is in the following format:\n",
    "\n",
    "   ```bash\n",
    "   {\n",
    "     \"type\": \"array\",\n",
    "     \"items\": {\n",
    "       \"type\": \"object\",\n",
    "       \"properties\": {\n",
    "         \"question\": {\n",
    "           \"type\": \"string\",\n",
    "           \"description\": \"The question being asked\"\n",
    "         },\n",
    "         \"answer\": {\n",
    "           \"type\": \"string\",\n",
    "           \"description\": \"The answer to the question\"\n",
    "         }\n",
    "       },\n",
    "       \"required\": [\"question\", \"answer\"]\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "\n",
    "   The following is an example of this file contents:\n",
    "\n",
    "   ```bash\n",
    "   [\n",
    "       {\n",
    "           \"question\": \"How do I launch DX+\",\n",
    "           \"answer\": \"You can launch DX+ by opening the DX+ application from your desktop or web portal and signing in using your credentials.\"\n",
    "       },\n",
    "       {\n",
    "           \"question\": \"What credentials do I use to sign into DX+\",\n",
    "           \"answer\": \"Use your assigned company username and password to sign into DX+.\"\n",
    "       },\n",
    "       {\n",
    "           \"question\": \"What should I do if I forget my DX+ password\",\n",
    "           \"answer\": \"Click the 'Forgot Password' link on the login screen or contact your system administrator to reset it.\"\n",
    "       }\n",
    "   ]\n",
    "   ```\n",
    "\n",
    "The following shows an example of these values declared in the command:\n",
    "\n",
    "```bash\n",
    "podman run \\\n",
    "    -p 3030:8080 \\\n",
    "    -e SYSTEM_PROMPT=\"Answer the question in 10 words or less using the following context\" \\\n",
    "    -e MODEL_CONTEXT_FILENAME=\"./context.json\" \\\n",
    "    -v $MODEL_CONTEXT_FILENAME:./contest.json \\\n",
    "    -e PIPELINE_URL=quay.io/wallarooai/wask/wask-va-pipeline:bfb42a37-45ea-4d99-8ebf-c37a1de7dd70 \\\n",
    "    -e CONFIG_CPUS=1.0 --cpus=15.0 --memory=20g \\\n",
    "    quay.io/wallarooai/wask/fitzroy-mini-ppc64le:v2025.2.0-6480\n",
    "```\n",
    "\n",
    "Once ready, deploy the model by running the updated Deploy Command for your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee51be6",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Inference requests are made by submitting Apache Arrow tables or Pandas Tables in Record Format as JSON.\n",
    "\n",
    "The Inference URL is in the format:\n",
    "\n",
    "```python\n",
    "$HOSTNAME:$PORT/infer\n",
    "```\n",
    "\n",
    "For example, if the hostname is `localhost` and the port is `3030`, the Inference URL is:\n",
    "\n",
    "```python\n",
    "localhost:3030/infer\n",
    "```\n",
    "\n",
    "The following shows an example of performing the inference request on the deployed Virtual Assistant via the `curl` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51057ef6-ed6b-49e6-8d75-1cbb16767894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[{\"time\":1768252715611,\"in\":{\"query\":\"How do I launch DX+?\"},\"out\":{\"generated_text\":\"Open the DX+ application from your desktop or web portal and sign in using your credentials.\"},\"anomaly\":{\"count\":0},\"metadata\":{\"last_model\":\"{\"model_name\":\"wask-qa-bot-byop-model\",\"model_sha\":\"175e19b1cabd8a05c7888c43f4bff90b4442be407d89364826505726d53d347b\"}\",\"pipeline_version\":\"bfb42a37-45ea-4d99-8ebf-c37a1de7dd70\",\"elapsed\":[6309508,41709268743],\"dropped\":[],\"partition\":\"b6578b9c5d36\"}}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl POST localhost:3030/infer \\\n",
    "   -H \"Content-Type: application/json\" \\\n",
    "   -v --data '[{\"query\":\"How do I launch DX+?\"}]'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2025.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
