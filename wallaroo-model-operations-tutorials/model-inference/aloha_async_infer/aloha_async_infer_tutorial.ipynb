{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.1_tutorials/wallaroo-model-operations-tutorials/model-inference/aloha_async_infer).\n",
    "\n",
    "## Async Infer Tutorial\n",
    "\n",
    "This tutorial demonstrates using async infer through the Wallaroo SDK.  The method `async wallaroo.pipeline.Pipeline.async_infer` provides asynchronous inference requests to a deployed model.  This allows organizations to submit inference requests, proceed with other tasks, then respond when the inference request is complete.  For more details, see [Run Asynchronous Inference through Local Variable](https://staging.docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-serve/#run-asynchronous-inference-through-local-variable)\n",
    "\n",
    "In this notebook we will walk through a simple pipeline deployment, then use `async_infer` inference on a model. For this example we will be using an open source model that uses an [Aloha CNN LSTM model](https://www.researchgate.net/publication/348920204_Using_Auxiliary_Inputs_in_Deep_Learning_Models_for_Detecting_DGA-based_Domain_Names) for classifying Domain names as being either legitimate or being used for nefarious purposes such as malware distribution.  \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* An installed Wallaroo instance.\n",
    "* The following Python libraries installed:\n",
    "  * `os`\n",
    "  * [`wallaroo`](https://pypi.org/project/wallaroo/): The Wallaroo SDK. Included with the Wallaroo JupyterHub service by default.\n",
    "  * [`pandas`](https://pypi.org/project/pandas/): Pandas, mainly used for Pandas DataFrame\n",
    "  * [`pyarrow`](https://pypi.org/project/pyarrow/): PyArrow for Apache Arrow support\n",
    "  * [`httpx`]:  Provides `async_infer` with the `AsyncClient` object.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "For our example, we will perform the following:\n",
    "\n",
    "* Create a workspace for our work.\n",
    "* Upload the Aloha model.\n",
    "* Create a pipeline that can ingest our submitted data, submit it to the model, and export the results\n",
    "* Run a sample async through a pandas DataFrame.\n",
    "* Run a sample async through an Apache Arrow Table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "\n",
    "# to display dataframe tables\n",
    "from IPython.display import display\n",
    "# used to display dataframe information without truncating\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "import pyarrow as pa\n",
    "import asyncio\n",
    "\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login through local Wallaroo instance\n",
    "\n",
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Workspace\n",
    "\n",
    "We will create a workspace to work in and call it the \"alohaworkspace\", then set it as current workspace environment.  We'll also create our pipeline in advance as `alohapipeline`.  The model name and the model file will be specified for use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_name = f'alohaworkspace'\n",
    "pipeline_name = f'alohapipeline'\n",
    "model_name = f'alohamodel'\n",
    "model_file_name = './alohacnnlstm.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>alohapipeline</td></tr><tr><th>created</th> <td>2024-12-09 21:42:13.603663+00:00</td></tr><tr><th>last_updated</th> <td>2024-12-12 17:09:53.517499+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>13</td></tr><tr><th>workspace_name</th> <td>alohaworkspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>8dbee118-19fd-4f88-ac73-15a7e779c1c9, 0c3a6584-a1cb-4606-9439-0e6a6a345da4, a5d47ffc-eb31-4896-83de-ad814c6eda94, c7d48fe3-4072-4168-bb0a-6bd6b9154cf6, bce5c7d8-d70d-407e-af2a-d5ccbcd499a7</td></tr><tr><th>steps</th> <td>alohamodel</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'alohapipeline', 'create_time': datetime.datetime(2024, 12, 9, 21, 42, 13, 603663, tzinfo=tzutc()), 'definition': '[]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = wl.get_workspace(name=workspace_name, create_if_not_exist=True)\n",
    "\n",
    "wl.set_current_workspace(workspace)\n",
    "\n",
    "aloha_pipeline = wl.build_pipeline(pipeline_name)\n",
    "aloha_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the workspace is created the current default workspace with the `get_current_workspace()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'alohaworkspace', 'id': 13, 'archived': False, 'created_by': '51aefe14-5431-4c0f-9427-10b5f5d75def', 'created_at': '2024-12-09T21:42:13.449487+00:00', 'models': [{'name': 'alohamodel', 'versions': 2, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 12, 9, 22, 51, 43, 89350, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 12, 9, 21, 42, 16, 359633, tzinfo=tzutc())}], 'pipelines': [{'name': 'alohapipeline', 'create_time': datetime.datetime(2024, 12, 9, 21, 42, 13, 603663, tzinfo=tzutc()), 'definition': '[]'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.get_current_workspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Models\n",
    "\n",
    "Now we will upload our models.  Note that for this example we are applying the model from a .ZIP file.  The Aloha model is a [protobuf](https://developers.google.com/protocol-buffers) file that has been defined for evaluating web pages, and we will configure it to use data in the `tensorflow` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wallaroo.framework import Framework\n",
    "\n",
    "model = wl.upload_model(model_name, \n",
    "                        model_file_name,\n",
    "                        framework=Framework.TENSORFLOW\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy a model\n",
    "\n",
    "Now that we have a model that we want to use we will create a deployment for it. \n",
    "\n",
    "We will tell the deployment we are using a tensorflow model and give the deployment name and the configuration we want for the deployment.\n",
    "\n",
    "To do this, we'll create our pipeline that can ingest the data, pass the data to our Aloha model, and give us a final output.  We'll call our pipeline `aloha-test-demo`, then deploy it so it's ready to receive data.  The deployment process usually takes about 45 seconds.\n",
    "\n",
    "* **Note**:  If you receive an error that the pipeline could not be deployed because there are not enough resources, undeploy any other pipelines and deploy this one again.  This command can quickly undeploy all pipelines to regain resources.  We recommend **not** running this command in a production environment since it will cancel any running pipelines:\n",
    "\n",
    "```python\n",
    "for p in wl.list_pipelines(): p.undeploy()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>alohapipeline</td></tr><tr><th>created</th> <td>2024-12-09 21:42:13.603663+00:00</td></tr><tr><th>last_updated</th> <td>2024-12-12 17:09:53.517499+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>13</td></tr><tr><th>workspace_name</th> <td>alohaworkspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>8dbee118-19fd-4f88-ac73-15a7e779c1c9, 0c3a6584-a1cb-4606-9439-0e6a6a345da4, a5d47ffc-eb31-4896-83de-ad814c6eda94, c7d48fe3-4072-4168-bb0a-6bd6b9154cf6, bce5c7d8-d70d-407e-af2a-d5ccbcd499a7</td></tr><tr><th>steps</th> <td>alohamodel</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'alohapipeline', 'create_time': datetime.datetime(2024, 12, 9, 21, 42, 13, 603663, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'alohamodel', 'version': '2950de9a-6eff-49f4-ade9-585f53b1a8db', 'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8'}]}}]\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.add_model_step(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment initiated for alohapipeline. Please check pipeline status.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>alohapipeline</td></tr><tr><th>created</th> <td>2024-12-09 21:42:13.603663+00:00</td></tr><tr><th>last_updated</th> <td>2024-12-12 17:09:57.980612+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>workspace_id</th> <td>13</td></tr><tr><th>workspace_name</th> <td>alohaworkspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>cfc1994c-c857-4b21-9d56-6ea2e768f6e3, 8dbee118-19fd-4f88-ac73-15a7e779c1c9, 0c3a6584-a1cb-4606-9439-0e6a6a345da4, a5d47ffc-eb31-4896-83de-ad814c6eda94, c7d48fe3-4072-4168-bb0a-6bd6b9154cf6, bce5c7d8-d70d-407e-af2a-d5ccbcd499a7</td></tr><tr><th>steps</th> <td>alohamodel</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'alohapipeline', 'create_time': datetime.datetime(2024, 12, 9, 21, 42, 13, 603663, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'alohamodel', 'version': '2950de9a-6eff-49f4-ade9-585f53b1a8db', 'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8'}]}}]\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.deploy(wait_for_status=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the pipeline is running and list what models are associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error\n",
      "Error\n",
      "Starting\n",
      "Running\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# wait for the deployment to start\n",
    "time.sleep(15)\n",
    "\n",
    "while aloha_pipeline.status()['status'] != 'Running':\n",
    "    print(aloha_pipeline.status()['status'])\n",
    "    time.sleep(15)\n",
    "\n",
    "print(aloha_pipeline.status()['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interferences\n",
    "\n",
    "### Async Infer from Pandas DataFrame\n",
    "\n",
    "Now that the pipeline is deployed and our Aloha model is in place, we'll perform an async inference via the `async_infer` method.  We'll start with a single row Pandas DataFrame and show the results.\n",
    "\n",
    "We'll define our `AsyncClient` used for other example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>out.main</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-12-12 17:11:05.131</td>\n",
       "      <td>[0.997564]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time    out.main\n",
       "0 2024-12-12 17:11:05.131  [0.997564]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "async_client = httpx.AsyncClient()\n",
    "\n",
    "smoke_test = pd.DataFrame.from_records(\n",
    "    [\n",
    "    {\n",
    "        \"text_input\":[\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            28,\n",
    "            16,\n",
    "            32,\n",
    "            23,\n",
    "            29,\n",
    "            32,\n",
    "            30,\n",
    "            19,\n",
    "            26,\n",
    "            17\n",
    "        ]\n",
    "    }\n",
    "]\n",
    ")\n",
    "\n",
    "result_async = await aloha_pipeline.async_infer(tensor=smoke_test,\n",
    "                                          async_client=async_client,\n",
    "                                          retries=2)\n",
    "display(result_async.loc[:, [\"time\",\"out.main\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async Infer From Apache Arrow Table\n",
    "\n",
    "For this example, we'll use three data sources of Apache Arrow tables to simulate drawing data, submitting an inference, then drawing data from another source while the asynchronous inference executes.  When all async inferences are complete, we'll publish all the results at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rs/yt_dh9xn6y39_h0_jth1mjb40000gq/T/ipykernel_62825/736083679.py:15: RuntimeWarning: coroutine 'run_one_batch' was never awaited\n",
      "  tasks = []\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "async def run_one_batch(file):\n",
    "    with pa.ipc.open_file(file) as source:\n",
    "        table = source.read_all() # to get pyarrow table\n",
    "\n",
    "    # ... potentially do other processing here or request data from a feature store ...\n",
    "    \n",
    "    result_async_batch = await aloha_pipeline.async_infer(tensor=table,\n",
    "                                                      async_client=async_client,\n",
    "                                                      retries=2)\n",
    "    return result_async_batch\n",
    "\n",
    "# Process three batches in parallel. Inference on the first will start while\n",
    "# the second file is read, and so on. Multiple inferences may be running at\n",
    "# the same time if the engine replicas are > 1.\n",
    "tasks = []\n",
    "for file in [\"./data/data_25k.arrow\", \"./data/data_1k.arrow\", \"./data/data_25k.arrow\"]:\n",
    "     tasks.append(run_one_batch(file))\n",
    "\n",
    "# Wait for all three batches to finish.\n",
    "results = await asyncio.gather(*tasks, return_exceptions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for task: 24954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "time: timestamp[ms]\n",
       "in.text_input: list<item: float>\n",
       "  child 0, item: float\n",
       "out.banjori: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.corebot: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.cryptolocker: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.dircrypt: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.gozi: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.kraken: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.locky: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.main: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.matsnu: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.pykspa: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.qakbot: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.ramdo: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.ramnit: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.simda: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.suppobox: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "anomaly.count: uint32 not null\n",
       "----\n",
       "time: [[2024-12-12 17:13:09.498,2024-12-12 17:13:09.498,2024-12-12 17:13:09.498,2024-12-12 17:13:09.498,2024-12-12 17:13:09.498,...,2024-12-12 17:13:09.498,2024-12-12 17:13:09.498,2024-12-12 17:13:09.498,2024-12-12 17:13:09.498,2024-12-12 17:13:09.498]]\n",
       "in.text_input: [[[0,0,0,0,0,...,32,30,19,26,17],[0,0,0,0,0,...,18,35,18,22,23],...,[0,0,0,0,0,...,27,28,19,33,23],[0,0,0,0,0,...,24,29,14,36,13]]]\n",
       "out.banjori: [[[0.0015195814],[7.447168e-18],...,[1.342218e-14],[1.3068625e-12]]]\n",
       "out.corebot: [[[0.98291475],[6.7359245e-8],...,[7.969789e-9],[1.1029467e-9]]]\n",
       "out.cryptolocker: [[[0.012099549],[0.17081994],...,[0.14756858],[0.014839977]]]\n",
       "out.dircrypt: [[[0.000047591115],[1.3220122e-9],...,[4.813928e-9],[2.2757316e-8]]]\n",
       "out.gozi: [[[0.000020289312],[1.2758657e-24],...,[1.0541016e-22],[8.438438e-15]]]\n",
       "out.kraken: [[[0.00031977257],[0.22559547],...,[0.2969691],[0.30495816]]]\n",
       "out.locky: [[[0.011029262],[0.3420985],...,[0.2706808],[0.11627986]]]\n",
       "out.main: [[[0.997564],[0.99999994],...,[1],[0.99999803]]]\n",
       "..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for task: 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "time: timestamp[ms]\n",
       "in.text_input: list<item: float>\n",
       "  child 0, item: float\n",
       "out.banjori: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.corebot: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.cryptolocker: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.dircrypt: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.gozi: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.kraken: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.locky: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.main: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.matsnu: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.pykspa: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.qakbot: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.ramdo: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.ramnit: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.simda: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.suppobox: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "anomaly.count: uint32 not null\n",
       "----\n",
       "time: [[2024-12-12 17:13:08.808,2024-12-12 17:13:08.808,2024-12-12 17:13:08.808,2024-12-12 17:13:08.808,2024-12-12 17:13:08.808,...,2024-12-12 17:13:08.808,2024-12-12 17:13:08.808,2024-12-12 17:13:08.808,2024-12-12 17:13:08.808,2024-12-12 17:13:08.808]]\n",
       "in.text_input: [[[0,0,0,0,0,...,32,30,19,26,17],[0,0,0,0,0,...,29,12,36,31,12],...,[0,0,0,0,0,...,35,16,35,27,16],[0,0,0,0,0,...,24,29,14,36,13]]]\n",
       "out.banjori: [[[0.0015195814],[0.00002837503],...,[0.0000056315566],[1.3068625e-12]]]\n",
       "out.corebot: [[[0.98291475],[0.000012753118],...,[0.0000033642746],[1.1029468e-9]]]\n",
       "out.cryptolocker: [[[0.012099549],[0.025435215],...,[0.13612255],[0.014839977]]]\n",
       "out.dircrypt: [[[0.000047591115],[6.150966e-10],...,[5.6732154e-11],[2.2757316e-8]]]\n",
       "out.gozi: [[[0.000020289312],[2.321774e-10],...,[2.773063e-8],[8.43847e-15]]]\n",
       "out.kraken: [[[0.00031977257],[0.051351104],...,[0.0025221605],[0.30495816]]]\n",
       "out.locky: [[[0.011029262],[0.022038758],...,[0.05455697],[0.116279885]]]\n",
       "out.main: [[[0.997564],[0.9885122],...,[0.9998954],[0.99999803]]]\n",
       "..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows for task: 24954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "time: timestamp[ms]\n",
       "in.text_input: list<item: float>\n",
       "  child 0, item: float\n",
       "out.banjori: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.corebot: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.cryptolocker: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.dircrypt: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.gozi: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.kraken: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.locky: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.main: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.matsnu: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.pykspa: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.qakbot: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.ramdo: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.ramnit: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.simda: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "out.suppobox: list<inner: float not null> not null\n",
       "  child 0, inner: float not null\n",
       "anomaly.count: uint32 not null\n",
       "----\n",
       "time: [[2024-12-12 17:13:09.443,2024-12-12 17:13:09.443,2024-12-12 17:13:09.443,2024-12-12 17:13:09.443,2024-12-12 17:13:09.443,...,2024-12-12 17:13:09.443,2024-12-12 17:13:09.443,2024-12-12 17:13:09.443,2024-12-12 17:13:09.443,2024-12-12 17:13:09.443]]\n",
       "in.text_input: [[[0,0,0,0,0,...,32,30,19,26,17],[0,0,0,0,0,...,18,35,18,22,23],...,[0,0,0,0,0,...,27,28,19,33,23],[0,0,0,0,0,...,24,29,14,36,13]]]\n",
       "out.banjori: [[[0.0015195814],[7.447168e-18],...,[1.342218e-14],[1.3068625e-12]]]\n",
       "out.corebot: [[[0.98291475],[6.7359245e-8],...,[7.969789e-9],[1.1029467e-9]]]\n",
       "out.cryptolocker: [[[0.012099549],[0.17081994],...,[0.14756858],[0.014839977]]]\n",
       "out.dircrypt: [[[0.000047591115],[1.3220122e-9],...,[4.813928e-9],[2.2757316e-8]]]\n",
       "out.gozi: [[[0.000020289312],[1.2758657e-24],...,[1.0541016e-22],[8.438438e-15]]]\n",
       "out.kraken: [[[0.00031977257],[0.22559547],...,[0.2969691],[0.30495816]]]\n",
       "out.locky: [[[0.011029262],[0.3420985],...,[0.2706808],[0.11627986]]]\n",
       "out.main: [[[0.997564],[0.99999994],...,[1],[0.99999803]]]\n",
       "..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(f\"Number of rows for task: {result.num_rows}\")\n",
    "    display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undeploy Pipeline\n",
    "\n",
    "When finished with our tests, we will undeploy the pipeline so we have the Kubernetes resources back for other tasks.  Note that if the deployment variable is unchanged aloha_pipeline.deploy() will restart the inference engine in the same configuration as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>alohapipeline</td></tr><tr><th>created</th> <td>2024-12-09 21:42:13.603663+00:00</td></tr><tr><th>last_updated</th> <td>2024-12-12 17:09:57.980612+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>13</td></tr><tr><th>workspace_name</th> <td>alohaworkspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>cfc1994c-c857-4b21-9d56-6ea2e768f6e3, 8dbee118-19fd-4f88-ac73-15a7e779c1c9, 0c3a6584-a1cb-4606-9439-0e6a6a345da4, a5d47ffc-eb31-4896-83de-ad814c6eda94, c7d48fe3-4072-4168-bb0a-6bd6b9154cf6, bce5c7d8-d70d-407e-af2a-d5ccbcd499a7</td></tr><tr><th>steps</th> <td>alohamodel</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'alohapipeline', 'create_time': datetime.datetime(2024, 12, 9, 21, 42, 13, 603663, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'alohamodel', 'version': '2950de9a-6eff-49f4-ade9-585f53b1a8db', 'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8'}]}}]\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aloha_pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2024.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
