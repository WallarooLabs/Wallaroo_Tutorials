{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cc9341",
   "metadata": {},
   "source": [
    "# CLIP ViT-B/32 Transformer (from ER) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33e888-28a8-48cc-b418-2d95ad206d98",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8228fc-00ce-4770-a863-8985633605b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "wl = wallaroo.Client(auth_type=\"sso\", interactive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72de5f7-02ef-4b0b-a478-f6a21335270a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Configure & Upload Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3caa911",
   "metadata": {},
   "source": [
    "### Save ü§ó Hugging Face pipeline locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb16621-78b4-4003-85e2-803d7657cb6a",
   "metadata": {},
   "source": [
    "We will use the `openai/clip-vit-base-patch32` model for the `zero-shot-image-classification` pipeline task from the official `ü§ó Hugging Face` [hub](https://huggingface.co/openai/clip-vit-base-patch32). The model can be found in model zoo [here](https://storage.cloud.google.com/wallaroo-model-zoo/model-auto-conversion/hugging-face/dummy-pipelines/zero-shot-image-classification-pipeline.zip?authuser=0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e95b3f",
   "metadata": {},
   "source": [
    "You can create a `zero-shot-image-classification` pipeline with the aforementioned model and save it locally as follows:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\n",
    "    SupportedTasks.ZERO_SHOT_IMAGE_CLASSIFICATION,\n",
    "    model=\"openai/clip-vit-base-patch32\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "pipe.save_pretrained(\"clip-vit-base-patch-32/\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d36553-46ce-4715-b8d0-461c1b686926",
   "metadata": {},
   "source": [
    "> **Important:** You also have to download [pipeline_config.json](https://huggingface.co/openai/clip-vit-base-patch32/blob/main/preprocessor_config.json) and place it inside the same directory. That's an issue coming from `Hugging Face ü§ó`, that's not able to load the file properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ca64f",
   "metadata": {},
   "source": [
    "As a last step, you have to `zip` the saved pipeline as follows:\n",
    "\n",
    "```bash\n",
    "zip -r clip-vit-base-patch-32.zip clip-vit-base-patch-32/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197db1a5",
   "metadata": {},
   "source": [
    "### Get Framework for the `zero-shot-image-classification` pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58281a1e",
   "metadata": {},
   "source": [
    "Let's see what frameworks are supported via the `Framework` Enum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fff3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['onnx',\n",
       " 'tensorflow',\n",
       " 'python',\n",
       " 'keras',\n",
       " 'sklearn',\n",
       " 'pytorch',\n",
       " 'xgboost',\n",
       " 'hugging-face-feature-extraction',\n",
       " 'hugging-face-image-classification',\n",
       " 'hugging-face-image-segmentation',\n",
       " 'hugging-face-image-to-text',\n",
       " 'hugging-face-object-detection',\n",
       " 'hugging-face-question-answering',\n",
       " 'hugging-face-stable-diffusion-text-2-img',\n",
       " 'hugging-face-summarization',\n",
       " 'hugging-face-text-classification',\n",
       " 'hugging-face-translation',\n",
       " 'hugging-face-zero-shot-classification',\n",
       " 'hugging-face-zero-shot-image-classification',\n",
       " 'hugging-face-zero-shot-object-detection',\n",
       " 'hugging-face-sentiment-analysis',\n",
       " 'hugging-face-text-generation',\n",
       " 'custom']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e.value for e in Framework]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4c8b6",
   "metadata": {},
   "source": [
    "The appropriate one for the `zero-shot-image-classification` pipeline is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9c6bb6-1c92-4aeb-82f0-b37cbf487aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Framework.HUGGING_FACE_ZERO_SHOT_IMAGE_CLASSIFICATION: 'hugging-face-zero-shot-image-classification'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Framework.HUGGING_FACE_ZERO_SHOT_IMAGE_CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a6a089-7c58-4525-8af9-69552637d3d7",
   "metadata": {},
   "source": [
    "### Configure PyArrow Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a8480-293f-405b-a58f-b8e852dbe2ad",
   "metadata": {},
   "source": [
    "You can find more info on the available inputs under the [official source code](https://github.com/huggingface/transformers/blob/v4.28.1/src/transformers/pipelines/zero_shot_image_classification.py#L78) from `ü§ó Hugging Face`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585a544",
   "metadata": {},
   "source": [
    "> ‚ö†Ô∏è Every extra input specified in the schema will raise an error when running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef115ab9-9c73-4c28-aad8-e915a92a746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('inputs', # required, fixed image dimensions\n",
    "        pa.list_(\n",
    "            pa.list_(\n",
    "                pa.list_(\n",
    "                    pa.int64(),\n",
    "                    list_size=3\n",
    "                ),\n",
    "                list_size=640 \n",
    "            ),\n",
    "        list_size=480\n",
    "    )),\n",
    "    pa.field('candidate_labels', pa.list_(pa.string(), list_size=4)), # required, equivalent to `options` in the provided demo\n",
    "]) \n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field('score', pa.list_(pa.float64(), list_size=4)), # has to be same as number of candidate labels\n",
    "    pa.field('label', pa.list_(pa.string(), list_size=4)), # has to be same as number of candidate labels\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600b756",
   "metadata": {},
   "source": [
    "### Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "953e0857-45ec-4603-b0be-3bba13df5db8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model conversion... It may take up to 10.0min.\n",
      "Model is Pending conversion..Converting..............Ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>er-clip-vit</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>4564b16e-bab6-4c12-a014-6ff042051f1a</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>clip-vit-base-patch-32.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>4efc24685a14e1682301cc0085b9db931aeb5f3f8247854bedc6863275ed0646</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mlflow-deploy:v2023.2.1-3530</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2023-25-Oct 13:36:45</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'er-clip-vit', 'version': '4564b16e-bab6-4c12-a014-6ff042051f1a', 'file_name': 'clip-vit-base-patch-32.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mlflow-deploy:v2023.2.1-3530', 'last_update_time': datetime.datetime(2023, 10, 25, 13, 36, 45, 224405, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = wl.upload_model('er-clip-vit', 'clip-vit-base-patch-32.zip', framework=Framework.HUGGING_FACE_ZERO_SHOT_IMAGE_CLASSIFICATION, input_schema=input_schema, output_schema=output_schema)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbb0930-8246-4731-84f9-eb1e0e1d9091",
   "metadata": {},
   "source": [
    "## Deploy Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1ac85a4-1978-4fb9-91c4-a4d728ed04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder() \\\n",
    "    .cpus(.25).memory('1Gi') \\\n",
    "    .sidekick_memory(model, '4Gi') \\\n",
    "    .sidekick_cpus(model, 1.) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea455f46-6f8b-4d5f-a195-74634c0c886b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 90s ......................... ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.244.3.53',\n",
       "   'name': 'engine-78f6bdb9fd-ml6ld',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'er-clip-vit-pipeline-new',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'er-clip-vit',\n",
       "      'version': '4564b16e-bab6-4c12-a014-6ff042051f1a',\n",
       "      'sha': '4efc24685a14e1682301cc0085b9db931aeb5f3f8247854bedc6863275ed0646',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.244.4.61',\n",
       "   'name': 'engine-lb-584f54c899-5clf5',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.244.4.60',\n",
       "   'name': 'engine-sidekick-er-clip-vit-25-7bcff8d494-8986c',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_name = \"er-clip-vit-pipeline-new\"\n",
    "pipeline = wl.build_pipeline(pipeline_name)\n",
    "pipeline.add_model_step(model)\n",
    "\n",
    "pipeline.deploy(deployment_config=deployment_config)\n",
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f305561-adc4-4362-9b83-547815a73fa5",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5dd7fe-e2bb-4421-abb1-953228cbd3d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.244.3.53',\n",
       "   'name': 'engine-78f6bdb9fd-ml6ld',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'er-clip-vit-pipeline-new',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'er-clip-vit',\n",
       "      'version': '4564b16e-bab6-4c12-a014-6ff042051f1a',\n",
       "      'sha': '4efc24685a14e1682301cc0085b9db931aeb5f3f8247854bedc6863275ed0646',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.244.4.61',\n",
       "   'name': 'engine-lb-584f54c899-5clf5',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.244.4.60',\n",
       "   'name': 'engine-sidekick-er-clip-vit-25-7bcff8d494-8986c',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "792817ce-f8fa-445b-8154-98263a646d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_urls = [\n",
    "    \"https://farm6.staticflickr.com/5200/5893309516_d22a116a65_z.jpg\",\n",
    "    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n",
    "    \"https://farm4.staticflickr.com/3726/9780496575_ec5d9c0e4f_z.jpg\",\n",
    "    \"https://farm5.staticflickr.com/4021/4548948723_ab46d70f85_z.jpg\",\n",
    "    \"https://farm1.staticflickr.com/162/342939460_6a7744c3c2_z.jpg\"\n",
    "]\n",
    "images = []\n",
    "\n",
    "for iu in image_urls:\n",
    "    image = Image.open(requests.get(iu, stream=True).raw)\n",
    "    image = image.resize((640, 480)) # fixed image dimensions\n",
    "    images.append(np.array(image))\n",
    "\n",
    "dataframe = pd.DataFrame({\"images\": images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d85172-8168-4f02-8442-426331ccdcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>candidate_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[177, 177, 177], [177, 177, 177], [177, 177,...</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[140, 25, 56], [144, 25, 67], [146, 24, 73],...</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[228, 235, 241], [229, 236, 242], [230, 237,...</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[60, 62, 61], [62, 64, 63], [67, 69, 68], [7...</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[24, 20, 11], [22, 18, 9], [18, 14, 5], [21,...</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              inputs  \\\n",
       "0  [[[177, 177, 177], [177, 177, 177], [177, 177,...   \n",
       "1  [[[140, 25, 56], [144, 25, 67], [146, 24, 73],...   \n",
       "2  [[[228, 235, 241], [229, 236, 242], [230, 237,...   \n",
       "3  [[[60, 62, 61], [62, 64, 63], [67, 69, 68], [7...   \n",
       "4  [[[24, 20, 11], [22, 18, 9], [18, 14, 5], [21,...   \n",
       "\n",
       "              candidate_labels  \n",
       "0  [cat, dog, horse, elephant]  \n",
       "1  [cat, dog, horse, elephant]  \n",
       "2  [cat, dog, horse, elephant]  \n",
       "3  [cat, dog, horse, elephant]  \n",
       "4  [cat, dog, horse, elephant]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = {\n",
    "        \"inputs\": images,\n",
    "        \"candidate_labels\": [[\"cat\", \"dog\", \"horse\", \"elephant\"]] * 5,\n",
    "}\n",
    "dataframe = pd.DataFrame(input_data)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa078ffd-859e-4706-8729-9e7d78f8bf60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.candidate_labels</th>\n",
       "      <th>in.inputs</th>\n",
       "      <th>out.label</th>\n",
       "      <th>out.score</th>\n",
       "      <th>check_failures</th>\n",
       "      <th>metadata.elapsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-10-25 13:39:28.824</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "      <td>[177, 177, 177, 177, 177, 177, 177, 177, 177, ...</td>\n",
       "      <td>[horse, dog, elephant, cat]</td>\n",
       "      <td>[0.7596803307533264, 0.21711139380931854, 0.02...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1854798121, 4294967295]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-10-25 13:39:28.824</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "      <td>[140, 25, 56, 144, 25, 67, 146, 24, 73, 142, 1...</td>\n",
       "      <td>[cat, dog, elephant, horse]</td>\n",
       "      <td>[0.9870228171348572, 0.00664688041433692, 0.00...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1854798121, 4294967295]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-10-25 13:39:28.824</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "      <td>[228, 235, 241, 229, 236, 242, 230, 237, 243, ...</td>\n",
       "      <td>[elephant, horse, dog, cat]</td>\n",
       "      <td>[0.9981434345245361, 0.001765866531059146, 6.8...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1854798121, 4294967295]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-10-25 13:39:28.824</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "      <td>[60, 62, 61, 62, 64, 63, 67, 69, 68, 72, 74, 7...</td>\n",
       "      <td>[elephant, dog, horse, cat]</td>\n",
       "      <td>[0.41468727588653564, 0.3483794331550598, 0.12...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1854798121, 4294967295]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-10-25 13:39:28.824</td>\n",
       "      <td>[cat, dog, horse, elephant]</td>\n",
       "      <td>[24, 20, 11, 22, 18, 9, 18, 14, 5, 21, 17, 8, ...</td>\n",
       "      <td>[dog, horse, cat, elephant]</td>\n",
       "      <td>[0.5713930130004883, 0.1722952425479889, 0.155...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1854798121, 4294967295]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time          in.candidate_labels  \\\n",
       "0 2023-10-25 13:39:28.824  [cat, dog, horse, elephant]   \n",
       "1 2023-10-25 13:39:28.824  [cat, dog, horse, elephant]   \n",
       "2 2023-10-25 13:39:28.824  [cat, dog, horse, elephant]   \n",
       "3 2023-10-25 13:39:28.824  [cat, dog, horse, elephant]   \n",
       "4 2023-10-25 13:39:28.824  [cat, dog, horse, elephant]   \n",
       "\n",
       "                                           in.inputs  \\\n",
       "0  [177, 177, 177, 177, 177, 177, 177, 177, 177, ...   \n",
       "1  [140, 25, 56, 144, 25, 67, 146, 24, 73, 142, 1...   \n",
       "2  [228, 235, 241, 229, 236, 242, 230, 237, 243, ...   \n",
       "3  [60, 62, 61, 62, 64, 63, 67, 69, 68, 72, 74, 7...   \n",
       "4  [24, 20, 11, 22, 18, 9, 18, 14, 5, 21, 17, 8, ...   \n",
       "\n",
       "                     out.label  \\\n",
       "0  [horse, dog, elephant, cat]   \n",
       "1  [cat, dog, elephant, horse]   \n",
       "2  [elephant, horse, dog, cat]   \n",
       "3  [elephant, dog, horse, cat]   \n",
       "4  [dog, horse, cat, elephant]   \n",
       "\n",
       "                                           out.score  check_failures  \\\n",
       "0  [0.7596803307533264, 0.21711139380931854, 0.02...               0   \n",
       "1  [0.9870228171348572, 0.00664688041433692, 0.00...               0   \n",
       "2  [0.9981434345245361, 0.001765866531059146, 6.8...               0   \n",
       "3  [0.41468727588653564, 0.3483794331550598, 0.12...               0   \n",
       "4  [0.5713930130004883, 0.1722952425479889, 0.155...               0   \n",
       "\n",
       "           metadata.elapsed  \n",
       "0  [1854798121, 4294967295]  \n",
       "1  [1854798121, 4294967295]  \n",
       "2  [1854798121, 4294967295]  \n",
       "3  [1854798121, 4294967295]  \n",
       "4  [1854798121, 4294967295]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.infer(dataframe,timeout=600,dataset=[\"in\", \"out\", \"metadata.elapsed\", \"time\", \"check_failures\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10efccd7-fd4a-4628-907c-e5d84397ea1c",
   "metadata": {},
   "source": [
    "## Undeploy Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34d053bc-b225-45e0-8e84-f45c30122f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s ..................................... ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n",
      " ok\n"
     ]
    }
   ],
   "source": [
    "for pipeline in wl.list_pipelines():\n",
    "    pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
