{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following tutorials are available from the [Wallaroo Tutorials Repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/notebooks_in_prod).\n",
    "\n",
    "# Stage 2: Training Process Automation Setup\n",
    " \n",
    "Now that we have decided on the type and structure of the model from Stage 1: Data Exploration And Model Selection, this notebook modularizes the various steps of the process in a structure that is compatible with production and with Wallaroo.\n",
    "\n",
    "We have pulled the preprocessing and postprocessing steps out of the training notebook into individual scripts that can also be used when the model is deployed.\n",
    "\n",
    "Assuming no changes are made to the structure of the model, this notebook, or a script based on this notebook, can then be scheduled to run on a regular basis, to refresh the model with more recent training data. We'd expect to run this notebook in conjunction with the Stage 3 notebook, `03_deploy_model.ipynb`.  For clarity in this demo, we have split the training/upload task into two notebooks, `02_automated_training_process.ipynb` and `03_deploy_model.ipynb`.\n",
    "\n",
    "## Resources\n",
    "\n",
    "The following resources are used as part of this tutorial:\n",
    "\n",
    "* **data**\n",
    "  * `data/seattle_housing_col_description.txt`: Describes the columns used as part data analysis.\n",
    "  * `data/seattle_housing.csv`: Sample data of the Seattle, Washington housing market between 2014 and 2015.\n",
    "* **code**\n",
    "  * `postprocess.py`: Formats the data after inference by the model is complete.\n",
    "  * `preprocess.py`: Formats the incoming data for the model.\n",
    "  * `simdb.py`: A simulated database to demonstrate sending and receiving queries.\n",
    "  * `wallaroo_client.py`: Additional methods used with the Wallaroo instance to create workspaces, etc.\n",
    "\n",
    "## Steps\n",
    "\n",
    "The following steps are part of this process:\n",
    "\n",
    "* [Retrieve Training Data](#retrieve-training-data): Connect to the data store and retrieve the training data.\n",
    "* [Data Transformations](#data-transformations): Evaluate the data and train the model.\n",
    "* [Generate and Test the Model](#generate-and-test-the-model): Create the model and verify it against the sample test data.\n",
    "* [Pickle The Model](#pickle-the-model): Prepare the model to be uploaded to Wallaroo.\n",
    "\n",
    "### Retrieve Training Data\n",
    "\n",
    "Note that this connection is simulated to demonstrate how data would be retrieved from an existing data store.  For training, we will use the data on all houses sold in this market with the last two years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import seaborn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "\n",
    "import simdb # module for the purpose of this demo to simulate pulling data from a database\n",
    "\n",
    "from preprocess import create_features  # our custom preprocessing\n",
    "from postprocess import postprocess    # our custom postprocessing\n",
    "\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# ignoring warnings for demonstration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = simdb.simulate_db_connection()\n",
    "tablename = simdb.tablename\n",
    "\n",
    "query = f\"select * from {tablename} where date > DATE(DATE(), '-24 month') AND sale_price is not NULL\"\n",
    "print(query)\n",
    "# read in the data\n",
    "housing_data = pd.read_sql_query(query, conn)\n",
    "\n",
    "conn.close()\n",
    "housing_data.loc[:, [\"id\", \"date\", \"list_price\", \"bedrooms\", \"bathrooms\", \"sqft_living\", \"sqft_lot\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformations\n",
    "\n",
    "To improve relative error performance, we will predict on `log10` of the sale price.\n",
    "\n",
    "Predict on log10 price to try to improve relative error performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data['logprice'] = np.log10(housing_data.list_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and test\n",
    "outcome = 'logprice'\n",
    "\n",
    "runif = np.random.default_rng(2206222).uniform(0, 1, housing_data.shape[0])\n",
    "gp = np.where(runif < 0.2, 'test', 'training')\n",
    "\n",
    "hd_train = housing_data.loc[gp=='training', :].reset_index(drop=True, inplace=False)\n",
    "hd_test = housing_data.loc[gp=='test', :].reset_index(drop=True, inplace=False)\n",
    "\n",
    "# split the training into training and val for xgboost\n",
    "runif = np.random.default_rng(123).uniform(0, 1, hd_train.shape[0])\n",
    "xgb_gp = np.where(runif < 0.2, 'val', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for xgboost\n",
    "train_features = hd_train.loc[xgb_gp=='train', :].reset_index(drop=True, inplace=False)\n",
    "train_features = np.array(create_features(train_features))\n",
    "train_labels = np.array(hd_train.loc[xgb_gp=='train', outcome])\n",
    "\n",
    "val_features = hd_train.loc[xgb_gp=='val', :].reset_index(drop=True, inplace=False)\n",
    "val_features = np.array(create_features(val_features))\n",
    "val_labels = np.array(hd_train.loc[xgb_gp=='val', outcome])\n",
    "\n",
    "print(f'train_features: {train_features.shape}, train_labels: {len(train_labels)}')\n",
    "print(f'val_features: {val_features.shape}, val_labels: {len(val_labels)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Test the Model\n",
    "\n",
    "Based on the experimentation and testing performed in **Stage 1: Data Exploration And Model Selection**, XGBoost was selected as the ML model and the variables for training were selected.  The model will be generated and tested against sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective = 'reg:squarederror', \n",
    "    max_depth=5, \n",
    "    base_score = np.mean(hd_train[outcome])\n",
    "    )\n",
    "\n",
    "xgb_model.fit( \n",
    "    train_features,\n",
    "    train_labels,\n",
    "    eval_set=[(train_features, train_labels), (val_features, val_labels)],\n",
    "    verbose=False,\n",
    "    early_stopping_rounds=35\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xgb_model.best_score)\n",
    "print(xgb_model.best_iteration)\n",
    "print(xgb_model.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = np.array(create_features(hd_test.copy()))\n",
    "test_labels = np.array(hd_test.loc[:, outcome])\n",
    "\n",
    "pframe = pd.DataFrame({\n",
    "    'pred' : postprocess(xgb_model.predict(test_features)),\n",
    "    'actual' : postprocess(test_labels)\n",
    "})\n",
    "\n",
    "ax = seaborn.scatterplot(\n",
    "    data=pframe,\n",
    "    x='pred',\n",
    "    y='actual',\n",
    "    alpha=0.2\n",
    ")\n",
    "matplotlib.pyplot.plot(pframe.pred, pframe.pred, color='DarkGreen')\n",
    "matplotlib.pyplot.title(\"test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pframe['se'] = (pframe.pred - pframe.actual)**2\n",
    "\n",
    "pframe['pct_err'] = 100*np.abs(pframe.pred - pframe.actual)/pframe.actual\n",
    "pframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.sqrt(np.mean(pframe.se))\n",
    "mape = np.mean(pframe.pct_err)\n",
    "\n",
    "print(f'rmse = {rmse}, mape = {mape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Model to Onnx\n",
    "\n",
    "This step converts the model to onnx for easy import into Wallaroo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxmltools.convert import convert_xgboost\n",
    "\n",
    "from skl2onnx.common.data_types import FloatTensorType, DoubleTensorType\n",
    "\n",
    "import preprocess\n",
    "\n",
    "# set the number of columns\n",
    "ncols = len(preprocess._vars)\n",
    "\n",
    "# derive the opset value\n",
    "\n",
    "from onnx.defs import onnx_opset_version\n",
    "from onnxconverter_common.onnx_ex import DEFAULT_OPSET_NUMBER\n",
    "TARGET_OPSET = min(DEFAULT_OPSET_NUMBER, onnx_opset_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to onnx\n",
    "\n",
    "onnx_model_converted = convert_xgboost(xgb_model, 'tree-based classifier',\n",
    "                             [('input', FloatTensorType([None, ncols]))],\n",
    "                             target_opset=TARGET_OPSET)\n",
    "\n",
    "# Save the model\n",
    "\n",
    "onnx.save_model(onnx_model_converted, \"housing_model_xgb.onnx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model trained and ready, we can now go to Stage 3: Deploy the Model in Wallaroo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arrowtests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dda4bf3640b7fafcd1648658b879b4cc9f6ba6084e8fb356fdaaa1a461d1690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
