{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d6daff",
   "metadata": {},
   "source": [
    "This can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.1_tutorials/wallaroo-automate/connection_api_bigquery_tutorial).\n",
    "\n",
    "## Wallaroo Connection and ML Workload Orchestration with BigQuery House Price Model Tutorial\n",
    "\n",
    "This tutorial provides a quick set of methods and examples regarding Wallaroo Connections.  For full details, see the Wallaroo Documentation site.\n",
    "\n",
    "Wallaroo provides Data Connections to organizations with a method of creating and managing automated tasks that can either be run on demand or a regular schedule.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "* **Orchestration**: A set of instructions written as a python script with a requirements library.  Orchestrations are uploaded to the Wallaroo instance as a .zip file.\n",
    "* **Task**: An implementation of an orchestration.  Tasks are run either once when requested, on a repeating schedule, or as a service.\n",
    "* **Connection**: Definitions set by MLOps engineers that are used by other Wallaroo users for connection information to a data source.  Usually paired with orchestrations.\n",
    "\n",
    "This tutorial will focus on using Google BigQuery as the data source.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "The tutorial will demonstrate the following:\n",
    "\n",
    "1. Create a Wallaroo connection to retrieving information from a Google BigQuery source table.\n",
    "1. Create a Wallaroo connection to store inference results into a Google BigQuery destination table.\n",
    "1. Upload Wallaroo ML Workload Orchestration that supports BigQuery connections with the connection details.\n",
    "1. Run the orchestration once as a Run Once Task and verify that the inference request succeeded and the inference results were saved to the external data store.\n",
    "1. Schedule the orchestration as a Scheduled Task and verify that the inference request succeeded and the inference results were saved to the external data store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67e1bb26",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* An installed Wallaroo instance.\n",
    "* The following Python libraries installed.  These are included by default in a Wallaroo instance's JupyterHub service.\n",
    "  * `os`\n",
    "  * [`wallaroo`](https://pypi.org/project/wallaroo/): The Wallaroo SDK. Included with the Wallaroo JupyterHub service by default.\n",
    "  * [`pandas`](https://pypi.org/project/pandas/): Pandas, mainly used for Pandas DataFrame\n",
    "  * [`pyarrow`](https://pypi.org/project/pyarrow/): PyArrow for Apache Arrow support\n",
    "* The following Python libraries.  These are **not** included in a Wallaroo instance's JupyterHub service.\n",
    "  * [`google-cloud-bigquery`](https://pypi.org/project/google-cloud-bigquery/): Specifically for its support for Google BigQuery.\n",
    "  * [`google-auth`](https://pypi.org/project/google-auth/): Used to authenticate for bigquery.\n",
    "  * [`db-dtypes`](https://pypi.org/project/db-dtypes/): Converts the BigQuery results to Apache Arrow table or pandas DataFrame.\n",
    "\n",
    "## Tutorial Resources\n",
    "\n",
    "* Models:\n",
    "  * `models/rf_model.onnx`: A model that predicts house price values.\n",
    "* Data:\n",
    "  * `data/xtest-1.df.json` and `data/xtest-1k.df.json`:  DataFrame JSON inference inputs with 1 input and 1,000 inputs.\n",
    "  * `data/xtest-1k.arrow`:  Apache Arrow inference inputs with 1 input and 1,000 inputs.\n",
    "  * Sample inference inputs in `CSV` that can be imported into Google BigQuery.\n",
    "    * `data/xtest-1k.df.json`: Random sample housing prices.\n",
    "    * `data/smallinputs.df.json`: Sample housing prices that return results lower than $1.5 million.\n",
    "    * `data/biginputs.df.json`: Sample housing prices that return results higher than $1.5 million.\n",
    "  * SQL queries to create the inputs/outputs tables with schema.\n",
    "    * `./resources/create_inputs_table.sql`: Inputs table with schema.\n",
    "    * `./resources/create_outputs_table.sql`: Outputs table with schema.\n",
    "    * `./resources/housrpricesga_inputs.avro`: Avro container of inputs table.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2b09330-0408-45eb-b321-b48b65041789",
   "metadata": {},
   "source": [
    "## Initial Steps\n",
    "\n",
    "For this tutorial, we'll create a workspace, upload our sample model and deploy a pipeline.  We'll perform some quick sample inferences to verify that everything it working.\n",
    "\n",
    "### Load Libraries\n",
    "\n",
    "Here we'll import the various libraries we'll use for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ee5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError, RequiredAttributeMissing\n",
    "\n",
    "# to display dataframe tables\n",
    "from IPython.display import display\n",
    "# used to display dataframe information without truncating\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import pyarrow as pa\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "# for Big Query connections\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import db_dtypes\n",
    "\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae645fd0",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  If logging in externally, update the `wallarooPrefix` and `wallarooSuffix` variables with the proper DNS information.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9d19a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login through local Wallaroo instance\n",
    "\n",
    "wl = wallaroo.Client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70e21500",
   "metadata": {},
   "source": [
    "### API URL\n",
    "\n",
    "The variable `wl.api_endpoint` is used to specify the connection to the Wallaroo instance's MLOps API URL, and is composed of the Wallaroo DNS prefix and suffix.  For full details, see the [Wallaroo API Connection Guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-api-guide/wallaroo-mlops-connection-guide/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a8340b1",
   "metadata": {},
   "source": [
    "## Variable Declaration\n",
    "\n",
    "The following variables will be used for our big query testing.  \n",
    "\n",
    "We'll use two connections:\n",
    "\n",
    "* bigquery_input_connection: The connection that will draw inference input data from a BigQuery table.\n",
    "* bigquery_output_connection:  The connection that will upload inference results into a BigQuery table.\n",
    "\n",
    "Not that for the connection arguments, we'll retrieve the information from the files `./bigquery_service_account_input_key.json` and `./bigquery_service_account_output_key.json` that include the  [service account key file(SAK)](https://cloud.google.com/bigquery/docs/authentication/service-account-file) information, as well as the dataset and table used.\n",
    "\n",
    "| Field | Included in SAK | \n",
    "|---|---|\n",
    "| type | âˆš | \n",
    "| project_id | âˆš |\n",
    "| private_key_id | âˆš |\n",
    "| private_key | âˆš |\n",
    "| client_email | âˆš |\n",
    "| auth_uri | âˆš |\n",
    "| token_uri | âˆš |\n",
    "| auth_provider_x509_cert_url | âˆš |\n",
    "| client_x509_cert_url | âˆš |\n",
    "| database | ðŸš« |\n",
    "| table | ðŸš« |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06a981ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://doc-test.api.wallaroocommunity.ninja'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(wl.api_endpoint)\n",
    "\n",
    "# Setting variables for later steps\n",
    "\n",
    "workspace_name = 'bigqueryapiworkspace'\n",
    "pipeline_name = 'bigqueryapipipeline'\n",
    "model_name = 'bigqueryapimodel'\n",
    "model_file_name = './models/rf_model.onnx'\n",
    "\n",
    "bigquery_connection_input_name = f\"bigqueryhouseapiinput\"\n",
    "bigquery_connection_input_type = \"BIGQUERY\"\n",
    "bigquery_connection_input_argument = json.load(open('./bigquery_service_account_input_key.json'))\n",
    "\n",
    "bigquery_connection_output_name = f\"bigqueryhouseapioutputs\"\n",
    "bigquery_connection_output_type = \"BIGQUERY\"\n",
    "bigquery_connection_output_argument = json.load(open('./bigquery_service_account_output_key.json'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98b78cf9",
   "metadata": {},
   "source": [
    "### Create the Workspace and Pipeline\n",
    "\n",
    "We'll now create our workspace and pipeline for the tutorial.  If this tutorial has been run previously, then this will retrieve the existing ones with the assumption they're for us with this tutorial.\n",
    "\n",
    "We'll set the retrieved workspace as the current workspace in the SDK, so all commands will default to that workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59aa845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = wl.get_workspace(name=workspace_name, create_if_not_exist=True)\n",
    "wl.set_current_workspace(workspace)\n",
    "\n",
    "workspace_id = workspace.id()\n",
    "\n",
    "pipeline = wl.build_pipeline(pipeline_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9fc274",
   "metadata": {},
   "source": [
    "### Upload the Model and Deploy Pipeline\n",
    "\n",
    "We'll upload our model into our sample workspace, then add it as a pipeline step before deploying the pipeline to it's ready to accept inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6a69610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>bigqueryapipipeline</td></tr><tr><th>created</th> <td>2024-04-17 18:03:54.216186+00:00</td></tr><tr><th>last_updated</th> <td>2024-04-17 18:03:54.216186+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>arch</th> <td>None</td></tr><tr><th>accel</th> <td>None</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>e7362ff1-64f7-4929-807b-67c8062dbcd3</td></tr><tr><th>steps</th> <td></td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'bigqueryapipipeline', 'create_time': datetime.datetime(2024, 4, 17, 18, 3, 54, 216186, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'bigqueryapimodel', 'version': '77035052-0c8a-44f8-aa59-90d5643b7f8e', 'sha': 'e22a0831aafd9917f3cc87a15ed267797f80e2afa12ad7d8810ca58f173b8cc6'}]}}]\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the model\n",
    "\n",
    "housing_model_control = (wl.upload_model(model_name, \n",
    "                                         model_file_name, \n",
    "                                         framework=wallaroo.framework.Framework.ONNX)\n",
    "                                         .configure(tensor_fields=[\"tensor\"])\n",
    "                        )\n",
    "\n",
    "# Add the model as a pipeline step\n",
    "\n",
    "pipeline.add_model_step(housing_model_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a484796e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>bigqueryapipipeline</td></tr><tr><th>created</th> <td>2024-04-17 18:03:54.216186+00:00</td></tr><tr><th>last_updated</th> <td>2024-04-17 18:03:56.375536+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>85bd635f-3a06-48eb-9446-8e6d195f4fe9, e7362ff1-64f7-4929-807b-67c8062dbcd3</td></tr><tr><th>steps</th> <td>bigqueryapimodel</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'bigqueryapipipeline', 'create_time': datetime.datetime(2024, 4, 17, 18, 3, 54, 216186, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'bigqueryapimodel', 'version': '77035052-0c8a-44f8-aa59-90d5643b7f8e', 'sha': 'e22a0831aafd9917f3cc87a15ed267797f80e2afa12ad7d8810ca58f173b8cc6'}]}}]\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#deploy the pipeline\n",
    "pipeline.deploy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "573bc347",
   "metadata": {},
   "source": [
    "## Connection Management via the Wallaroo MLOps API\n",
    "\n",
    "The following steps will demonstration using the Wallaroo MLOps API to:\n",
    "\n",
    "* Create the BigQuery connections\n",
    "* Add the connections to the targeted workspace\n",
    "* Use the connections for inference requests and uploading the results to a BigQuery dataset table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a4a01b8",
   "metadata": {},
   "source": [
    "### Create Connections via API\n",
    "\n",
    "We will create the data source connection via the Wallaroo api request:\n",
    "\n",
    "`/v1/api/connections/create`\n",
    "\n",
    "This takes the following parameters:\n",
    "\n",
    "* **name** (*String* *Required*): The name of the connection.\n",
    "* **type** (*String* *Required*): The user defined type of connection.\n",
    "* **details** (*String* *Required*):  User defined configuration details for the data connection.  These can be `{'username':'dataperson', 'password':'datapassword', 'port': 3339}`, or `{'token':'abcde123==', 'host':'example.com', 'port:1234'}`, or other user defined combinations.\n",
    "\n",
    "* **IMPORTANT NOTE**:  Data connections names **must** be unique.  Attempting to create a data connection with the same `name` as an existing data connection will result in an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "917c8088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7e6fe020-fd3b-4157-b5c4-e64e21187179'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieve the authorization token\n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "url = f\"{wl.api_endpoint}/v1/api/connections/create\"\n",
    "\n",
    "# input connection\n",
    "data = {\n",
    "    'name': bigquery_connection_input_name,\n",
    "    'type' : bigquery_connection_input_type,\n",
    "    'details': bigquery_connection_input_argument\n",
    "}\n",
    "\n",
    "response=requests.post(url, headers=headers, json=data).json()\n",
    "display(response)\n",
    "# saved for later steps\n",
    "connection_input_id = response['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217412a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5f91a5dd-9df0-4950-a883-d9789efc48cc'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieve the authorization token\n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "url = f\"{wl.api_endpoint}/v1/api/connections/create\"\n",
    "\n",
    "# output connection\n",
    "data = {\n",
    "    'name': bigquery_connection_output_name,\n",
    "    'type' : bigquery_connection_output_type,\n",
    "    'details': bigquery_connection_output_argument\n",
    "}\n",
    "\n",
    "response=requests.post(url, headers=headers, json=data).json()\n",
    "display(response)\n",
    "# saved for later steps\n",
    "connection_output_id = response['id']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb4ac6bb",
   "metadata": {},
   "source": [
    "\n",
    "### Add Connections to Workspace via API\n",
    "\n",
    "The connections will be added to the sample workspace with the MLOps API request:\n",
    "\n",
    "`/v1/api/connections/add_to_workspace`\n",
    "\n",
    "This takes the following parameters:\n",
    "\n",
    "* **workspace_id** (*String* *Required*): The name of the connection.\n",
    "* **connection_id** (*String* *Required*): The UUID connection ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "077d450d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '500aa19b-5f3a-45b4-9af6-2b2ea89b1a3a'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'id': '6859a504-969d-4e23-8691-48f04ae36d23'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# retrieve the authorization token\n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "url = f\"{wl.api_endpoint}/v1/api/connections/add_to_workspace\"\n",
    "\n",
    "data = {\n",
    "    'workspace_id': workspace_id,\n",
    "    'connection_id': connection_input_id\n",
    "}\n",
    "\n",
    "response=requests.post(url, headers=headers, json=data)\n",
    "display(response.json())\n",
    "\n",
    "data = {\n",
    "    'workspace_id': workspace_id,\n",
    "    'connection_id': connection_output_id\n",
    "}\n",
    "\n",
    "response=requests.post(url, headers=headers, json=data)\n",
    "display(response.json())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d316530",
   "metadata": {},
   "source": [
    "### Connect to Google BigQuery\n",
    "\n",
    "With our connections set, we'll now use them for an inference request through the following steps:\n",
    "\n",
    "1. Retrieve the input data from a BigQuery request from the input connection details.\n",
    "1. Perform the inference.\n",
    "1. Upload the inference results into another BigQuery table from the output connection details."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a7d7609",
   "metadata": {},
   "source": [
    "### Create Google Credentials\n",
    "\n",
    "From our BigQuery request, we'll create the credentials for our BigQuery connection.\n",
    "\n",
    "We will use the MLOps API call:\n",
    "\n",
    "`/v1/api/connections/get`\n",
    "\n",
    "to retrieve the connection. This request takes the following parameters:\n",
    "\n",
    "* **name** (*String* *Required*): The name of the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47069c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the connection input details\n",
    "\n",
    "# retrieve the authorization token\n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "url = f\"{wl.api_endpoint}/v1/api/connections/get\"\n",
    "\n",
    "data = {\n",
    "    'name': bigquery_connection_input_name\n",
    "}\n",
    "\n",
    "connection_input_details=requests.post(url, headers=headers, json=data).json()['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "593499ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the connection output details\n",
    "\n",
    "# retrieve the authorization token\n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "url = f\"{wl.api_endpoint}/v1/api/connections/get\"\n",
    "\n",
    "data = {\n",
    "    'name': bigquery_connection_output_name\n",
    "}\n",
    "\n",
    "connection_output_details=requests.post(url, headers=headers, json=data).json()['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436b342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the bigquery credentials\n",
    "\n",
    "bigquery_input_credentials = service_account.Credentials.from_service_account_info(\n",
    "    connection_input_details)\n",
    "\n",
    "bigquery_output_credentials = service_account.Credentials.from_service_account_info(\n",
    "    connection_output_details)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0231cc4",
   "metadata": {},
   "source": [
    "### Connect to Google BigQuery\n",
    "\n",
    "We can now generate a client from our connection details, specifying the project that was included in the `big_query_connection` details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09801a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigqueryinputclient = bigquery.Client(\n",
    "    credentials=bigquery_input_credentials, \n",
    "    project=connection_input_details['project_id']\n",
    ")\n",
    "bigqueryoutputclient = bigquery.Client(\n",
    "    credentials=bigquery_output_credentials, \n",
    "    project=connection_output_details['project_id']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05f91f2b",
   "metadata": {},
   "source": [
    "### Query Data\n",
    "\n",
    "Now we'll create our query and retrieve information from out dataset and table as defined in the file `bigquery_service_account_key.json`.  The table is expected to be in the format of the file `./data/xtest-1k.df.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf4a7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataframe_input = bigqueryinputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT tensor\n",
    "        FROM {connection_input_details['dataset']}.{connection_input_details['table']}\"\"\"\n",
    "    ).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4924489d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tensor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[4.0, 2.5, 2900.0, 5505.0, 2.0, 0.0, 0.0, 3.0, 8.0, 2900.0, 0.0, 47.6063, -122.02, 2970.0, 5251.0, 12.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2.0, 2.5, 2170.0, 6361.0, 1.0, 0.0, 2.0, 3.0, 8.0, 2170.0, 0.0, 47.7109, -122.017, 2310.0, 7419.0, 6.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[3.0, 2.5, 1300.0, 812.0, 2.0, 0.0, 0.0, 3.0, 8.0, 880.0, 420.0, 47.5893, -122.317, 1300.0, 824.0, 6.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4.0, 2.5, 2500.0, 8540.0, 2.0, 0.0, 0.0, 3.0, 9.0, 2500.0, 0.0, 47.5759, -121.994, 2560.0, 8475.0, 24.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[3.0, 1.75, 2200.0, 11520.0, 1.0, 0.0, 0.0, 4.0, 7.0, 2200.0, 0.0, 47.7659, -122.341, 1690.0, 8038.0, 62.0, 0.0, 0.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  tensor\n",
       "0     [4.0, 2.5, 2900.0, 5505.0, 2.0, 0.0, 0.0, 3.0, 8.0, 2900.0, 0.0, 47.6063, -122.02, 2970.0, 5251.0, 12.0, 0.0, 0.0]\n",
       "1     [2.0, 2.5, 2170.0, 6361.0, 1.0, 0.0, 2.0, 3.0, 8.0, 2170.0, 0.0, 47.7109, -122.017, 2310.0, 7419.0, 6.0, 0.0, 0.0]\n",
       "2      [3.0, 2.5, 1300.0, 812.0, 2.0, 0.0, 0.0, 3.0, 8.0, 880.0, 420.0, 47.5893, -122.317, 1300.0, 824.0, 6.0, 0.0, 0.0]\n",
       "3    [4.0, 2.5, 2500.0, 8540.0, 2.0, 0.0, 0.0, 3.0, 9.0, 2500.0, 0.0, 47.5759, -121.994, 2560.0, 8475.0, 24.0, 0.0, 0.0]\n",
       "4  [3.0, 1.75, 2200.0, 11520.0, 1.0, 0.0, 0.0, 4.0, 7.0, 2200.0, 0.0, 47.7659, -122.341, 1690.0, 8038.0, 62.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_dataframe_input.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1eedb4f",
   "metadata": {},
   "source": [
    "### Sample Inference\n",
    "\n",
    "With our data retrieved, we'll perform an inference and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b8ab22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.tensor</th>\n",
       "      <th>out.variable</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[4.0, 2.5, 2900.0, 5505.0, 2.0, 0.0, 0.0, 3.0, 8.0, 2900.0, 0.0, 47.6063, -122.02, 2970.0, 5251.0, 12.0, 0.0, 0.0]</td>\n",
       "      <td>[718013.75]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[2.0, 2.5, 2170.0, 6361.0, 1.0, 0.0, 2.0, 3.0, 8.0, 2170.0, 0.0, 47.7109, -122.017, 2310.0, 7419.0, 6.0, 0.0, 0.0]</td>\n",
       "      <td>[615094.56]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[3.0, 2.5, 1300.0, 812.0, 2.0, 0.0, 0.0, 3.0, 8.0, 880.0, 420.0, 47.5893, -122.317, 1300.0, 824.0, 6.0, 0.0, 0.0]</td>\n",
       "      <td>[448627.72]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[4.0, 2.5, 2500.0, 8540.0, 2.0, 0.0, 0.0, 3.0, 9.0, 2500.0, 0.0, 47.5759, -121.994, 2560.0, 8475.0, 24.0, 0.0, 0.0]</td>\n",
       "      <td>[758714.2]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[3.0, 1.75, 2200.0, 11520.0, 1.0, 0.0, 0.0, 4.0, 7.0, 2200.0, 0.0, 47.7659, -122.341, 1690.0, 8038.0, 62.0, 0.0, 0.0]</td>\n",
       "      <td>[513264.7]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  \\\n",
       "0 2024-04-17 18:04:13.032   \n",
       "1 2024-04-17 18:04:13.032   \n",
       "2 2024-04-17 18:04:13.032   \n",
       "3 2024-04-17 18:04:13.032   \n",
       "4 2024-04-17 18:04:13.032   \n",
       "\n",
       "                                                                                                               in.tensor  \\\n",
       "0     [4.0, 2.5, 2900.0, 5505.0, 2.0, 0.0, 0.0, 3.0, 8.0, 2900.0, 0.0, 47.6063, -122.02, 2970.0, 5251.0, 12.0, 0.0, 0.0]   \n",
       "1     [2.0, 2.5, 2170.0, 6361.0, 1.0, 0.0, 2.0, 3.0, 8.0, 2170.0, 0.0, 47.7109, -122.017, 2310.0, 7419.0, 6.0, 0.0, 0.0]   \n",
       "2      [3.0, 2.5, 1300.0, 812.0, 2.0, 0.0, 0.0, 3.0, 8.0, 880.0, 420.0, 47.5893, -122.317, 1300.0, 824.0, 6.0, 0.0, 0.0]   \n",
       "3    [4.0, 2.5, 2500.0, 8540.0, 2.0, 0.0, 0.0, 3.0, 9.0, 2500.0, 0.0, 47.5759, -121.994, 2560.0, 8475.0, 24.0, 0.0, 0.0]   \n",
       "4  [3.0, 1.75, 2200.0, 11520.0, 1.0, 0.0, 0.0, 4.0, 7.0, 2200.0, 0.0, 47.7659, -122.341, 1690.0, 8038.0, 62.0, 0.0, 0.0]   \n",
       "\n",
       "  out.variable  anomaly.count  \n",
       "0  [718013.75]              0  \n",
       "1  [615094.56]              0  \n",
       "2  [448627.72]              0  \n",
       "3   [758714.2]              0  \n",
       "4   [513264.7]              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = pipeline.infer(inference_dataframe_input)\n",
    "display(result.head(5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7620230",
   "metadata": {},
   "source": [
    "### Upload the Results\n",
    "\n",
    "With the query complete, we'll upload the results back to the BigQuery dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fe1115f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_table = bigqueryoutputclient.get_table(f\"{connection_output_details['dataset']}.{connection_output_details['table']}\")\n",
    "\n",
    "bigqueryoutputclient.insert_rows_from_dataframe(\n",
    "    output_table, \n",
    "    dataframe=result.rename(columns={\"in.tensor\":\"in_tensor\", \"out.variable\":\"out_variable\", \"anomaly.count\":\"anomaly_count\"})\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "912fc6ba",
   "metadata": {},
   "source": [
    "### Verify the Upload\n",
    "\n",
    "We can verify the upload by requesting the last few rows of the output table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2292bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in_tensor</th>\n",
       "      <th>out_variable</th>\n",
       "      <th>anomaly_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[4.0, 2.5, 2500.0, 8540.0, 2.0, 0.0, 0.0, 3.0, 9.0, 2500.0, 0.0, 47.5759, -121.994, 2560.0, 8475.0, 24.0, 0.0, 0.0]</td>\n",
       "      <td>[758714.2]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[4.0, 2.5, 2900.0, 5505.0, 2.0, 0.0, 0.0, 3.0, 8.0, 2900.0, 0.0, 47.6063, -122.02, 2970.0, 5251.0, 12.0, 0.0, 0.0]</td>\n",
       "      <td>[718013.75]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[2.0, 2.5, 2170.0, 6361.0, 1.0, 0.0, 2.0, 3.0, 8.0, 2170.0, 0.0, 47.7109, -122.017, 2310.0, 7419.0, 6.0, 0.0, 0.0]</td>\n",
       "      <td>[615094.56]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[3.0, 1.75, 2200.0, 11520.0, 1.0, 0.0, 0.0, 4.0, 7.0, 2200.0, 0.0, 47.7659, -122.341, 1690.0, 8038.0, 62.0, 0.0, 0.0]</td>\n",
       "      <td>[513264.7]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-17 18:04:13.032</td>\n",
       "      <td>[3.0, 2.5, 1300.0, 812.0, 2.0, 0.0, 0.0, 3.0, 8.0, 880.0, 420.0, 47.5893, -122.317, 1300.0, 824.0, 6.0, 0.0, 0.0]</td>\n",
       "      <td>[448627.72]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  \\\n",
       "0 2024-04-17 18:04:13.032   \n",
       "1 2024-04-17 18:04:13.032   \n",
       "2 2024-04-17 18:04:13.032   \n",
       "3 2024-04-17 18:04:13.032   \n",
       "4 2024-04-17 18:04:13.032   \n",
       "\n",
       "                                                                                                               in_tensor  \\\n",
       "0    [4.0, 2.5, 2500.0, 8540.0, 2.0, 0.0, 0.0, 3.0, 9.0, 2500.0, 0.0, 47.5759, -121.994, 2560.0, 8475.0, 24.0, 0.0, 0.0]   \n",
       "1     [4.0, 2.5, 2900.0, 5505.0, 2.0, 0.0, 0.0, 3.0, 8.0, 2900.0, 0.0, 47.6063, -122.02, 2970.0, 5251.0, 12.0, 0.0, 0.0]   \n",
       "2     [2.0, 2.5, 2170.0, 6361.0, 1.0, 0.0, 2.0, 3.0, 8.0, 2170.0, 0.0, 47.7109, -122.017, 2310.0, 7419.0, 6.0, 0.0, 0.0]   \n",
       "3  [3.0, 1.75, 2200.0, 11520.0, 1.0, 0.0, 0.0, 4.0, 7.0, 2200.0, 0.0, 47.7659, -122.341, 1690.0, 8038.0, 62.0, 0.0, 0.0]   \n",
       "4      [3.0, 2.5, 1300.0, 812.0, 2.0, 0.0, 0.0, 3.0, 8.0, 880.0, 420.0, 47.5893, -122.317, 1300.0, 824.0, 6.0, 0.0, 0.0]   \n",
       "\n",
       "  out_variable  anomaly_count  \n",
       "0   [758714.2]              0  \n",
       "1  [718013.75]              0  \n",
       "2  [615094.56]              0  \n",
       "3   [513264.7]              0  \n",
       "4  [448627.72]              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {connection_output_details['dataset']}.{connection_output_details['table']}\n",
    "        ORDER BY time DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58207f04",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "With the tutorial complete, we can undeploy the pipeline and return the resources back to the Wallaroo instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb96a086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>bigqueryapipipeline</td></tr><tr><th>created</th> <td>2024-04-17 18:03:54.216186+00:00</td></tr><tr><th>last_updated</th> <td>2024-04-17 18:03:56.375536+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>85bd635f-3a06-48eb-9446-8e6d195f4fe9, e7362ff1-64f7-4929-807b-67c8062dbcd3</td></tr><tr><th>steps</th> <td>bigqueryapimodel</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'bigqueryapipipeline', 'create_time': datetime.datetime(2024, 4, 17, 18, 3, 54, 216186, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'bigqueryapimodel', 'version': '77035052-0c8a-44f8-aa59-90d5643b7f8e', 'sha': 'e22a0831aafd9917f3cc87a15ed267797f80e2afa12ad7d8810ca58f173b8cc6'}]}}]\"}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk.2024.1",
   "language": "python",
   "name": "wallaroosdk.2024.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
