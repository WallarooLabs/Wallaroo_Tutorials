{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d9aa6a",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/llamacpp-with-safeguards).\n",
    "\n",
    "The following tutorial demonstrates the Llama 3 70b Instruct Q5 Large Language Model (LLM) with a Harmful Language Listener (HLL).  This provides validation monitoring to detect language that could be considered harmful:  obscene, racist, insulting, or other benchmarks.\n",
    "\n",
    "This tutorial demonstrates how to:\n",
    "\n",
    "* Upload the LLM and HLL.\n",
    "* Create a Wallaroo pipeline and set the LLM and then the HLL as pipeline steps.\n",
    "* Deploy the models and perform sample inferences.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "The LLM used in this demonstrates has the following attributes.\n",
    "\n",
    "* Framework: `vllm` for more optimized model deployment, uploaded to Wallaroo in the [Wallaroo Arbitrary Python aka Bring Your Own Predict (BYOP) Framework](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "* Artifacts:  The original model is here the Llama 3 8B Instruct Hugging Face model:[Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "* Input/Output Types:  Both the input and outputs are text.\n",
    "\n",
    "The HLL used in this demonstration has the following attributes:\n",
    "\n",
    "* Framework: `vllm` for more optimized model deployment, uploaded to Wallaroo in the [Wallaroo Arbitrary Python aka Bring Your Own Predict (BYOP) Framework](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "* Artifacts:  The HLL model is encapsulated as part of the BYOP framework.\n",
    "* Input/Output Types:  The HLL takes the following inputs and outputs.\n",
    "  * HLL Input:\n",
    "    * `text` (*String*): The original input text to the LLM.\n",
    "    * `generated_text` (*String*): The text created by the LLM.  This will be evaluated by the HLL for any harmful language.\n",
    "  * HLL Output:\n",
    "    * `harmful` (*Boolean*): Determines if the `generated_text` is harmful.\n",
    "    * `reasoning` (*String*): The reasons why the `generated_text` is considered harmful or not.\n",
    "    * `confidence` (*Float*): The confidence the model has of whether the `generated_text` is harmful or now.\n",
    "    * `generated_text` (*String*): The text generated by the LLM.  This is passed on as part of the HLL's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ded1f5",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "We start by importing the required libraries.  This includes the following:\n",
    "\n",
    "* [Wallaroo SDK](https://pypi.org/project/wallaroo/):  Used to upload and deploy the model in Wallaroo.\n",
    "* [pyarrow](https://pypi.org/project/pyarrow/):  Models uploaded to Wallaroo are defined in the input/output format.\n",
    "* [pandas](https://pypi.org/project/pandas/):  Data is submitted to models deployed in Wallaroo as either Apache Arrow Table format or pandas Record Format as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca0bc3e-9a4a-4b34-bb17-bd0fffb6a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "from wallaroo.dynamic_batching_config import DynamicBatchingConfig\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b3ecc",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is set through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e6712f9-871f-4224-8a9e-d44520a25628",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558ea37-f227-45dc-a637-c8a7a0e552db",
   "metadata": {},
   "source": [
    "### Upload the LLM\n",
    "\n",
    "To upload the LLM and HLL, we use the `wallaroo.client.Client.upload_model` method which takes the following parameters.\n",
    "\n",
    "* The name to assign to the LLM.\n",
    "* The file path to upload the LLM.\n",
    "* The Framework set to `wallaroo.framework.Framework.CUSTOM` for our Hugging Face model encapsulated in the BYOP framework.\n",
    "* The input and output schemas.\n",
    "\n",
    "For more information, see the Wallaroo [Model Upload](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/) guide.\n",
    "\n",
    "First we'll set the input and output schemas for our LLM in Apache PyArrow Schema format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a6693e0-f96b-45e5-8253-437c9ca28bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string())\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string()),\n",
    "    pa.field(\"generated_text\", pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970c4af",
   "metadata": {},
   "source": [
    "Then issue the upload command.  For this example, we'll add a **model configuration** to specify [Dynamic Batching for LLMs](https://docs.wallaroo.ai/wallaroo-llm/wallaroo-llm-optimizations/wallaroo-llm-optimizations-dynamic-batching/) which improves the performance of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd8718e6-3475-42d1-b5c5-e3a57fc54b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime............successful\n",
      "\n",
      "Ready\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>llama-cpp-sdk-safeguards</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>c28e8fee-a1e0-48eb-a906-430fe1eba7ac</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>byop_llamacpp_safeguards.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>45752b3566691a641787abd9b1b9d94809f8a74d545283d599e8a2cdc492d110</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5825</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-12-Dec 15:34:44</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'llama-cpp-sdk-safeguards', 'version': 'c28e8fee-a1e0-48eb-a906-430fe1eba7ac', 'file_name': 'byop_llamacpp_safeguards.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5825', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2024, 12, 12, 15, 34, 44, 418715, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = wl.upload_model('llama-cpp-sdk-safeguards', \n",
    "    'byop_llamacpp_safeguards.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema\n",
    ").configure(input_schema=input_schema,\n",
    "            output_schema=output_schema,\n",
    "            dynamic_batching_config=DynamicBatchingConfig(max_batch_delay_ms=1000, \n",
    "                                                          batch_size_target=8)\n",
    "            )\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd5e8d",
   "metadata": {},
   "source": [
    "Next we upload the HLL in the same process:  define the input and output schemas, and then upload the model.\n",
    "\n",
    "Note that for the HLL, the inputs are the LLM's **outputs**.  The HLL includes with its outputs the LLM's `generated_text` field so it is passed back to the original receiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5576b58b-8458-4292-b038-b7c486fbabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Safeguards Harmful Language Listener\n",
    "#Define schemas\n",
    "input_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string()),\n",
    "    pa.field(\"generated_text\", pa.string())\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field(\"harmful\", pa.bool_()),\n",
    "    pa.field(\"reasoning\", pa.string()),\n",
    "    pa.field(\"confidence\", pa.float32()),\n",
    "    pa.field(\"generated_text\", pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6efe2490-6143-4e51-a161-befadb37891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime................................successful\n",
      "\n",
      "Ready\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>byop-safeguards-harmful-5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>98893de8-6c13-44cf-b098-b4f1f44ff483</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>byop-safeguards-harmful.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>c41ff30b7032262e6ceffed2da658a44d16e698c1e826c3526b6a2379c8d2b1b</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5825</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-12-Dec 14:50:39</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'byop-safeguards-harmful-5', 'version': '98893de8-6c13-44cf-b098-b4f1f44ff483', 'file_name': 'byop-safeguards-harmful.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5825', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2024, 12, 12, 14, 50, 39, 274918, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload harmful language listener\n",
    "listener = wl.upload_model('byop-safeguards-harmful-5', \n",
    "    'byop-safeguards-harmful.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema,\n",
    ")\n",
    "listener"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0767f9-8b74-4025-831e-49ebb06599b6",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "For our deployment, we deploy both the LLM and HLL in the same pipeline as **pipeline steps**.  Input provided to the pipeline is submitted first to the LLM.  The output from the LLM is then the input to the HLL, and the HLL's output is then provided back to the requester.\n",
    "\n",
    "The deployment configuration sets the resources allocated for the LLM and the HLL with the following options:\n",
    "\n",
    "* LLM\n",
    "  * CPUs: 6\n",
    "  * Memory:  10 Gi\n",
    "* HLL Listener\n",
    "  * CPUs: 2\n",
    "  * Memory:  10 Gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e7a7cf-5fc8-44ec-b16b-d0bd82bb22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(llm, 6) \\\n",
    "    .sidekick_memory(llm, '10Gi') \\\n",
    "    .sidekick_cpus(listener, 2) \\\n",
    "    .sidekick_memory(listener, '10Gi') \\\n",
    "    .sidekick_env(listener, json.load(open(\"credentials.json\", 'r'))) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380151d",
   "metadata": {},
   "source": [
    "The Wallaroo pipeline is created with the `build_pipeline` method.  The LLM and HLL are set as the **pipeline steps**, then deployed with the previously defined deployment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0c6c5-bee6-4be3-9fd3-67f30532a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"safeguards-llamacpp-2\")\n",
    "pipeline.add_model_step(llm)\n",
    "pipeline.add_model_step(listener)\n",
    "pipeline.deploy(deployment_config=deployment_config, wait_for_status=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90ed42",
   "metadata": {},
   "source": [
    "Once deployed, we'll check on the `status`.  When the `status` is `Running`, we continue to the inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e1e7b8b-f92e-4caf-837c-4444061fdbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.4.4.27',\n",
       "   'name': 'engine-6c578848c9-bhs29',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'safeguards-llamacpp-2',\n",
       "      'status': 'Running',\n",
       "      'version': '2b61e016-1e92-4f7a-8efb-f09b29cd126a'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 151,\n",
       "      'name': 'byop-safeguards-harmful-5',\n",
       "      'sha': 'c41ff30b7032262e6ceffed2da658a44d16e698c1e826c3526b6a2379c8d2b1b',\n",
       "      'status': 'Running',\n",
       "      'version': '98893de8-6c13-44cf-b098-b4f1f44ff483'},\n",
       "     {'model_version_id': 152,\n",
       "      'name': 'llama-cpp-sdk-safeguards',\n",
       "      'sha': '45752b3566691a641787abd9b1b9d94809f8a74d545283d599e8a2cdc492d110',\n",
       "      'status': 'Running',\n",
       "      'version': 'c28e8fee-a1e0-48eb-a906-430fe1eba7ac'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.4.4.26',\n",
       "   'name': 'engine-lb-6676794678-bbpfm',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.4.5.5',\n",
       "   'name': 'engine-sidekick-llama-cpp-sdk-safeguards-152-77f5cb6df-fmqgg',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'},\n",
       "  {'ip': '10.4.1.3',\n",
       "   'name': 'engine-sidekick-byop-safeguards-harmful-5-151-7ff646f54d-z7xz2',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de95ca9-0a64-4d8f-b91f-60b1527e26f1",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For our inference, we submit either a pandas DataFrame or Apache Arrow table with our text query.  In this case:  `Describe what Wallaroo.AI is'.\n",
    "\n",
    "Once submitted, we display the `harmful`, `confidence`, and `reason`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c468b4da-ff9f-4639-b972-c8d7cd68da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': ['Describe what Wallaroo.AI is']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9feef8-ce6d-4aca-9f2e-2105a8efd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pipeline.infer(data, timeout=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90dc873f-4dbf-4a84-9225-a117408be152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"out.confidence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab81c3ae-7b53-45e3-9ebf-cccae6e87b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"out.harmful\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c9268a7-f68e-48f6-8254-a4e1e772dd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This response provides a neutral and informative description of Wallaroo.AI, highlighting its capabilities without perpetuating any biases or stereotypes.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"out.reasoning\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebd5ad",
   "metadata": {},
   "source": [
    "### Undeploy the Models\n",
    "\n",
    "With the tutorial complete, we undeploy the model and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ee607-63f5-4f37-9b6f-dd788b9d39eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
