{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a815751-facd-4a95-91be-9f9ef38c97f8",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.2_tutorials/wallaroo-llms/llamacpp-llava).\n",
    "\n",
    "## Quantized Llava 34B Deployment using Wallaroo and Llama.cpp\n",
    "\n",
    "The following tutorial demonstrates deploying the LLaVA v1.6 34B GGUF model in Wallaroo, a powerful image-text-to-text model available on Hugging Face.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "Key points to consider:\n",
    "\n",
    "* The model is based on the LLaVA (Large Language and Vision Assistant) architecture, which combines language understanding with visual perception capabilities.\n",
    "* It is quantized using the GGUF (GPT-Generated Unified Format) for efficient deployment and reduced memory footprint.\n",
    "* The repository contains multiple quantization variants, allowing for flexibility in deployment based on hardware constraints and performance requirements. We have chosen Q5_K_M, which is the variant the offers higher precision quantization for improved output quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b47f6d-af56-4659-926d-0f5aa1240cfb",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "We start by importing the libraries used for the rest of the tutorial.  This includes the [Wallaroo SDK](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/) used to upload, deploy, and infer on LLMs in Wallaroo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca0bc3e-9a4a-4b34-bb17-bd0fffb6a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23df62",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6712f9-871f-4224-8a9e-d44520a25628",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918eb3c4",
   "metadata": {},
   "source": [
    "### BYOP Overview\n",
    "\n",
    "This BYOP model takes a text prompt and an image in an array format, and it will give an answer about what the question and the image were. It uses the latest version of a quantized model called `Llava 1.6` found on `HuggingFace`, which is loaded via `Llama.cpp`.\n",
    "\n",
    "There are two artifacts that are being used by the BYOP model:  the actual model and the CLIP model for calculating the image embeddings, which can be found here:\n",
    "\n",
    "* [Llava 1.6 34B Quantized](https://huggingface.co/cjpais/llava-v1.6-34B-gguf/blob/main/llava-v1.6-34b.Q5_K_M.gguf)\n",
    "* [CLIP Model](https://huggingface.co/cjpais/llava-v1.6-34B-gguf/blob/main/mmproj-model-f16.gguf)\n",
    "\n",
    "Deploying LLama.cpp with the Wallaroo BYOP framework requires [Llama-cpp-python](https://github.com/abetlen/llama-cpp-python). This example uses [Llama 70B Instruct Q5_K_M](https://huggingface.co/QuantFactory/Meta-Llama-3-70B-Instruct-GGUF) for testing and deploying Llama.cpp.\n",
    "\n",
    "#### BYOP Implementation Details\n",
    "\n",
    "The sample LLM is contained in the [Wallaroo Arbitrary Python aka Bring Your Own Predict (BYOP)](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/) framework, which allows for LLM deployment with customized user parameters and behaviors.\n",
    "\n",
    "1. In order to run [Llama-cpp-python](https://github.com/abetlen/llama-cpp-python) on GPU, `llama-cpp-python` is installed using the `subprocess` library in `python`, straight into the Python BYOP code:\n",
    "\n",
    "``` python\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "pip_command = (\n",
    "    f'CMAKE_ARGS=\"-DLLAMA_CUDA=on\" {sys.executable} -m pip install llama-cpp-python'\n",
    ")\n",
    "\n",
    "subprocess.check_call(pip_command, shell=True)\n",
    "```\n",
    "\n",
    "1. The model is loaded via the BYOP's `_load_model` method, which supports the biggest context and offloads all the model's layers to the GPU.\n",
    "\n",
    "```python\n",
    "def _load_model(self, model_path):\n",
    "    llm = Llama(\n",
    "        model_path=f\"{model_path}/artifacts/Meta-Llama-3-70B-Instruct.Q5_K_M.gguf\",\n",
    "        n_ctx=4096,\n",
    "        n_gpu_layers=-1,\n",
    "        logits_all=True,\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "```\n",
    "\n",
    "1. The prompt is constructed based on the chosen model as an instruct-variant.\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a generic chatbot, try to answer questions the best you can.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "\n",
    "result = self.model.create_chat_completion(\n",
    "    messages=messages, max_tokens=1024, stop=[\"<|eot_id|>\"]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558ea37-f227-45dc-a637-c8a7a0e552db",
   "metadata": {},
   "source": [
    "### Upload Model\n",
    "\n",
    "Before uploading, we define the input and output schemas in Apache PyArrow format.  For this example we convert the inputs and output schemas to base64 in preparation of uploading via [the Wallaroo MLOps API](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-api-guide/wallaroo-mlops-api-essential-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4229a5-be9b-4c70-8ff1-0b4bd3abf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('text', pa.string()),\n",
    "    pa.field('image', pa.list_(pa.list_(pa.list_(pa.uint8()))))\n",
    "])\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b110de40-b387-443b-9470-af71d5c01179",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64.b64encode(\n",
    "                bytes(input_schema.serialize())\n",
    "            ).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4826a5-8a28-4b5a-a8da-1ffb29d21aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64.b64encode(\n",
    "                bytes(output_schema.serialize())\n",
    "            ).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f30e18-50c5-4afd-9a5c-01638ff78f16",
   "metadata": {},
   "source": [
    "### Upload via the Wallaroo MLOps API\n",
    "\n",
    "For this tutorial we upload the LLM using the Wallaroo MLOps API endpoint `/v1/api/models/upload_and_convert`, providing the following:\n",
    "\n",
    "* `token`:  [The authentication bearer token](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-api-guide/wallaroo-mlops-connection-guide/)\n",
    "* `hostname`:  The hostname of the Wallaroo instance the LLM is uploaded to.\n",
    "\n",
    "```bash\n",
    "curl --progress-bar -X POST \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -H \"Authorization: Bearer {token}\" \\\n",
    "  -F 'metadata={\"name\": \"llava-llamacpp-gpu\", \"visibility\": \"private\", \"workspace_id\": <your-workspace-id>, \"conversion\": {\"framework\": \"custom\", \"python_version\": \"3.8\", \"requirements\": []}, \"input_schema\": \"/////ygBAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAIAAADMAAAABAAAAEz///8AAAEMFAAAABwAAAAEAAAAAQAAABQAAAAFAAAAaW1hZ2UAAABA////eP///wAAAQwUAAAAHAAAAAQAAAABAAAAFAAAAAQAAABpdGVtAAAAAGz///+k////AAABDBQAAAAcAAAABAAAAAEAAAAUAAAABAAAAGl0ZW0AAAAAmP///9D///8AAAECEAAAABwAAAAEAAAAAAAAAAQAAABpdGVtAAAGAAgABAAGAAAACAAAABAAFAAIAAYABwAMAAAAEAAQAAAAAAABBRAAAAAcAAAABAAAAAAAAAAEAAAAdGV4dAAAAAAEAAQABAAAAA==\", \"output_schema\": \"/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA=\"};type=application/json' \\\n",
    "  -F \"file=@byop_llava.zip;type=application/octet-stream\" \\\n",
    "  https://{hostname}/v1/api/models/upload_and_convert | cat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce37e4a-959e-40b1-a9b1-d9cee8b053a2",
   "metadata": {},
   "source": [
    "### Retrieve the Model\n",
    "\n",
    "Once uploaded and ready for deployment, the model is retrieved through the [`list_models` method](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-optimize/wallaroo-model-operations-model-configure/), retrieving the most recent version of the model and saving it to the `model` variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e7677d-d3b5-4d0a-b779-6f59707a78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wl.list_models()[0].versions()[-1]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0767f9-8b74-4025-831e-49ebb06599b6",
   "metadata": {},
   "source": [
    "### Deploy LLM\n",
    "\n",
    "Deploying the Llama.cpp LLM follows these steps:\n",
    "\n",
    "* [Set the deployment configuration](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-deploy-model/wallaroo-model-operations-deploy-model-deployment-configuration/):  This sets what resources are allocated from the cluster for the LLMs exclusive use.  For this example, the following resources are allocated to the LLM:\n",
    "  * CPUs: 8\n",
    "  * RAM: 30 Gi\n",
    "  * GPUs: 1\n",
    "* Deploy the LLM:  In this phase, the LLM is added to a [Wallaroo Pipeline](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-deploy-model/) as a **model step**, then deployed with the deployment configuration.\n",
    "\n",
    "Once the model is deployed, it is ready for inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7a7cf-5fc8-44ec-b16b-d0bd82bb22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the deployment configuration\n",
    "\n",
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(model, 8) \\\n",
    "    .sidekick_memory(model, \"30Gi\") \\\n",
    "    .sidekick_gpus(model, 1) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0c6c5-bee6-4be3-9fd3-67f30532a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the LLM to a Wallaroo pipeline\n",
    "\n",
    "pipeline = wl.build_pipeline(\"llavacpp-pipeline-v3\")\n",
    "pipeline.add_model_step(model)\n",
    "\n",
    "# deploy the model with the deployment configuration\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1e7b8b-f92e-4caf-837c-4444061fdbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de95ca9-0a64-4d8f-b91f-60b1527e26f1",
   "metadata": {},
   "source": [
    "### Inference Requests\n",
    "\n",
    "Inference requests are submitted to deployed LLMs in Wallaroo either as pandas DataFrames, or Apache Arrow Tables.\n",
    "\n",
    "For this example, a pandas DataFrame is submitted with two columns:\n",
    "\n",
    "* `text`: The question asked of the LLM.\n",
    "* `image`: An image converted into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913b96a5-4f6e-4770-be4c-9b8cc2e138d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('bear.jpeg')\n",
    "image = np.array(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c468b4da-ff9f-4639-b972-c8d7cd68da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': ['What is the animal in the image?'], \n",
    "                     'image': [image.tolist()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e640888",
   "metadata": {},
   "source": [
    "The request is submitted to the deployed LLM, and the `generated_text` field contains the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9feef8-ce6d-4aca-9f2e-2105a8efd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pipeline.infer(data, timeout=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1fecc-bea9-429b-bcdd-e201d95dd661",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"out.generated_text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d038-84e0-4174-a05d-342e7f14586b",
   "metadata": {},
   "source": [
    "### Undeploy\n",
    "\n",
    "With the tutorial complete, we undeploy the LLM and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb5e52-6c24-4a85-ab62-30875554c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
