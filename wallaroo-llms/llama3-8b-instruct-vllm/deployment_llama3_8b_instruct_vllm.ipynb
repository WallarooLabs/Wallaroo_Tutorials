{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cddd10-25e6-4891-bd6a-a14f81ac351e",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.2_tutorials/wallaroo-llms/llamacpp-llava).\n",
    "\n",
    "## Llama 3 8B Instruct with vLLM\n",
    "\n",
    "The following tutorial demonstrates deploying a [Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) LLM with the [`vLLM` library](https://github.com/vllm-project/vllm).  This LLM accepts a text prompt from a user and generates a text response.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "This tutorial shows how to upload the vLLM, deploy it, and perform inference requests through Wallaroo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4ceab-98b1-4f2a-a0b5-680770727c32",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "We start by importing the libraries used for the rest of the tutorial.  This includes the [Wallaroo SDK](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/) used to upload, deploy, and infer on LLMs in Wallaroo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09da43e6-9e0d-483b-8855-61f22ede7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0150ca4",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32932fea-5190-4e36-aca5-69ca0fc629a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2540fa",
   "metadata": {},
   "source": [
    "### BYOP Overview\n",
    "\n",
    "This BYOP model takes a text prompt and returns a text output generated by the vLLM.\n",
    "\n",
    "#### BYOP Implementation Details\n",
    "\n",
    "The sample LLM is contained in the [Wallaroo Arbitrary Python aka Bring Your Own Predict (BYOP)](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/) framework, which allows for LLM deployment with customized user parameters and behaviors.\n",
    "\n",
    "[Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) is used for this example of deploying a vLLM.\n",
    "\n",
    "1. In order to run [vLLM](https://github.com/vllm-project/vllm) on CUDA, `vLLM` is installed using the `subprocess` library in `python`, straight into the Python BYOP code:\n",
    "\n",
    "``` python\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "pip_command = (\n",
    "    f'{sys.executable} -m pip install https://github.com/vllm-project/vllm/releases/download/v0.5.2/vllm-0.5.2+cu118-cp38-cp38-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118'\n",
    ")\n",
    "\n",
    "subprocess.check_call(pip_command, shell=True)\n",
    "```\n",
    "\n",
    "1. The model is loaded via the BYOP's `_load_model` method and setting model weights that are found [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n",
    "\n",
    "```python\n",
    "def _load_model(self, model_path):\n",
    "    llm = LLM(\n",
    "        model=f\"{model_path}/artifacts/Meta-Llama-3-8B-Instruct/\"\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ae08f-47cf-4372-9e44-5c343b0e1761",
   "metadata": {},
   "source": [
    "### Upload Model\n",
    "\n",
    "Before uploading, we define the input and output schemas in Apache PyArrow format.  For this example we convert the inputs and output schemas to base64 in preparation of uploading via [the Wallaroo MLOps API](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-api-guide/wallaroo-mlops-api-essential-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa35c88e-395d-4c89-bf2d-233f3183639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/////3AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAQAAAB0ZXh0AAAAAAQABAAEAAAA'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string()),\n",
    "])\n",
    "\n",
    "base64.b64encode(\n",
    "    bytes(input_schema.serialize())\n",
    ").decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c277d5a-e33a-49ea-807e-5814d05e2c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA='"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_schema = pa.schema([\n",
    "    pa.field(\"generated_text\", pa.string()),\n",
    "])\n",
    "\n",
    "base64.b64encode(\n",
    "    bytes(output_schema.serialize())\n",
    ").decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bccbf6-87bb-45bb-a437-5edbc2f81f19",
   "metadata": {},
   "source": [
    "### Upload via the Wallaroo MLOps API\n",
    "\n",
    "For this tutorial we upload the LLM using the Wallaroo MLOps API endpoint `/v1/api/models/upload_and_convert`, providing the following:\n",
    "\n",
    "* `token`:  [The authentication bearer token](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-api-guide/wallaroo-mlops-connection-guide/)\n",
    "* `hostname`:  The hostname of the Wallaroo instance the LLM is uploaded to.\n",
    "\n",
    "```bash\n",
    "curl --progress-bar -X POST \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -H \"Authorization: Bearer {token}\" \\\n",
    "  -F 'metadata={\"name\": \"byop-llama-8b-v2\", \"visibility\": \"private\", \"workspace_id\": <your-workspace-id>, \"conversion\": {\"framework\": \"custom\", \"python_version\": \"3.8\", \"requirements\": []}, \"input_schema\": \"/////3AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAQAAAB0ZXh0AAAAAAQABAAEAAAA\", \"output_schema\": \"/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA=\"};type=application/json' \\\n",
    "  -F \"file=@byop-llama3-8b-instruct-vllm.zip;type=application/octet-stream\" \\\n",
    "  https://{hostname}/v1/api/models/upload_and_convert | cat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033e290-d466-487f-93df-93af9d49bdfe",
   "metadata": {},
   "source": [
    "### Retrieve the Model\n",
    "\n",
    "Once uploaded and ready for deployment, the model is retrieved through the [`list_models` method](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-optimize/wallaroo-model-operations-model-configure/), retrieving the most recent version of the model and saving it to the `model` variable for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d3c67-3c3b-4630-8d02-933d8507a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wl.list_models()[0].versions()[-1]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705dc16-f70b-4afc-a6c8-9f693c887385",
   "metadata": {},
   "source": [
    "### Deploy LLM\n",
    "\n",
    "Deploying the Llama.cpp LLM follows these steps:\n",
    "\n",
    "* [Set the deployment configuration](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-deploy-model/wallaroo-model-operations-deploy-model-deployment-configuration/):  This sets what resources are allocated from the cluster for the LLMs exclusive use.  For this example, the following resources are allocated to the LLM:\n",
    "  * CPUs: 4\n",
    "  * RAM: 10 Gi\n",
    "  * GPUs: 1\n",
    "* Deploy the LLM:  In this phase, the LLM is added to a [Wallaroo Pipeline](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-deploy-model/) as a **model step**, then deployed with the deployment configuration.\n",
    "\n",
    "Once the model is deployed, it is ready for inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "499212ba-7a6a-4f06-a59d-908c7fefe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(model, 4) \\\n",
    "    .sidekick_memory(model, '10Gi') \\\n",
    "    .sidekick_gpus(model, 1) \\\n",
    "    .deployment_label(\"wallaroo.ai/accelerator:a1002\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0aa40-d037-4874-b662-7950cc120270",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"vllm-pipe-v9\")\n",
    "pipeline.add_model_step(model)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fca07c-1950-41cb-a81a-1f91281fd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967709f8-b1e6-4fef-9b09-e9fe1678787f",
   "metadata": {},
   "source": [
    "### Inference Requests\n",
    "\n",
    "Inference requests are submitted to deployed LLMs in Wallaroo either as pandas DataFrames, or Apache Arrow Tables.\n",
    "\n",
    "For this example, a pandas DataFrame is submitted with two columns:\n",
    "\n",
    "* `text`: The question asked of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef741c36-a98a-411b-a1a7-0d6a5bc28700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"text\": [\"Tell me about XAI.\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4235ca",
   "metadata": {},
   "source": [
    "The request is submitted to the deployed LLM, and the `generated_text` field contains the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55717d7d-8061-47d4-bf7c-7b5b1cd09940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c59f9e-30a8-42b0-b1d2-8e7f1a81348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "result = pipeline.infer(data, timeout=10000)\n",
    "end = time.time()\n",
    "\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8bcede-2c3d-4143-8199-fcce7c9fbd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c23-dd73-4d5c-8842-59a9eb9bc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"out.generated_text\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0684f59-f9b0-4396-9850-6e4984d7b4df",
   "metadata": {},
   "source": [
    "### Undeploy\n",
    "\n",
    "With the tutorial complete, we undeploy the LLM and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906949e3-b9f2-430f-920f-04caf8613011",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fb9b4d-3cec-4292-a3b6-b8bb09472488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
