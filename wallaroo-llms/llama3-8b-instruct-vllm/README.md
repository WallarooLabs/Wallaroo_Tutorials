# Llama 3 8B Instruct Inference with vLLM

## Business Context

This BYOP model uses the [`vLLM` library](https://github.com/vllm-project/vllm) and the **Llama 3 8B Instruct** LLM in order to take a text prompt from the user and generate a response as text as well.

### What is vLLM?

vLLM, or Very Large Language Model serving engine, is designed to enhance the efficiency and performance of deploying large language models (LLMs). It stands out for its innovative approach to managing LLM inference, utilizing a novel attention algorithm known as PagedAttention. This technology effectively organizes attention keys and values into smaller, manageable segments, significantly reducing memory usage and boosting throughput compared to traditional methods.

One of the key advantages of vLLM is its ability to achieve much higher throughputâ€”up to 24 times greater than HuggingFace Transformers, a widely-used LLM library. This capability allows for serving a larger number of users with fewer computational resources, making vLLM an attractive option for organizations looking to optimize their LLM deployments.

## Model Overview

- What is the framework(s) that the customer used in order to create this model? `vllm` is used in order to load the model in a more optimised fashion.
- What are the model artifacts? The original model can be found here:
  - [Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- What are the input/output data types? This `BYOP` model requires an input text, and it ouptputs whatever has been generated by the llm as text as well.
- Sample input data for testing: N/A
- Expected output data for testing: N/A

## Baseline Requirements

### Hardware Requirements (Optional)

- What did the customer use when training/testing the model? N/A
- Are there any constraints in terms of the hardware that will be used? (due to costs maybe?) GPU Only (1 A100 machine) = a2-ultragpu-1g.

### Performance Metrics (Optional)

- Does the customer have different performance metrics that they are tracking for this particular model? N/A

## Implementation details

In this section, we will emphasize the interesting parts of the BYOP code that has been developed for this multimodal model:

1. In order to run [vLLM](https://github.com/vllm-project/vllm) on CUDA, some extra arguments need to be passed to the `pip install` command, which raises some issues with the way we parse requirements in BYOP. Because of this fact, we will install `vLLM` using the `subprocess` library in `python`, straight into the Python BYOP code:

``` python
import subprocess
import sys

pip_command = (
    f'{sys.executable} -m pip install https://github.com/vllm-project/vllm/releases/download/v0.5.2/vllm-0.5.2+cu118-cp38-cp38-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118'
)

subprocess.check_call(pip_command, shell=True)
```

2. Loading the model is pretty straightforward using `vLLM`. Basically, all you need is the model weights that are found [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).

```python
def _load_model(self, model_path):
    llm = LLM(
        model=f"{model_path}/artifacts/Meta-Llama-3-8B-Instruct/"
    )

    return llm
```

### Notebook Relevant Details

#### Deployment Configuration

```python
deployment_config = DeploymentConfigBuilder() \
    .cpus(1).memory('2Gi') \
    .sidekick_cpus(model, 4) \
    .sidekick_memory(model, '10Gi') \
    .sidekick_gpus(model, 1) \
    .deployment_label("wallaroo.ai/accelerator:a100) \
    .build()
```

### How to run the BYOP?

1. Download the notebook from the `notebooks` folder.
2. Download the BYOP zip file from [here](https://storage.cloud.google.com/wallaroo-model-zoo/model-auto-conversion/auto-packaging/byop/llama/byop-llama3-8b-vllm.zip ).
3. Use the notebook to upload and deploy.
