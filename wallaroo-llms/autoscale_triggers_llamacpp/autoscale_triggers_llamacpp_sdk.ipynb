{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0285f10",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/autoscale_triggers_llamacpp).\n",
    "\n",
    "## Autoscale Triggers with Llama 3 8B with Llama.cpp Tutorial\n",
    "\n",
    "Wallaroo deployment configurations set what resources are allocated to LLMs for inference requests.  **Autoscale triggers** provide LLMs greater flexibility by:\n",
    "\n",
    "* Increasing resources to LLMs based on **scale up and down triggers**.  This decreases inference latency when more requests come in, then spools idle resources back down to save on costs.\n",
    "* Smooths the allocation of resources by optional **autoscaling windows** that allows scaling up and down over a longer period of time, preventing sudden resources spikes and drops.\n",
    "\n",
    "Autoscale triggers work through deployment configurations that have **minimum and maximum autoscale replicas** set by the parameter `replica_autoscale_min_max`.  The default minimum is 0 replicas.  Resources are scaled as follows:\n",
    "\n",
    "* 0 Replicas up:  If there is 1 or more inference requests in the queue, 1 replica is spun up to process the requests in the queue.  Additional resources are spun up based on the `autoscale_cpu_utilization` setting, where additional replicas are spun up or down when average cpu utilization across all replicas passes the `autoscale_cpu_utilization` percentage.\n",
    "* If `scale_up_queue_depth` is set: `scale_up_queue_depth` is based on the number of requests in the queue plus the requests currently being processed, divided by the number of **available** replicas.  If this threshold is exceeded, then additional replicas are spun up based on the `autoscaling_window` default of 300 seconds.\n",
    "\n",
    "This tutorial focuses on demonstrating deploying a Llama V3 8B with Llama.cpp LLM with Wallaroo through the following steps:\n",
    "\n",
    "* Uploading the LLM to Wallaroo.\n",
    "* Defining the autoscale triggers and deploying the LLM with that configuration.\n",
    "* Performing sample inferences on the deployed LLM.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use LLM Listener Monitoring to monitor LLM performance and outputs:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "Schedule your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The following tutorial requires the following:\n",
    "\n",
    "* Llama V3 8B with llama-cpp encapsulated in the Wallaroo Arbitrary Python aka BYOP Framework.  This is available through a Wallaroo representative.\n",
    "* Wallaroo version 2024.3 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34509ea-0f31-4ef6-a7c9-80915a1f0d56",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd0cd130-3eca-4c22-b0ea-2e5df7c21f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "\n",
    "from wallaroo.pipeline import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.object import EntityNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beeb163",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is established via the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577677ce-3152-47ee-9d7f-8ac59cdfee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759df0f",
   "metadata": {},
   "source": [
    "### Create the Workspace\n",
    "\n",
    "We will create or retrieve a workspace and call it the `llamacpp-testing`, then set it as current workspace environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17a9f542-946a-441d-bb05-982e377fc933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set workspace to `llamacpp-testing`\n",
    "workspace = wl.get_workspace(\"llamacpp-testing\", create_if_not_exist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e41916bf-f3d6-400e-8da8-a2def83785aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'llamacpp-testing', 'id': 37, 'archived': False, 'created_by': 'gabriel.sandu@wallaroo.ai', 'created_at': '2024-10-09T15:47:16.888728+00:00', 'models': [{'name': 'byop-llama3-q2-max-tokens', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 9, 15, 51, 19, 67945, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 9, 15, 51, 19, 67945, tzinfo=tzutc())}, {'name': 'byop-llama3-q2', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 9, 16, 1, 16, 48599, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 9, 16, 1, 16, 48599, tzinfo=tzutc())}, {'name': 'byop-llamacpp-llama3-8b-q5-v1', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 15, 20, 25, 52, 719031, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 15, 20, 25, 52, 719031, tzinfo=tzutc())}, {'name': 'byop-llamacpp-llama3-8b-q5-v2', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 15, 21, 1, 19, 192231, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 15, 21, 1, 19, 192231, tzinfo=tzutc())}, {'name': 'byop-llamacpp-llama3-8b-q5-v3', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 15, 21, 47, 11, 671499, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 15, 21, 47, 11, 671499, tzinfo=tzutc())}, {'name': 'byop-llamacpp-llama3-8b-q5-v4', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 15, 22, 9, 25, 155660, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 15, 22, 9, 25, 155660, tzinfo=tzutc())}, {'name': 'byop-llamacpp-llama3-8b-q5-v5', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 15, 22, 23, 50, 164895, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 15, 22, 23, 50, 164895, tzinfo=tzutc())}, {'name': 'byop-llamacpp-llama3-8b-instruct-q5', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 15, 22, 39, 0, 340823, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 15, 22, 39, 0, 340823, tzinfo=tzutc())}, {'name': 'byop-llama3-8b-vllm', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 10, 16, 12, 59, 43, 872101, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 10, 16, 12, 59, 43, 872101, tzinfo=tzutc())}], 'pipelines': [{'name': 'scale-test-cp', 'create_time': datetime.datetime(2024, 10, 15, 13, 45, 48, 652485, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama3-8b-instruct-llamacpp-v1', 'create_time': datetime.datetime(2024, 10, 15, 20, 31, 11, 992396, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'scale-test-fm', 'create_time': datetime.datetime(2024, 10, 11, 18, 37, 35, 718535, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama3-8b-instruct-llamacpp-v2', 'create_time': datetime.datetime(2024, 10, 15, 20, 42, 26, 447368, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'scale-test-jb', 'create_time': datetime.datetime(2024, 10, 9, 16, 23, 29, 380756, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'scale-test-fm-3', 'create_time': datetime.datetime(2024, 10, 18, 18, 48, 55, 887911, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama3-8b-instruct-llamacpp-t4', 'create_time': datetime.datetime(2024, 10, 15, 20, 46, 52, 471972, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama3-8b-instruct-llamacpp', 'create_time': datetime.datetime(2024, 10, 15, 22, 11, 44, 69111, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama3-8b-instruct-llamacpp2', 'create_time': datetime.datetime(2024, 10, 15, 22, 25, 0, 995668, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama3-8b-instruct-llamacpp3', 'create_time': datetime.datetime(2024, 10, 15, 22, 40, 20, 354793, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'scale-test-fm-2', 'create_time': datetime.datetime(2024, 10, 14, 15, 48, 31, 681620, tzinfo=tzutc()), 'definition': '[]'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed3b1b2",
   "metadata": {},
   "source": [
    "### Retrieve Model\n",
    "\n",
    "In this example, the model is already uploaded to this workspace.  We retrieve it with the `wallaroo.client.Client.get_model` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e5a45d1-344c-449a-acfc-8e4c301dc967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>byop-llamacpp-llama3-8b-instruct-q5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>4511af71-bdcb-4604-85c0-10ef31a2e319</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>byop-llamacpp-llama3-8b-q5.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>f15edeab3c7fbf08579703cebc415d33085dbfe08eeae2472f8442a2a2124aea</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.3.0-main-5739</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-15-Oct 22:39:51</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>37</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>llamacpp-testing</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'byop-llamacpp-llama3-8b-instruct-q5', 'version': '4511af71-bdcb-4604-85c0-10ef31a2e319', 'file_name': 'byop-llamacpp-llama3-8b-q5.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.3.0-main-5739', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2024, 10, 15, 22, 39, 51, 164042, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = wl.get_model(\"byop-llamacpp-llama3-8b-instruct-q5\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5907773",
   "metadata": {},
   "source": [
    "### Deploy the LLM\n",
    "\n",
    "The LLM is deployed through the following process:\n",
    "\n",
    "* Create a Wallaroo Pipeline and Set the LLM as a Pipeline Step:  This sets the process for how inference inputs is passed through deployed LLMs and supporting ML models.\n",
    "* Define the Deployment Configuration:  This sets what resources are allocated for the LLM's use from the clusters.\n",
    "* Deploy the LLM:  This deploys the LLM with the defined deployment configuration and pipeline steps.\n",
    "\n",
    "#### Build Pipeline and Set Steps\n",
    "\n",
    "In this process, we create the pipeline, then assign the LLM as a pipeline step to receive inference data and process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c42fa1c-4e16-4c4e-b121-76b7821e3928",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"scale-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dc6425e-3fb6-4a56-ad4c-06824b4cf592",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "614a5b53-6112-4a87-9fac-1fb0fd6ef3f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>scale-test</td></tr><tr><th>created</th> <td>2024-10-23 13:04:03.411687+00:00</td></tr><tr><th>last_updated</th> <td>2024-10-23 13:32:37.517314+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>37</td></tr><tr><th>workspace_name</th> <td>llamacpp-testing</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>60bc4c5d-a4ee-48ae-948e-b3bf3aab1da9, 90bdfec7-7dfe-4cea-b8b1-10b104fb91f8, 725b9957-8e4a-409a-ad46-122b0016a4c9</td></tr><tr><th>steps</th> <td>byop-llama3-8b-vllm</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'scale-test', 'create_time': datetime.datetime(2024, 10, 23, 13, 4, 3, 411687, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'byop-llamacpp-llama3-8b-instruct-q5', 'version': '4511af71-bdcb-4604-85c0-10ef31a2e319', 'sha': 'f15edeab3c7fbf08579703cebc415d33085dbfe08eeae2472f8442a2a2124aea'}]}}]\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.add_model_step(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316a85f3",
   "metadata": {},
   "source": [
    "#### Define the Deployment Configuration with Autoscaling Triggers\n",
    "\n",
    "For this step, the following resources are defined for allocation to the LLM when deployed through the class `wallaroo.deployment_config.DeploymentConfigBuilder`:\n",
    "\n",
    "* Cpus:  4\n",
    "* Memory:  6 Gi\n",
    "* Gpus: 1.  When setting `gpus` for deployment, the `deployment_label` must be defined to select the appropriate nodepool with the requested gpu resources.\n",
    "\n",
    "As part of the deployment configuration, we set the autoscale triggers with the following parameters.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| `scale_up_queue_depth` | `(queue_depth: int)`  | The queue trigger for autoscaling additional replicas up.  This requires the deployment configuration parameter `replica_autoscale_min_max` is set.  `scale_up_queue_depth` is determined by the formula `(number of requests in the queue + requests being processed) / (The number of available replicas set over the autoscaling_window)`.  This field **overrides** the deployment configuration parameter `cpu_utilization`.  The `scale_up_queue_depth` applies to **all** resources in the deployment configuration.  |\n",
    "| `scale_down_queue_depth` | `(queue_depth: int)`, *Default: 1* | Only applies with `scale_up_queue_depth` is configured.  The queue trigger for autoscaling replicas down.  Scales down resources based on the formula `(number of requests in the queue + requests being processed) / (The number of available replicas set over the autoscaling_window)`. |\n",
    "| `autoscaling_window` | `(window_seconds: int)` (*Default: 300*, *Minimum allowed: 60*) | The period over which to scale up or scale down resources.  **Only** applies when `scale_up_queue_depth` is configured. |\n",
    "| `replica_autoscale_min_max` | `(maximum: int, minimum: int = 0)` | Provides replicas to be scaled from 0 to some maximum number of replicas.  This allows deployments to spin up additional replicas as more resources are required, then spin them back down to save on resources and costs. |\n",
    "\n",
    "For our example:\n",
    "\n",
    "* `scale_up_queue_depth`: 5\n",
    "* `scale_down_queue_depth`: 1\n",
    "* `autoscaling_window`: 60 (seconds)\n",
    "* `replica_autoscale_min_max`: 2 (maximum), 0 (minimum)\n",
    "* Resources per replica:\n",
    "  * Cpus: 4\n",
    "  * Gpu: 1\n",
    "  * Memory: 6Gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67035dd0-332d-4feb-a87b-0e702168cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu deployment\n",
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(model, 4) \\\n",
    "    .sidekick_memory(model, '6Gi') \\\n",
    "    .sidekick_gpus(model, 1) \\\n",
    "    .deployment_label(\"wallaroo.ai/accelerator:t4\") \\\n",
    "    .replica_autoscale_min_max(2,0) \\\n",
    "    .scale_up_queue_depth(5) \\\n",
    "    .scale_down_queue_depth(1) \\\n",
    "    .autoscaling_window(60) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98751a12",
   "metadata": {},
   "source": [
    "#### Deploy the Pipeline\n",
    "\n",
    "With the parameters set and the deployment configuration with autoscale triggers defined, we deploy the LLM through the `pipeline.deploy` method and specify the deployment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb187e1d-ffaf-4307-b05c-b7cf94fdaa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab2f46",
   "metadata": {},
   "source": [
    "#### Verify Pipeline Deployment Status\n",
    "\n",
    "Before submitting inference requests, we verify the pipeline deployment status is `Running`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2db78a0c-a548-410e-8010-ddb735c51171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.4.7.8',\n",
       "   'name': 'engine-74c54c9478-7vs6l',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'scale-test',\n",
       "      'status': 'Running',\n",
       "      'version': '68ea1561-f5f9-42b6-b0c4-800922dc27af'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 19,\n",
       "      'name': 'byop-llamacpp-llama3-8b-instruct-q5',\n",
       "      'sha': 'f15edeab3c7fbf08579703cebc415d33085dbfe08eeae2472f8442a2a2124aea',\n",
       "      'status': 'Running',\n",
       "      'version': '4511af71-bdcb-4604-85c0-10ef31a2e319'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.4.1.26',\n",
       "   'name': 'engine-lb-6b59985857-97sdr',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.4.7.9',\n",
       "   'name': 'engine-sidekick-byop-llamacpp-llama3-8b-instruct-q5-19-85ctstwl',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d580d4a",
   "metadata": {},
   "source": [
    "### Sample Inference\n",
    "\n",
    "Once the LLM is deployed, we'll perform an inference with the `wallaroo.pipeline.Pipeline.infer` method, which accepts either a pandas DataFrame or an Apache Arrow table.\n",
    "\n",
    "For this example, we'll create a pandas DataFrame with a text query and submit that for our inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "191c4363-b971-470e-a4bd-95be49ac2fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': ['Describe what roland garros is']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53172c34-76e3-437d-a402-91d59db29bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Roland Garros, also known as the French Open, is a prestigious Grand Slam tennis tournament held annually in Paris, France. It\\'s one of the four majors in professional tennis and is considered one of the most iconic and challenging tournaments in the sport.\\n\\nRoland Garros takes place over two weeks in late May and early June on clay courts at the Stade Roland-Garros stadium. The event has a rich history, dating back to 1891, and is often referred to as the \"most romantic\" Grand Slam due to its unique atmosphere and stunning surroundings.\\n\\nThe tournament is named after Roland Garros, a French aviator, engineer, and writer who was also an avid tennis player. He was a pioneer in aviation and was credited with being the first pilot to cross the Mediterranean Sea by air.\\n\\nRoland Garros features five main events: men\\'s singles, women\\'s singles, men\\'s doubles, women\\'s doubles, and mixed doubles. The tournament attracts some of the world\\'s top tennis players, with many considering it a highlight of their professional careers.\\n\\nThe French Open is known for its challenging conditions, particularly on the clay courts, which are renowned for their slow pace and high bounce. This requires players to have strong footwork, endurance, and tactical awareness to outmaneuver their opponents.\\n\\nThroughout the tournament, fans can expect thrilling matches, dramatic upsets, and memorable moments that often define the careers of tennis superstars. Roland Garros is truly a special event in the world of tennis, and its rich history, stunning atmosphere, and iconic status make it an unforgettable experience for players and spectators alike.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=pipeline.infer(data, timeout=10000)\n",
    "result[\"out.generated_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d5ea4",
   "metadata": {},
   "source": [
    "### Undeploy LLM\n",
    "\n",
    "With the tutorial complete, we undeploy the LLM and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c349928-10fc-4eb7-8747-11edc31c40bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s .................................... ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>scale-test-jb</td></tr><tr><th>created</th> <td>2024-10-09 16:23:29.380756+00:00</td></tr><tr><th>last_updated</th> <td>2024-10-11 17:14:39.088831+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>37</td></tr><tr><th>workspace_name</th> <td>llamacpp-testing</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>a1301693-88cb-4219-a037-ce009e030aa4, 428d18a3-8321-4aef-9d41-b00c97aab6f6, 513ce8ec-0eae-47ee-b1d3-3892d9f0b8f9, 545c1850-3403-41ea-9ed9-5fd820e55f50, 6e612ae7-25ad-4df9-b91c-9e6e11d69506, 2319a92a-14ae-419a-864f-63a2b6911cd4, 8f73fc75-d6f4-432f-a77b-f5eb588f4696, 98336b0a-7503-4ace-ad52-dff236909420</td></tr><tr><th>steps</th> <td>byop-llama3-q2-max-tokens</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'scale-test-jb', 'create_time': datetime.datetime(2024, 10, 9, 16, 23, 29, 380756, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'byop-llama3-q2-max-tokens', 'version': '2b73abb1-7ac2-45f5-9908-300478346f60', 'sha': '7ea5ea4616c293038acbea602b8ddec2bbba99c1c41dc45a372eb32fe9743620'}]}}]\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
