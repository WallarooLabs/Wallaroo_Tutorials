{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a815751-facd-4a95-91be-9f9ef38c97f8",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/llm_dynamic_batching_tutorial).\n",
    "\n",
    "## Dynamic Batching with Llama 3 8B Instruct vLLM Tutorial\n",
    "\n",
    "When multiple inference requests are sent from one or multiple clients, a **Dynamic Batching Configuration** accumulates those inference requests as one \"batch\", and processed at once.  This increases efficiency and inference result performance by using resources in one accumulated batch rather than starting and stopping for each individual request.  Once complete, the individual inference results are returned back to each client.  \n",
    "\n",
    "The following tutorial demonstrates configuring a Llama 3 8B Instruct vLLM with a Wallaroo Dynamic Batching Configuration.\n",
    "\n",
    "This example uses the Llama V3 Instruct LLM.  For access to these sample models and for a demonstration of how to use LLM Listener Monitoring to monitor LLM performance and outputs:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial demonstrates using Wallaroo to:\n",
    "\n",
    "* Upload a LLM\n",
    "* Define a Dynamic Batching Configuration and apply it to the LLM.\n",
    "* Deploy a the LLM with a Deployment Configuration that allocates resources to the LLM; the Dynamic Batch Configuration is applied at the LLM level, so it inherited during deployment.\n",
    "* Demonstrate how to perform a sample inference.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The following tutorial requires the following:\n",
    "\n",
    "* Llama V3 Instruct LLM encapsulated in the Wallaroo Arbitrary Python aka BYOP Framework.  This is available through a Wallaroo representative.\n",
    "* Wallaroo version 2024.4 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b47f6d-af56-4659-926d-0f5aa1240cfb",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca0bc3e-9a4a-4b34-bb17-bd0fffb6a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f4fe1c",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e6712f9-871f-4224-8a9e-d44520a25628",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558ea37-f227-45dc-a637-c8a7a0e552db",
   "metadata": {},
   "source": [
    "### Upload Model\n",
    "\n",
    "For our example, we'll upload the model using the Wallaroo MLOps API.  The method `wallaroo.client.Client.generate_upload_model_api_command` generates a `curl` script for uploading models to Wallaroo via the Wallaroo MLOps API.  The generated `curl` script is based on the Wallaroo SDK user's current workspace.  This is useful for environments that do not have the Wallaroo SDK installed, or uploading very large models (10 gigabytes or more).\n",
    "\n",
    "This method takes the following parameters:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| **base_url** | *String* (*Required*) | The Wallaroo domain name.  For example:  `wallaroo.example.com`. |\n",
    "| **name** | *String* (*Required*) | The name to assign the model at upload.  This must match DNS naming conventions. |\n",
    "| **path** | *String* (*Required*) | Path to the ML or LLM model file. |\n",
    "| **framework** | *String* (*Required*) | The framework from `wallaroo.framework.Framework`  For a complete list, see [Wallaroo Supported Models]({{<ref \"wallaroo-model-operations-upload-register#wallaroo-supported-models\">}}). |\n",
    "| **input_schema** |  *String* (*Required*) | The model’s input schema in PyArrow.Schema format. |\n",
    "| **output_schema** |  *String* (*Required*) | The model’s output schema in PyArrow.Schema format. |\n",
    "\n",
    "This outputs a `curl` command in the following format (indentions added for emphasis).  The sections marked with `{}` represent the variable names that are injected into the script from the above parameter or from the current SDK session:\n",
    "\n",
    "* `{Current Workspace['id']}`: The value of the `id` for the current workspace.\n",
    "* `{Bearer Token}`: The bearer token used to authentication to the Wallaroo MLOps API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf32f67-c943-4e0c-a57a-1a94d7bdae3b",
   "metadata": {},
   "source": [
    "#### Define And Encode the Schemas\n",
    "\n",
    "We define the **input** and **output** schemas in Apache PyArrow format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4229a5-be9b-4c70-8ff1-0b4bd3abf707",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('text', pa.string()),\n",
    "])\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b7798",
   "metadata": {},
   "source": [
    "#### Generate the curl Command\n",
    "\n",
    "We generate the `curl` command through the `generate_upload_model_api_command` as follows - replace the `base_url` with the one used for your Wallaroo instance.\n",
    "\n",
    "Use the curl command to upload the model to the Wallaroo instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b10c8c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    curl --progress-bar -X POST            -H \"Content-Type: multipart/form-data\"            -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJYNFc1RzNBb19qTnRDSlJNRXhOVTM1X1NrNHYwMVdRQVRabm9WUC1NQ0RZIn0.eyJleHAiOjE3MjgwNTgxNjYsImlhdCI6MTcyODA1ODEwNiwianRpIjoiYWZjYTQwZGMtZTI3Yi00YjgwLThmMmQtMzUyNmM2ODE5YWI3IiwiaXNzIjoiaHR0cHM6Ly9kb2MtdGVzdC53YWxsYXJvb2NvbW11bml0eS5uaW5qYS9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJhdWQiOlsibWFzdGVyLXJlYWxtIiwiYWNjb3VudCJdLCJzdWIiOiJmNzVhODYyOS03MGFiLTQxMDAtOGIzNy0wNGNmNzllNjY3ZWUiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJzZGstY2xpZW50Iiwic2Vzc2lvbl9zdGF0ZSI6ImVjZTIzMzJjLTA0NGItNDU3Ni05MWNjLTRmYzEwYTliMzc1ZiIsImFjciI6IjEiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiY3JlYXRlLXJlYWxtIiwiZGVmYXVsdC1yb2xlcy1tYXN0ZXIiLCJvZmZsaW5lX2FjY2VzcyIsImFkbWluIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJtYXN0ZXItcmVhbG0iOnsicm9sZXMiOlsidmlldy1yZWFsbSIsInZpZXctaWRlbnRpdHktcHJvdmlkZXJzIiwibWFuYWdlLWlkZW50aXR5LXByb3ZpZGVycyIsImltcGVyc29uYXRpb24iLCJjcmVhdGUtY2xpZW50IiwibWFuYWdlLXVzZXJzIiwicXVlcnktcmVhbG1zIiwidmlldy1hdXRob3JpemF0aW9uIiwicXVlcnktY2xpZW50cyIsInF1ZXJ5LXVzZXJzIiwibWFuYWdlLWV2ZW50cyIsIm1hbmFnZS1yZWFsbSIsInZpZXctZXZlbnRzIiwidmlldy11c2VycyIsInZpZXctY2xpZW50cyIsIm1hbmFnZS1hdXRob3JpemF0aW9uIiwibWFuYWdlLWNsaWVudHMiLCJxdWVyeS1ncm91cHMiXX0sImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIGVtYWlsIHByb2ZpbGUiLCJzaWQiOiJlY2UyMzMyYy0wNDRiLTQ1NzYtOTFjYy00ZmMxMGE5YjM3NWYiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaHR0cHM6Ly9oYXN1cmEuaW8vand0L2NsYWltcyI6eyJ4LWhhc3VyYS11c2VyLWlkIjoiZjc1YTg2MjktNzBhYi00MTAwLThiMzctMDRjZjc5ZTY2N2VlIiwieC1oYXN1cmEtdXNlci1lbWFpbCI6ImpvaG4uaGFuc2FyaWNrQHdhbGxhcm9vLmFpIiwieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoiYWRtaW5fdXNlciIsIngtaGFzdXJhLWFsbG93ZWQtcm9sZXMiOlsidXNlciIsImFkbWluX3VzZXIiXSwieC1oYXN1cmEtdXNlci1ncm91cHMiOiJ7fSJ9LCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJqb2huLmhhbnNhcmlja0B3YWxsYXJvby5haSIsImVtYWlsIjoiam9obi5oYW5zYXJpY2tAd2FsbGFyb28uYWkifQ.L8JQOjXeNOLzhwSVzcDFUpdKHWzgN_H5DT12kh2CRb-mMJNeFlFzQT99piTgsv6QmYWXbaLU1aM86aQYjJX_5tdXj5CfttvJvM3xVovOiv_yt_7A1VGQa6zILAA-zYws5o6f9HesK6dOatuuWrl_vZyEMvalJom_IH3vQxEjPwKiGsZuirH38XEUmFcoLggxXTnfTtIyho_4Fl_X74U1lL-Xpw7LT8P7B9XzRs_ix4tO8HRTGuuxBj6IdeWxDlP6ZDVf_UTAeZGHAf6xguvX2DUZu1-49qVqcaA34hQzeTW_QnitGhcmQ4eoJYUbQCvM8EPDB1Uj37PgfctBx6xc9A\"            -F \"metadata={\"name\": \"llama3-70b-instruct\", \"visibility\": \"private\", \"workspace_id\": 6, \"conversion\": {\"arch\": \"x86\", \"accel\": \"none\", \"framework\": \"custom\", \"python_version\": \"3.8\", \"requirements\": []}, \"input_schema\": \"/////3AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAQAAAB0ZXh0AAAAAAQABAAEAAAA\", \"output_schema\": \"/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA=\"};type=application/json\"            -F \"file=@byop-llama-3-80b-instruct.zip;type=application/octet-stream\"        https://example.wallaroo.ai//v1/api/models/upload_and_convert\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wl.generate_upload_model_api_command(\n",
    "    base_url='https://example.wallaroo.ai/',\n",
    "    name='llama3-8b-vllm-max-tokens-no-lock-v1', \n",
    "    path='byop-llama-3-80b-instruct.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce37e4a-959e-40b1-a9b1-d9cee8b053a2",
   "metadata": {},
   "source": [
    "### Retrieve the LLM\n",
    "\n",
    "Once uploaded, we retrieve the LLM with the `wallaroo.client.Client.get_models` command, specifying the same model name used during the upload command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313e9fca-5188-4df5-914b-0b120c261c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = wl.get_model('llama3-8b-vllm-max-tokens-no-lock-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0270ace4-4737-45cc-b32f-6f51c83989a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>llama3-8b-vllm-max-tokens-no-lock-v1</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>096963bc-fc88-483a-9cdc-f606e0d57e26</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>llama3-8b-vllm.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>b86841ca5d0976f6d688342caebcef2bcdbafd4f8d833ed9060f161a6a8b854c</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>ghcr.io/wallaroolabs/mac-deploy:v2024.3.0-main-5654</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-18-Sep 16:14:16</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>1</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>panagiotis.vardanis@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'llama3-8b-vllm-max-tokens-no-lock-v1', 'version': '096963bc-fc88-483a-9cdc-f606e0d57e26', 'file_name': 'llama3-8b-vllm.zip', 'image_path': 'ghcr.io/wallaroolabs/mac-deploy:v2024.3.0-main-5654', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2024, 9, 18, 16, 14, 16, 933130, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0767f9-8b74-4025-831e-49ebb06599b6",
   "metadata": {},
   "source": [
    "### Define the Dynamic Batching Config\n",
    "\n",
    "The **Dynamic Batch Config** is configured in the Wallaroo SDK via the from `wallaroo.dynamic_batching_config.DynamicBatchingConfig` object, which takes the following parameters.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| `max_batch_delay_ms` | *Integer* (*Default: 10*) | Set the maximum batch delay in **milliseconds**. |\n",
    "| `batch_size_target` | *Integer* (*Default: 4*) | Set the target batch size; can not be less than or equal to zero. |\n",
    "| `batch_size_limit` | *Integer* (*Default: None*) | Set the batch size limit; can not be less than or equal to zero.  This is used to control the maximum batch size. |\n",
    "\n",
    "With the above set, the Dynamic Batch Config is created via the `wallaroo.dynamic_batching_config.DynamicBatchingConfig.build()` command.\n",
    "\n",
    "For this example, we will configure the LLM with the following Dynamic Batch Config:\n",
    "\n",
    "* `max_batch_delay_ms`=1000\n",
    "* `batch_size_target`=8\n",
    "* `batch_size_limit`=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "339094e9-633b-434e-b12d-8489396a785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wallaroo.dynamic_batching_config import DynamicBatchingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98474742-626d-4db2-b58a-ff2f3e25a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = llm_model.configure(input_schema=input_schema, \n",
    "                                output_schema=output_schema, \n",
    "                                dynamic_batching_config=DynamicBatchingConfig(max_batch_delay_ms=1000, \n",
    "                                                                              batch_size_target=8, \n",
    "                                                                              batch_size_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5f5267",
   "metadata": {},
   "source": [
    "### Deploy LLM with Dynamic Batch Configuration\n",
    "\n",
    "Deploying a LLM with a Dynamic Batch configuration requires the same steps as deploying a LLM **without** a Dynamic Batch configuration:\n",
    "\n",
    "* Define the deployment configuration to set the number of CPUs, RAM, and GPUs per replica.\n",
    "* Create a Wallaroo pipeline and add the LLM with the Dynamic Batch configuration as a model step.\n",
    "* Deploy the Wallaroo pipeline with the deployment configuration.\n",
    "\n",
    "The deployment configuration sets what resources are allocated to the LLM upon deployment.  For this example, we allocate the following resources:\n",
    "\n",
    "* cpus: 4\n",
    "* memory:  15Gi\n",
    "* gpus: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7a7cf-5fc8-44ec-b16b-d0bd82bb22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .replica_count(3) \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(llm_model, 4) \\\n",
    "    .sidekick_memory(llm_model, '15Gi') \\\n",
    "    .sidekick_gpus(llm_model, 1) \\\n",
    "    .deployment_label(\"wallaroo.ai/accelerator:a100\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f114239",
   "metadata": {},
   "source": [
    "We create the pipeline with the `wallaroo.client.Client.build_pipeline` method.\n",
    "\n",
    "Wallaroo pipelines are created with the `wallaroo.client.Client.build_pipeline` method.  [Pipeline steps](https://staging.docs.wallaroo.ai/202402/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-deploy-model/#pipeline-steps) are used to determine how inference data is provided to the LLM.  For Dynamic Batching, only **one pipeline step** is allowed.\n",
    "\n",
    "The following demonstrates creating a Wallaroo pipeline, and assigning the LLM as a pipeline step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763808e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = \"llama-3-8b-vllm-dynamic-1000-8-10-4096-lock-fix\"\n",
    "pipeline = wl.build_pipeline(pipeline_name)\n",
    "pipeline.add_model_step(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91a25c9",
   "metadata": {},
   "source": [
    "With LLM, deployment configuration, and pipeline ready, we can deploy.  Note that the Dynamic Batch Config is not specified during the deployment - that is assigned to the LLM, and inherits those settings for its deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0c6c5-bee6-4be3-9fd3-67f30532a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de95ca9-0a64-4d8f-b91f-60b1527e26f1",
   "metadata": {},
   "source": [
    "### Sample Inference\n",
    "\n",
    "Once the LLM is deployed, we'll perform an inference with the `wallaroo.pipeline.Pipeline.infer` method, which accepts either a pandas DataFrame or an Apache Arrow table.\n",
    "\n",
    "For this example, we'll create a pandas DataFrame with a text query and submit that for our inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c9feef8-ce6d-4aca-9f2e-2105a8efd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': ['Describe what Wallaroo.AI is']})\n",
    "results = pipeline.infer(data, timeout=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d038-84e0-4174-a05d-342e7f14586b",
   "metadata": {},
   "source": [
    "### Undeploy LLM\n",
    "\n",
    "With the tutorial complete, we undeploy the LLM and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6cb5e52-6c24-4a85-ab62-30875554c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s ...................................... ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>llama-3-8b-vllm-end-token</td></tr><tr><th>created</th> <td>2024-09-09 12:32:34.262824+00:00</td></tr><tr><th>last_updated</th> <td>2024-09-09 14:07:15.755138+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>1</td></tr><tr><th>workspace_name</th> <td>panagiotis.vardanis@wallaroo.ai - Default Workspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>30d0a13a-3d69-4cb6-83ae-6ef4935f211e, c192a3a7-d5f7-448f-956a-e872c2fc941b, 7d4a2b20-a0bc-47c0-965f-c4940211c0cc</td></tr><tr><th>steps</th> <td>llama3-8b-vllm-max-tokens-v3</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'llama-3-8b-vllm-end-token', 'create_time': datetime.datetime(2024, 9, 9, 12, 32, 34, 262824, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'llama3-8b-vllm-max-tokens-v3', 'version': '7d657393-e85f-4ab7-a117-d6c2478f46ea', 'sha': 'ad2454b92b113379acc33c0bcb4e427ea356a8edb6808901b703585c9561a8ef'}]}}]\"}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2024.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
