{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd16a788-cb70-4706-ad63-2ffde0c05cfe",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/2024.1_llms/wallaroo-llms/llm-monitoring/llm-listener-monitoring).\n",
    "\n",
    "## LLM Listener Monitoring with Llama V3 Instruct\n",
    "\n",
    "The following example demonstrates using LLM Listeners to monitor a deployed Llama V3 Instruct LLM and score it based on a set of criteria.\n",
    "\n",
    "This example uses the Llama V3 Instruct LLM.  For access to these sample models and for a demonstration of how to use LLM Listener Monitoring to monitor LLM performance and outputs:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "LLM Listeners leverage [Wallaroo Inference Automation](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-automate/).  LLM Listeners are offline processes that score the LLM's inference outputs against standard metrics including:\n",
    "\n",
    "* Toxicity\n",
    "* Sentiment\n",
    "* Profanity\n",
    "* Hate\n",
    "* Etc\n",
    "\n",
    "Users can also create custom LLM Listeners to score the LLM against custom metrics.  LLM Listeners are composed of models trained to evaluate LLM outputs, so can be updated or refined according to the organization's needs.\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial demonstrates the following:\n",
    "\n",
    "* Upload a LLM Listener developed to score LLMs off a set of standard criteria.\n",
    "* Using Wallaroo Inference Automation, execute the LLM Listener as a task to evaluate the LLama V3 Instruct LLM and display the scores.\n",
    "\n",
    "## Tutorial Steps\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fd2e7a-1a81-4959-a148-209861493ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# used to display DataFrame information without truncating\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import pyarrow as pa\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a511041",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdec6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client(request_timeout=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2f781-7288-4b58-aa11-cad4eaf25f0c",
   "metadata": {},
   "source": [
    "### Set Workspace and Variables\n",
    "\n",
    "The following creates or connects to an existing workspace, and sets it as the current workspace.  For more details on Wallaroo workspaces, see [Wallaroo Workspace Management Guide](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-optimize/wallaroo-workspace-management/).\n",
    "\n",
    "We will set the variables used for our deployed LLM model, and the models used for our LLM Listener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36769fcf-e7b6-4343-8aa4-9c8ced637e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'llm-models', 'id': 322863, 'archived': False, 'created_by': 'adf08921-fc3a-4018-b55f-775cd0796538', 'created_at': '2024-03-25T20:33:10.564383+00:00', 'models': [{'name': 'llama-instruct', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 3, 25, 20, 53, 31, 707885, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 3, 25, 20, 53, 31, 707885, tzinfo=tzutc())}, {'name': 'llama-v2', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 4, 18, 20, 58, 52, 684374, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 4, 18, 20, 58, 52, 684374, tzinfo=tzutc())}, {'name': 'llama3-instruct', 'versions': 2, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 5, 1, 19, 19, 18, 437490, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 5, 1, 18, 13, 47, 784249, tzinfo=tzutc())}, {'name': 'toxic-bert', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2024, 5, 2, 23, 22, 2, 675607, tzinfo=tzutc()), 'created_at': datetime.datetime(2024, 5, 2, 23, 22, 2, 675607, tzinfo=tzutc())}], 'pipelines': [{'name': 'llama-instruct-pipeline', 'create_time': datetime.datetime(2024, 4, 11, 17, 5, 46, 75486, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama2-pipeline', 'create_time': datetime.datetime(2024, 4, 18, 21, 17, 44, 893427, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llamav3-instruct', 'create_time': datetime.datetime(2024, 5, 1, 19, 51, 8, 240637, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'llama-shadow', 'create_time': datetime.datetime(2024, 5, 2, 21, 50, 54, 293036, tzinfo=tzutc()), 'definition': '[]'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace_name = \"llm-models\"  \n",
    "model_name = \"toxic-bert\"\n",
    "post_model_name = \"postprocess\"\n",
    "\n",
    "wl.set_current_workspace(wl.get_workspace(workspace_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acf5f98-2156-442f-9b09-a4d732be9cf9",
   "metadata": {},
   "source": [
    "### Upload LLM Listener Models and Create a Monitoring Pipeline\n",
    "\n",
    "This monitoring pipeline consists of a [Hugging Face sentiment analyzer](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/#wallaroo-supported-models) and a [BYOP post-processing](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/#wallaroo-supported-models) step.\n",
    "\n",
    "The following models are used:\n",
    "\n",
    "* `toxic_bert`: A [Hugging Face Text Classification]({{<ref \"wallaroo-model-operations-upload-register#wallaroo-supported-models\">}}) model that evaluates LLM outputs and outputs an array of scores.\n",
    "* `postprocess`:  A [Python model]({{<ref \"wallaroo-model-operations-upload-register#wallaroo-supported-models\">}}) that takes the `toxic_bert` outputs and converts them into the following field outputs, scored from 0 to 1, with 1 being the worst:\n",
    "  * `identity_hate`\n",
    "  * `insult`\n",
    "  * `obscene`\n",
    "  * `severe_toxic`\n",
    "  * `threat`\n",
    "  * `toxic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf56a60a-21b9-46f8-9863-109a4bd80b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime....................................................successful\n",
      "\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# upload the sentiment analyzer\n",
    "\n",
    "input_schema = pa.schema([\n",
    "     pa.field('inputs', pa.string()), # required\n",
    "     pa.field('top_k', pa.int64()),  \n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "     pa.field('label', pa.list_(pa.string(), list_size=6)), # list with a number of items same as top_k, list_size can be skipped but may lead in worse performance\n",
    "     pa.field('score', pa.list_(pa.float64(), list_size=6)), # list with a number of items same as top_k, list_size can be skipped but may lead in worse performance\n",
    "])\n",
    "\n",
    "framework=Framework.HUGGING_FACE_TEXT_CLASSIFICATION\n",
    "model_file_name = './models/unitary-toxic-bert.zip'\n",
    "\n",
    "bert_model = wl.upload_model(model_name,\n",
    "                         model_file_name,\n",
    "                         framework=framework,\n",
    "                         input_schema=input_schema,\n",
    "                         output_schema=output_schema,\n",
    "                         convert_wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4103ed05-1cbc-4362-84f7-0569cba2410f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the postprocessor \n",
    "\n",
    "input_schema = pa.schema([\n",
    "        pa.field('label', pa.list_(pa.string(), list_size=6)), # list with a number of items same as top_k, list_size can be skipped but may lead in worse performance\n",
    "        pa.field('score', pa.list_(pa.float64(), list_size=6)), # list with a number of items same as top_k, list_size can be skipped but may lead in worse performance\n",
    "    ])\n",
    "\n",
    "# Define the schema for the 'output' DataFrame\n",
    "output_schema = pa.schema([\n",
    "    pa.field('identity_hate', pa.float64()),\n",
    "    pa.field('insult', pa.float64()),        \n",
    "    pa.field('obscene', pa.float64()),      \n",
    "    pa.field('severe_toxic', pa.float64()),  \n",
    "    pa.field('threat', pa.float64()),        \n",
    "    pa.field('toxic', pa.float64())           \n",
    "])\n",
    "\n",
    "# upload the post process model\n",
    "post_model = wl.upload_model(\"postprocess\", \n",
    "                             \"./models/postprocess.zip\", \n",
    "                             framework=wallaroo.framework.Framework.PYTHON,\n",
    "                             input_schema=input_schema, \n",
    "                             output_schema=output_schema \n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5563a45-172a-4d3f-92f3-8342da994e62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>toxic-bert</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>e511643c-30a4-48b9-a45e-f458d991a916</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>unitary-toxic-bert.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>30b5c2d0c1a2102ad63ef7d84e953b018b45a0c021ea14916708ea1c8142ff38</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mlflow-deploy:v2023.4.2-4668</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-02-May 23:26:34</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'toxic-bert', 'version': 'e511643c-30a4-48b9-a45e-f458d991a916', 'file_name': 'unitary-toxic-bert.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mlflow-deploy:v2023.4.2-4668', 'arch': None, 'last_update_time': datetime.datetime(2024, 5, 2, 23, 26, 34, 468157, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ccf6707-3f76-4c15-a536-49580be55457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>postprocess</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>3a02feda-7336-4ef4-820a-f13265e3f251</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>postprocess.py</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>0d230ee260e4a86b2cc62c66445c7173e23a6f1bf696d239b45b4f0e2086ca85</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-02-May 23:30:10</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'postprocess', 'version': '3a02feda-7336-4ef4-820a-f13265e3f251', 'file_name': 'postprocess.py', 'image_path': None, 'arch': None, 'last_update_time': datetime.datetime(2024, 5, 2, 23, 30, 10, 278882, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(post_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89612555-e4e2-4ed5-a2a8-10b96f0f2be1",
   "metadata": {},
   "source": [
    "### Deploy the Listener Models\n",
    "\n",
    "We deploy the listener models.  We create a deployment configuration and set the Hugging Face sentiment analyzer to 4 cpus and 8 Gi RAM.\n",
    "\n",
    "We create the pipeline with the `build_pipeline` method, and add the models as the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c00c1f2-498f-48b5-948b-4907de6d06e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>full-toxmonitor-pipeline</td></tr><tr><th>created</th> <td>2024-05-02 23:30:28.397835+00:00</td></tr><tr><th>last_updated</th> <td>2024-05-02 23:30:28.397835+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>arch</th> <td>None</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>d75bc057-6392-42a9-8bdb-5d3661b731c4</td></tr><tr><th>steps</th> <td></td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'full-toxmonitor-pipeline', 'create_time': datetime.datetime(2024, 5, 2, 23, 30, 28, 397835, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'toxic-bert', 'version': 'e511643c-30a4-48b9-a45e-f458d991a916', 'sha': '30b5c2d0c1a2102ad63ef7d84e953b018b45a0c021ea14916708ea1c8142ff38'}]}}, {'ModelInference': {'models': [{'name': 'postprocess', 'version': '3a02feda-7336-4ef4-820a-f13265e3f251', 'sha': '0d230ee260e4a86b2cc62c66445c7173e23a6f1bf696d239b45b4f0e2086ca85'}]}}]\"}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the summarizer config \n",
    "deployment_config = wallaroo.DeploymentConfigBuilder() \\\n",
    "    .cpus(0.25).memory('1Gi') \\\n",
    "    .sidekick_cpus(bert_model, 4) \\\n",
    "    .sidekick_memory(bert_model, \"8Gi\") \\\n",
    "    .build()\n",
    "\n",
    "pipeline_name = 'full-toxmonitor-pipeline'\n",
    "pipeline=wl.build_pipeline(pipeline_name)\n",
    "pipeline.add_model_step(bert_model)\n",
    "pipeline.add_model_step(post_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d590b",
   "metadata": {},
   "source": [
    "With the pipeline set, we deploy the pipeline with the deployment configuration.  This allocates the resources from the cluster for the LLM Listener models use.\n",
    "\n",
    "Once the models are deployed, we check the status and verify it's running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4089f6f-a7ed-4f12-ae82-144afdbae0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7719c095-3984-4de3-ace1-49faf709d04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.60.4.215',\n",
       "   'name': 'engine-86569ff7c-6rcdp',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'full-toxmonitor-pipeline',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'toxic-bert',\n",
       "      'version': 'e511643c-30a4-48b9-a45e-f458d991a916',\n",
       "      'sha': '30b5c2d0c1a2102ad63ef7d84e953b018b45a0c021ea14916708ea1c8142ff38',\n",
       "      'status': 'Running'},\n",
       "     {'name': 'postprocess',\n",
       "      'version': '3a02feda-7336-4ef4-820a-f13265e3f251',\n",
       "      'sha': '0d230ee260e4a86b2cc62c66445c7173e23a6f1bf696d239b45b4f0e2086ca85',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.60.2.44',\n",
       "   'name': 'engine-lb-5df9b487cf-mjmfl',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.60.2.45',\n",
       "   'name': 'engine-sidekick-toxic-bert-9-7f69f7f58f-98z55',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c8f0a-3139-4b76-ac53-53abb99b2f04",
   "metadata": {},
   "source": [
    "### Set Up LLM Listener\n",
    "\n",
    "The LLM Listener leverages the Wallaroo Inference Automation for its execution.  This is uploaded from the file `llm-monitor.zip` as a Workload Orchestration;  this includes a Python script detailing how to deploy the LLM Listener models and evaluate the outputs of the LLM models.\n",
    "\n",
    "The orchestration performs the following when executed:\n",
    "\n",
    "* Accept the following arguments to determine which LLM to evaluate:\n",
    "  * `llm_workspace`: The name of the workspace the LLM is deployed from.\n",
    "  * `llm_pipeline`: The pipeline the LLM is deployed from.\n",
    "  * `llm_output_field`: The LLM's text output field.\n",
    "  * `monitor_workspace`: The workspace the LLM Listener models are deployed from.\n",
    "  * `monitor_pipeline`: The pipeline the LLM listener models are deployed from.\n",
    "  * `window_length`: The amount of time to evaluate from when the task is executed in hours.  For example, `1` would evaluate the past hour.  Use `-1` for no limits.  This will gather the [standard inference results window]({{<ref \"wallaroo-model-operations-service-pipeline-logs#get-pipeline-logs\">}}).\n",
    "  * `n_toxlabels`: The number of toxic labels.  For our `toxic_bert` LLM Listener, the number of fields is 6.\n",
    "* Deploy LLM Listener models.\n",
    "* Gathers the `llama3-instruct`'s [Inference Results]({{<ref \"wallaroo-model-operations-service-pipeline-logs\">}}), and processes the `out.generated_text` field through the LLM Listener models.\n",
    "  * These either either the default inference result outputs, or specified by a date range of inference results.\n",
    "* The LLM listener then scores the LLM's outputs and provides the scores listed above.  These are extracted at any time as its own Inference Results.\n",
    "\n",
    "As a Workload Orchestration, the LLM Listener is executed either as a **Run Once** - which executes once, reports its results, then stops, or **Run Scheduled**, which is executed on set schedule (every 5 minutes, every hour, etc).\n",
    "\n",
    "The following shows running the LLM Listener as a Run Once task, that evaluates the `llama3-instruct` LLM.  The LLM Listener arguments can be modified to evaluate any other deployed LLMs with their own text output fields.\n",
    "\n",
    "This assumes that the LLM Listener was already uploaded and is ready to accept new tasks, and we have saved it to the variable `llm_listener`.\n",
    "\n",
    "See [Inference Automation](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-automate/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e091a0-d373-4716-9f2a-837d085f015b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "main.py                                        2024-05-02 23:29:02         6068\n",
      "requirements.txt                               2024-05-02 22:39:06           27\n"
     ]
    }
   ],
   "source": [
    "# orchestration\n",
    "files_to_include = [\n",
    "    'main.py', # execution script\n",
    "    'requirements.txt', # required if you have additional package dependencies beyond what's included in wallaroo enivironment\n",
    "]\n",
    "\n",
    "zipfile_name = 'llm-monitor.zip'\n",
    "\n",
    "with zipfile.ZipFile(zipfile_name, mode='w') as archive:\n",
    "    for filename in files_to_include:\n",
    "        archive.write(filename)\n",
    "        \n",
    "# verify the contents\n",
    "with zipfile.ZipFile(zipfile_name, mode='r') as archive:\n",
    "    archive.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2741e6-ca46-49fd-afaf-d10d92b166c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_listener = wl.upload_orchestration(name=\"llm-toxicity-listener\", path=f'./{zipfile_name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491a39a9",
   "metadata": {},
   "source": [
    "Once uploaded, we verify the orchestration is ready for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48eff3b3-3bca-4c87-8142-5ab036ac4e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pending_packaging\n",
      "pending_packaging\n",
      "pending_packaging\n",
      "pending_packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n",
      "packaging\n"
     ]
    }
   ],
   "source": [
    "while llm_listener.status() not in ['ready', 'error']:\n",
    "    print(orchestration.status())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cb7ae2",
   "metadata": {},
   "source": [
    "### Execute LLM Listener\n",
    "\n",
    "As a Workload Orchestration, the LLM Listener is executed either as a **Run Once** - which executes once, reports its results, then stops, or **Run Scheduled**, which is executed on set schedule (every 5 minutes, every hour, etc).\n",
    "\n",
    "The following shows running the LLM Listener as a Run Once task, that evaluates the `llama3-instruct` LLM.  The LLM Listener arguments can be modified to evaluate any other deployed LLMs with their own text output fields.\n",
    "\n",
    "This assumes that the LLM Listener was already uploaded and is ready to accept new tasks, and we have saved it to the variable `llm_listener`.\n",
    "\n",
    "Here we create the Run Once task for the LLM Listener and provide it the deployed LLM's workspace and pipeline, and the LLM Listener's models workspace and name.  We give the task the name `sample monitor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "870fccf7-7b4f-4045-8df3-01865dc0fde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the default args\n",
    "args = {\n",
    "    'llm_workspace' : 'llm-models' ,\n",
    "    'llm_pipeline': 'llamav3-instruct',\n",
    "    'llm_output_field': 'out.generated_text',\n",
    "    'monitor_workspace': 'llm-models',\n",
    "    'monitor_pipeline' : 'full-toxmonitor-pipeline',\n",
    "    'window_length': -1,  # in hours. If -1, no limit (for testing)\n",
    "    'n_toxlabels': 6,\n",
    "}\n",
    "\n",
    "task = llm_listener.run_once(name=\"monitor-initial-test\", json_args=args, timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b4601",
   "metadata": {},
   "source": [
    "We list the tasks and verify that this has the status `success`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48acc154-b79c-4539-a3ce-39f7f379fa32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>id</th><th>name</th><th>last run status</th><th>type</th><th>active</th><th>schedule</th><th>created at</th><th>updated at</th></tr><tr><td>2b939bc4-f6d7-4467-a8b9-016841de1bc0</td><td>monitor-initial-test</td><td>success</td><td>Temporary Run</td><td>True</td><td>-</td><td>2024-02-May 23:37:49</td><td>2024-02-May 23:38:05</td></tr></table>"
      ],
      "text/plain": [
       "[<wallaroo.task.Task at 0x7fdbf88cecd0>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wl.list_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8fb70f",
   "metadata": {},
   "source": [
    "We can view the LLM Listener logs to view the general results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "465cafd0-c6d7-46f7-99a5-a54eebd216c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre><code>2024-02-May 23:38:06 starting\n",
       "2024-02-May 23:38:10 starting task\n",
       "2024-02-May 23:38:54                      time  ... anomaly.count\n",
       "2024-02-May 23:38:54 0 2024-05-02 23:38:11.716  ...             0\n",
       "2024-02-May 23:38:54 \n",
       "2024-02-May 23:38:54 Avg Batch Toxicity: 0.0006922021857462823\n",
       "2024-02-May 23:38:54 [1 rows x 10 columns]\n",
       "2024-02-May 23:38:54 Over Threshold: 0\n",
       "2024-02-May 23:38:54 Task complete</code></pre>"
      ],
      "text/plain": [
       "['2024-05-02T23:38:06.757240447Z stdout F starting',\n",
       " '2024-05-02T23:38:10.526315213Z stdout F starting task',\n",
       " '2024-05-02T23:38:54.95001132Z stdout F                      time  ... anomaly.count',\n",
       " '2024-05-02T23:38:54.950077877Z stdout F 0 2024-05-02 23:38:11.716  ...             0',\n",
       " '2024-05-02T23:38:54.950086494Z stdout F ',\n",
       " '2024-05-02T23:38:54.950100794Z stdout F Avg Batch Toxicity: 0.0006922021857462823',\n",
       " '2024-05-02T23:38:54.95009406Z stdout F [1 rows x 10 columns]',\n",
       " '2024-05-02T23:38:54.950107564Z stdout F Over Threshold: 0',\n",
       " '2024-05-02T23:38:54.950113455Z stdout F Task complete']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.last_runs()[0].logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a57f08a",
   "metadata": {},
   "source": [
    "The LLM Listener models results are stored in in the [Inference Results](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-serve/wallaroo-model-operations-service-pipeline-logs/) logs.  Each task run generates a new entry.\n",
    "\n",
    "From these results we can monitor the performance of the LLM results and check for toxicity or other issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed8366cd-9efe-43e6-8efe-600ac770ee42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.inputs</th>\n",
       "      <th>in.top_k</th>\n",
       "      <th>out.identity_hate</th>\n",
       "      <th>out.insult</th>\n",
       "      <th>out.obscene</th>\n",
       "      <th>out.severe_toxic</th>\n",
       "      <th>out.threat</th>\n",
       "      <th>out.toxic</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-05-02 23:38:11.716</td>\n",
       "      <td>Wallaroo.AI is an AI platform that enables developers to build, deploy, and manage AI and machine learning models at scale. It provides a cloud-based infrastructure for building, training, and deploying AI models, as well as a set of tools and APIs for integrating AI into various applications.\\n\\nWallaroo.AI is designed to make it easy for developers to build and deploy AI models, regardless of their level of expertise in machine learning. It provides a range of features, including support for popular machine learning frameworks such as TensorFlow and PyTorch, as well as a set of pre-built AI models and APIs for common use cases such as image and speech recognition, natural language processing, and predictive analytics.\\n\\nWallaroo.AI is particularly well-suited for developers who are looking to build AI-powered applications, but may not have extensive expertise in machine learning or AI development. It provides a range of tools and resources to help developers get started with building AI-powered applications, including a cloud-based development environment, a set of pre-built AI models and APIs, and a range of tutorials and documentation.</td>\n",
       "      <td>6</td>\n",
       "      <td>[0.00014974642544984818]</td>\n",
       "      <td>[0.00017831822333391756]</td>\n",
       "      <td>[0.00018145183275919408]</td>\n",
       "      <td>[0.00012232053268235177]</td>\n",
       "      <td>[0.00013229982869233936]</td>\n",
       "      <td>[0.0006922021857462823]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  \\\n",
       "0 2024-05-02 23:38:11.716   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                in.inputs  \\\n",
       "0  Wallaroo.AI is an AI platform that enables developers to build, deploy, and manage AI and machine learning models at scale. It provides a cloud-based infrastructure for building, training, and deploying AI models, as well as a set of tools and APIs for integrating AI into various applications.\\n\\nWallaroo.AI is designed to make it easy for developers to build and deploy AI models, regardless of their level of expertise in machine learning. It provides a range of features, including support for popular machine learning frameworks such as TensorFlow and PyTorch, as well as a set of pre-built AI models and APIs for common use cases such as image and speech recognition, natural language processing, and predictive analytics.\\n\\nWallaroo.AI is particularly well-suited for developers who are looking to build AI-powered applications, but may not have extensive expertise in machine learning or AI development. It provides a range of tools and resources to help developers get started with building AI-powered applications, including a cloud-based development environment, a set of pre-built AI models and APIs, and a range of tutorials and documentation.   \n",
       "\n",
       "   in.top_k         out.identity_hate                out.insult  \\\n",
       "0         6  [0.00014974642544984818]  [0.00017831822333391756]   \n",
       "\n",
       "                out.obscene          out.severe_toxic  \\\n",
       "0  [0.00018145183275919408]  [0.00012232053268235177]   \n",
       "\n",
       "                 out.threat                out.toxic  anomaly.count  \n",
       "0  [0.00013229982869233936]  [0.0006922021857462823]              0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_listener_results = pipeline.logs()\n",
    "display(llm_listener_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
