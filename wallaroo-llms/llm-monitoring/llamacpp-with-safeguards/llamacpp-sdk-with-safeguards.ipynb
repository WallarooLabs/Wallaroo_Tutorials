{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32d9aa6a",
   "metadata": {},
   "source": [
    "This tutorial can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2026.1_tutorials/wallaroo-llms/llm-monitoring/llamacpp-with-safeguards).\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "## LLM Harmful Language Listener Tutorial\n",
    "\n",
    "The following tutorial demonstrates the Llama 3 70b Instruct Q5 Large Language Model (LLM) with a Harmful Language Listener.  This provides validation monitoring to detect language that could be considered harmful:  obscene, racist, insulting, or other benchmarks.\n",
    "\n",
    "This tutorial demonstrates how to:\n",
    "\n",
    "* Upload the LLM and the Harmful Language Listener.\n",
    "* Create a Wallaroo pipeline and set the LLM and then the Listener as pipeline steps.\n",
    "* Deploy the models and perform sample inferences.\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "The LLM used in this demonstrates has the following attributes.\n",
    "\n",
    "* Framework: `vllm` for more optimized model deployment, uploaded to Wallaroo in the [Wallaroo Custom Model aka Bring Your Own Predict (BYOP) Framework](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "* Artifacts:  The original model is here the Llama 3 8B Instruct Hugging Face model:[Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "* Input/Output Types:  Both the input and outputs are text.\n",
    "\n",
    "The Harmful Language Listener used in this demonstration has the following attributes:\n",
    "\n",
    "* Framework: `vllm` for more optimized model deployment, uploaded to Wallaroo in the [Wallaroo Custom Model aka Bring Your Own Predict (BYOP) Framework](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "* Artifacts:  The Listener model is encapsulated as part of the BYOP framework.\n",
    "* Input/Output Types:  The Listener takes the following inputs and outputs.\n",
    "  * Listener Input:\n",
    "    * `text` (*String*): The original input text to the LLM.\n",
    "    * `generated_text` (*String*): The text created by the LLM.  This will be evaluated by the Listener for any harmful language.\n",
    "  * Listener Output:\n",
    "    * `harmful` (*Boolean*): Determines if the `generated_text` is harmful.\n",
    "    * `reasoning` (*String*): The reasons why the `generated_text` is considered harmful or not.\n",
    "    * `confidence` (*Float*): The confidence the model has of whether the `generated_text` is harmful or now.\n",
    "    * `generated_text` (*String*): The text generated by the LLM.  This is passed on as part of the Listener's output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ded1f5",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "We start by importing the required libraries.  This includes the following:\n",
    "\n",
    "* [Wallaroo SDK](https://pypi.org/project/wallaroo/):  Used to upload and deploy the model in Wallaroo.\n",
    "* [pyarrow](https://pypi.org/project/pyarrow/):  Models uploaded to Wallaroo are defined in the input/output format.\n",
    "* [pandas](https://pypi.org/project/pandas/):  Data is submitted to models deployed in Wallaroo as either Apache Arrow Table format or pandas Record Format as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca0bc3e-9a4a-4b34-bb17-bd0fffb6a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "from wallaroo.dynamic_batching_config import DynamicBatchingConfig\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b3ecc",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is set through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6712f9-871f-4224-8a9e-d44520a25628",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558ea37-f227-45dc-a637-c8a7a0e552db",
   "metadata": {},
   "source": [
    "### Upload the LLM\n",
    "\n",
    "To upload the LLM and Listener, we use the `wallaroo.client.Client.upload_model` method which takes the following parameters.\n",
    "\n",
    "* The name to assign to the LLM.\n",
    "* The file path to upload the LLM.\n",
    "* The Framework set to `wallaroo.framework.Framework.CUSTOM` for our Hugging Face model encapsulated in the BYOP framework.\n",
    "* The input and output schemas.\n",
    "\n",
    "For more information, see the Wallaroo [Model Upload](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/) guide.\n",
    "\n",
    "First we'll set the input and output schemas for our LLM in Apache PyArrow Schema format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a6693e0-f96b-45e5-8253-437c9ca28bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string())\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string()),\n",
    "    pa.field(\"generated_text\", pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d970c4af",
   "metadata": {},
   "source": [
    "Then issue the upload command.  For this example, we'll add a **model configuration** to specify [Dynamic Batching for LLMs](https://docs.wallaroo.ai/wallaroo-llm/wallaroo-llm-optimizations/wallaroo-llm-optimizations-dynamic-batching/) which improves the performance of LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd8718e6-3475-42d1-b5c5-e3a57fc54b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime......\n",
      "Model is attempting loading to a container runtime............successful\n",
      "\n",
      "Ready\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>llama-cpp-sdk-safeguards</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>9c03eaa2-d0d4-4adb-86a1-26df7bf3eb33</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>byop_llamacpp_safeguards.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>45752b3566691a641787abd9b1b9d94809f8a74d545283d599e8a2cdc492d110</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5845</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-17-Dec 16:43:42</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>john.hansarick@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'llama-cpp-sdk-safeguards', 'version': '9c03eaa2-d0d4-4adb-86a1-26df7bf3eb33', 'file_name': 'byop_llamacpp_safeguards.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5845', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2024, 12, 17, 16, 43, 42, 698255, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = wl.upload_model('llama-cpp-sdk-safeguards', \n",
    "    './models/byop_llamacpp_safeguards.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema\n",
    ").configure(input_schema=input_schema,\n",
    "            output_schema=output_schema,\n",
    "            dynamic_batching_config=DynamicBatchingConfig(max_batch_delay_ms=1000, \n",
    "                                                          batch_size_target=8)\n",
    "            )\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcd5e8d",
   "metadata": {},
   "source": [
    "Next we upload the Listener in the same process:  define the input and output schemas, and then upload the model.\n",
    "\n",
    "Note that for the Listener, the inputs are the LLM's **outputs**.  The Listener includes with its outputs the LLM's `generated_text` field so it is passed back to the original receiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5576b58b-8458-4292-b038-b7c486fbabca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Safeguards Harmful Language Listener\n",
    "#Define schemas\n",
    "input_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string()),\n",
    "    pa.field(\"generated_text\", pa.string())\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field(\"harmful\", pa.bool_()),\n",
    "    pa.field(\"reasoning\", pa.string()),\n",
    "    pa.field(\"confidence\", pa.float32()),\n",
    "    pa.field(\"generated_text\", pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efe2490-6143-4e51-a161-befadb37891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime..............................successful\n",
      "\n",
      "Ready\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>byop-safeguards-harmful-5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>6a71d544-89de-411e-97a7-2a5dc5cd92f6</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>byop-safeguards-harmful.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>c41ff30b7032262e6ceffed2da658a44d16e698c1e826c3526b6a2379c8d2b1b</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5845</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-17-Dec 16:46:34</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>5</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>john.hansarick@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'byop-safeguards-harmful-5', 'version': '6a71d544-89de-411e-97a7-2a5dc5cd92f6', 'file_name': 'byop-safeguards-harmful.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.4.0-5845', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2024, 12, 17, 16, 46, 34, 885397, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#upload harmful language listener\n",
    "listener = wl.upload_model('byop-safeguards-harmful-5', \n",
    "    './models/byop-safeguards-harmful.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema,\n",
    ")\n",
    "listener"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0767f9-8b74-4025-831e-49ebb06599b6",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "For our deployment, we deploy both the LLM and Listener in the same pipeline as **pipeline steps**.  Input provided to the pipeline is submitted first to the LLM.  The output from the LLM is then the input to the Listener, and the Listener's output is then provided back to the requester.\n",
    "\n",
    "The deployment configuration sets the resources allocated for the LLM and the Listener with the following options:\n",
    "\n",
    "* LLM\n",
    "  * CPUs: 6\n",
    "  * Memory:  10 Gi\n",
    "* Harmful Language Listener\n",
    "  * CPUs: 2\n",
    "  * Memory:  10 Gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e7a7cf-5fc8-44ec-b16b-d0bd82bb22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(llm, 6) \\\n",
    "    .sidekick_memory(llm, '10Gi') \\\n",
    "    .sidekick_cpus(listener, 2) \\\n",
    "    .sidekick_memory(listener, '10Gi') \\\n",
    "    .sidekick_env(listener, json.load(open(\"credentials.json\", 'r'))) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9380151d",
   "metadata": {},
   "source": [
    "The Wallaroo pipeline is created with the `build_pipeline` method.  The LLM and Listener are set as the **pipeline steps**, then deployed with the previously defined deployment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76f0c6c5-bee6-4be3-9fd3-67f30532a0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment initiated for safeguards-llamacpp. Please check pipeline status.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>safeguards-llamacpp</td></tr><tr><th>created</th> <td>2024-12-17 16:52:43.998692+00:00</td></tr><tr><th>last_updated</th> <td>2024-12-17 16:52:44.049227+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>workspace_id</th> <td>5</td></tr><tr><th>workspace_name</th> <td>john.hansarick@wallaroo.ai - Default Workspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>7ecd5285-6576-4539-99b0-a067a88836c1, 3641c4fe-c5be-46e6-bd94-93326d57ede2</td></tr><tr><th>steps</th> <td>llama-cpp-sdk-safeguards</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'safeguards-llamacpp', 'create_time': datetime.datetime(2024, 12, 17, 16, 52, 43, 998692, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'llama-cpp-sdk-safeguards', 'version': '9c03eaa2-d0d4-4adb-86a1-26df7bf3eb33', 'sha': '45752b3566691a641787abd9b1b9d94809f8a74d545283d599e8a2cdc492d110'}]}}, {'ModelInference': {'models': [{'name': 'byop-safeguards-harmful-5', 'version': '6a71d544-89de-411e-97a7-2a5dc5cd92f6', 'sha': 'c41ff30b7032262e6ceffed2da658a44d16e698c1e826c3526b6a2379c8d2b1b'}]}}]\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = wl.build_pipeline(\"safeguards-llamacpp\")\n",
    "pipeline.add_model_step(llm)\n",
    "pipeline.add_model_step(listener)\n",
    "pipeline.deploy(deployment_config=deployment_config, wait_for_status=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec90ed42",
   "metadata": {},
   "source": [
    "Once deployed, we'll check on the `status`.  When the `status` is `Running`, we continue to the inference steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e1e7b8b-f92e-4caf-837c-4444061fdbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.28.3.7',\n",
       "   'name': 'engine-576b7f5b4-m9h9j',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'safeguards-llamacpp',\n",
       "      'status': 'Running',\n",
       "      'version': '7ecd5285-6576-4539-99b0-a067a88836c1'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 1,\n",
       "      'name': 'llama-cpp-sdk-safeguards',\n",
       "      'sha': '45752b3566691a641787abd9b1b9d94809f8a74d545283d599e8a2cdc492d110',\n",
       "      'status': 'Running',\n",
       "      'version': '9c03eaa2-d0d4-4adb-86a1-26df7bf3eb33'},\n",
       "     {'model_version_id': 2,\n",
       "      'name': 'byop-safeguards-harmful-5',\n",
       "      'sha': 'c41ff30b7032262e6ceffed2da658a44d16e698c1e826c3526b6a2379c8d2b1b',\n",
       "      'status': 'Running',\n",
       "      'version': '6a71d544-89de-411e-97a7-2a5dc5cd92f6'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.28.2.7',\n",
       "   'name': 'engine-lb-6676794678-r4w99',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.28.3.8',\n",
       "   'name': 'engine-sidekick-byop-safeguards-harmful-5-2-654fb4d87f-c5jp8',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'},\n",
       "  {'ip': '10.28.2.8',\n",
       "   'name': 'engine-sidekick-llama-cpp-sdk-safeguards-1-6d8bfddc4-nfhhb',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the pipeline status before performing an inference\n",
    "\n",
    "import time\n",
    "time.sleep(15)\n",
    "\n",
    "while pipeline.status()['status'] != 'Running':\n",
    "   time.sleep(15)\n",
    "   pipeline.status()['status']\n",
    "\n",
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de95ca9-0a64-4d8f-b91f-60b1527e26f1",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "For our inference, we submit either a pandas DataFrame or Apache Arrow table with our text query.  In this case:  `Describe what Wallaroo.AI is`.\n",
    "\n",
    "Once submitted, we display the `harmful`, `confidence`, and `reason`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c468b4da-ff9f-4639-b972-c8d7cd68da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': ['Describe what Wallaroo.AI is']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c9feef8-ce6d-4aca-9f2e-2105a8efd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pipeline.infer(data, timeout=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90dc873f-4dbf-4a84-9225-a117408be152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"out.confidence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab81c3ae-7b53-45e3-9ebf-cccae6e87b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"out.harmful\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c9268a7-f68e-48f6-8254-a4e1e772dd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This response provides a neutral and informative description of Wallaroo.ai without any potential biases or stereotypes.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"out.reasoning\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebd5ad",
   "metadata": {},
   "source": [
    "### Undeploy the Models\n",
    "\n",
    "With the tutorial complete, we undeploy the model and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "967ee607-63f5-4f37-9b6f-dd788b9d39eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s ...... ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>safeguards-llamacpp</td></tr><tr><th>created</th> <td>2024-12-17 16:52:43.998692+00:00</td></tr><tr><th>last_updated</th> <td>2024-12-17 16:52:44.049227+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>5</td></tr><tr><th>workspace_name</th> <td>john.hansarick@wallaroo.ai - Default Workspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>7ecd5285-6576-4539-99b0-a067a88836c1, 3641c4fe-c5be-46e6-bd94-93326d57ede2</td></tr><tr><th>steps</th> <td>llama-cpp-sdk-safeguards</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'safeguards-llamacpp', 'create_time': datetime.datetime(2024, 12, 17, 16, 52, 43, 998692, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'llama-cpp-sdk-safeguards', 'version': '9c03eaa2-d0d4-4adb-86a1-26df7bf3eb33', 'sha': '45752b3566691a641787abd9b1b9d94809f8a74d545283d599e8a2cdc492d110'}]}}, {'ModelInference': {'models': [{'name': 'byop-safeguards-harmful-5', 'version': '6a71d544-89de-411e-97a7-2a5dc5cd92f6', 'sha': 'c41ff30b7032262e6ceffed2da658a44d16e698c1e826c3526b6a2379c8d2b1b'}]}}]\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4384e14",
   "metadata": {},
   "source": [
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
