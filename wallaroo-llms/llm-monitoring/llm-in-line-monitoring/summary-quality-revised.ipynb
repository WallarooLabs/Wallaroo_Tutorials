{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c97de6f2",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/llm-monitoring/llm-in-line-monitoring).\n",
    "\n",
    "## LLM Validation Listener Example\n",
    "\n",
    "The following example demonstrates using LLM Validation Listener to evaluate LLM performance at inference time.\n",
    "\n",
    "LLM Validation Listener validates LLMs' inferences during the inference process.  These validations are implemented as an in-line step in the same Wallaroo pipeline with the LLM.  These validations are customized for whatever monitoring the user request, such as summary quality, translation quality score, and other use cases.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "LLM Validation Listeners follow this process:\n",
    "\n",
    "* Each validation step is uploaded as [Bring Your Own Predict (BYOP)](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/#wallaroo-supported-models)) or [Hugging Face](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/#wallaroo-supported-models)) model into Wallaroo.  These models monitor the outputs of the LLM and score them based on whatever criteria the data scientist developers.\n",
    "* These model steps evaluate inference data directly from the LLM, creating additional fields based on the LLM's inference output.\n",
    "  * For example, if the LLM outputs the field `text`, the validation model's outputs would be the fields `summary_quality`, `translation_quality_score`, etc.\n",
    "* These steps are monitored with [Wallaroo assays](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-observe/wallaroo-pipeline-assays/) to analyze the scores each validation step produces and publish assay analyses based on established criteria.\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial demonstrates the following:\n",
    "\n",
    "* Upload an LLM Validation Listener developed to evaluate the output of a Llama v3 Llamacpp LLM previously uploaded to Wallaroo.\n",
    "* Add the LLM Validation Listener in the same pipeline as the Llama v3 Llamacpp LLM.\n",
    "* Perform sample inference and show the how the LLM Validation Listener scores the LLM outputs.\n",
    "\n",
    "## Tutorial Steps\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6767a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b24de",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da477ca8-f2bd-43be-a86c-59924d7cb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b656fbaf",
   "metadata": {},
   "source": [
    "### Set Workspace\n",
    "\n",
    "The following creates or connects to an existing workspace based on the variable `workspace_name`, and sets it as the current workspace.  For more details on Wallaroo workspaces, see [Wallaroo Workspace Management Guide](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-optimize/wallaroo-workspace-management/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c393ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = wl.get_workspace('summary_quality_llm_demo', create_if_not_exist=True)\n",
    "_ = wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0da885",
   "metadata": {},
   "source": [
    "### Upload LLM Validation Listener Model\n",
    "\n",
    "The LLM Validation Listener model is uploaded form the BYOP model `summarisation_quality_final.zip`, which is a Quality Summarization model that evaluates the LLM's `generated_text` output and scores it.  This has the following inputs and outputs:\n",
    "\n",
    "* **Inputs**\n",
    "  * `text`: *String*\n",
    "  * `generated_text`: *String* ; This is the output of the Llama V3 model.\n",
    "* **Outputs**\n",
    "  * `generated_text`: *String* ; This is the same `generated_text` from the Llama v3 model, passed through as an inference output.\n",
    "  * `score`: *Float64*; The total score based on the `generated_text` field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea2d341-9e39-439d-be41-f3939807ceb8",
   "metadata": {},
   "source": [
    "#### Schema Definition\n",
    "\n",
    "We set the model's input and output schemas in Apache PyArrow Schema format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6727c4b-4bc3-4fd9-a143-bf391d68f346",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('text', pa.string()),\n",
    "    pa.field('generated_text', pa.string())\n",
    "]) \n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string()),\n",
    "    pa.field('score', pa.float64()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dea02e-f22d-4ce4-84b1-d16167032af4",
   "metadata": {},
   "source": [
    "#### Upload the Model\n",
    "\n",
    "We now upload the model as the framework `wallaroo.framework.Framework.CUSTOM`.  For more details on uploading models, see [Model Upload](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/).  We store the model version reference to the variable `validation_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a41ea4c-63ce-4c0d-9006-73f828c6cd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime...................................................................................................successful\n",
      "\n",
      "Ready\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>summquality</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>14fca0ba-69d1-44b0-9fbb-ff39c07884b8</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>summarisation_quality_final.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>c221cf1cab35c089847138aeac5a2e179430fa45fbddd281bcb1614876541c81</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mlflow-deploy:v2023.4.1-4351</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-23-May 18:27:38</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'summquality', 'version': '14fca0ba-69d1-44b0-9fbb-ff39c07884b8', 'file_name': 'summarisation_quality_final.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mlflow-deploy:v2023.4.1-4351', 'arch': None, 'last_update_time': datetime.datetime(2024, 5, 23, 18, 27, 38, 799392, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_model = wl.upload_model('summquality', \n",
    "    'summarisation_quality_final.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema\n",
    ")\n",
    "display(validation_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a41572a-c213-40c1-972a-effe1c9abfbb",
   "metadata": {},
   "source": [
    "### Retrieve the LLM\n",
    "\n",
    "If the LLM is already uploaded we retrieve it with the method `wallaroo.client.Client.get_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc91b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = wl.get_model('llamav3-llamacpp-passthrough-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b438ae2-1023-4034-a240-2c05ec079e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>llamav3-llamacpp-passthrough-1</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>71993033-561b-455d-89ea-933f112eb523</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>byop_llamacpp_llama3_extra.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>54f3b58c3efb4bf1c02a144683dd6431fcb606fb884ce7b1d853f9bffb71b6b4</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mlflow-deploy:v2023.4.1-4351</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-23-May 19:56:57</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'llamav3-llamacpp-passthrough-1', 'version': '71993033-561b-455d-89ea-933f112eb523', 'file_name': 'byop_llamacpp_llama3_extra.zip', 'image_path': None, 'arch': None, 'last_update_time': datetime.datetime(2024, 5, 23, 19, 53, 37, 303055, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee74dbc",
   "metadata": {},
   "source": [
    "### Set Deployment Configuration\n",
    "\n",
    "The deployment configuration sets the resources assigned to the LLM and the LLM Validation Listener model.  For this example:\n",
    "\n",
    "* LLM:  6 cpus, 10 Gi RAM\n",
    "* In-Line Monitor: 2 cpus, 8 Gi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98a10ee9-5a57-4ff9-bbf4-1bc9164afe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(2).memory('2Gi') \\\n",
    "    .sidekick_cpus(validation_model, 2) \\\n",
    "    .sidekick_memory(validation_model, '8Gi') \\\n",
    "    .sidekick_cpus(llama, 6) \\\n",
    "    .sidekick_memory(llama, '10Gi') \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14408a17",
   "metadata": {},
   "source": [
    "### Deploy Models\n",
    "\n",
    "We deploy assign both models to the same pipeline, the LLM assigned first, and the Monitoring model second to score the results of the LLM.  These are deployed with the defined deployment configuration.\n",
    "\n",
    "See [Model Deploy](https://docs.wallaroo.ai/wallaroo-llm/wallaroo-llm-package-deployment/) for more details on deploying LLMs in Wallaroo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbe0d2e-598d-46c1-9981-f0e7137f5191",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"llm-summ-quality-1\")\n",
    "pipeline.add_model_step(llama)\n",
    "pipeline.add_model_step(model)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83adf23c",
   "metadata": {},
   "source": [
    "Once deployment is complete, we can check the deployment status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afc219d8-faa9-45ed-a89f-0f283cfba715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.208.2.14',\n",
       "   'name': 'engine-5b8586f4c8-fbzkx',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'llm-summ-quality-1',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'llamav3-llamacpp-passthrough-1',\n",
       "      'version': '71993033-561b-455d-89ea-933f112eb523',\n",
       "      'sha': '54f3b58c3efb4bf1c02a144683dd6431fcb606fb884ce7b1d853f9bffb71b6b4',\n",
       "      'status': 'Running'},\n",
       "     {'name': 'summquality',\n",
       "      'version': '14fca0ba-69d1-44b0-9fbb-ff39c07884b8',\n",
       "      'sha': 'c221cf1cab35c089847138aeac5a2e179430fa45fbddd281bcb1614876541c81',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.208.2.12',\n",
       "   'name': 'engine-lb-dcd9c8cd7-f64hr',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.208.2.13',\n",
       "   'name': 'engine-sidekick-summquality-204-585d4466ff-hr2gv',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'},\n",
       "  {'ip': '10.208.0.2',\n",
       "   'name': 'engine-sidekick-llamav3-llamacpp-passthrough-1-208-5fcd894vlc76',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1ae24-4190-4b61-9bb4-17db148ab810",
   "metadata": {},
   "source": [
    "### Sample LLM and Validation Monitor Inference\n",
    "\n",
    "We perform an inference by submitting an Apache Arrow table to the deployed LLM and LLM Validation Listener, and displaying the results.  Apache arrow tables provide low latency methods of data transmission and inference.\n",
    "\n",
    "The following fields are output from the inference:\n",
    "\n",
    "* `out.generated_text`:  The LLM's generated text.\n",
    "* `out.score`: The quality score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d04910c1-d00c-4f10-a59d-eb41c628f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Please summarize this text: Simplify production AI for seamless self-checkout or cashierless experiences at scale, enabling any retail store to offer a modern shopping journey. We reduce the technical overhead and complexity for delivering a checkout experience that’s easy and efficient no matter where your stores are located.Eliminate Checkout Delays: Easy and fast model deployment for a smooth self-checkout process, allowing customers to enjoy faster, hassle-free shopping experiences. Drive Operational Efficiencies: Simplifying the process of scaling AI-driven self-checkout solutions to multiple retail locations ensuring uniform customer experiences no matter the location of the store while reducing in-store labor costs. Continuous Improvement: Enabling integrated data insights for informing self-checkout improvements across various locations, ensuring the best customer experience, regardless of where they shop.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4100fe4a-e3aa-45c6-8147-343f34b9224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pa.Table.from_pydict({\"text\" : [text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c92c248c-2990-4a0e-b658-1366f65becba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "time: timestamp[ms]\n",
       "in.text: string not null\n",
       "out.generated_text: string not null\n",
       "out.score: float not null\n",
       "check_failures: int8\n",
       "----\n",
       "time: [[2024-05-23 20:08:00.423]]\n",
       "in.text: [[\"Please summarize this text: Simplify production AI for seamless self-checkout or cashierless experiences at scale, enabling any retail store to offer a modern shopping journey. We reduce the technical overhead and complexity for delivering a checkout experience that’s easy and efficient no matter where your stores are located.Eliminate Checkout Delays: Easy and fast model deployment for a smooth self-checkout process, allowing customers to enjoy faster, hassle-free shopping experiences. Drive Operational Efficiencies: Simplifying the process of scaling AI-driven self-checkout solutions to multiple retail locations ensuring uniform customer experiences no matter the location of the store while reducing in-store labor costs. Continuous Improvement: Enabling integrated data insights for informing self-checkout improvements across various locations, ensuring the best customer experience, regardless of where they shop.\"]]\n",
       "out.generated_text: [[\" Here's a summary of the text:\n",
       "\n",
       "This AI technology simplifies and streamlines self-checkout processes for retail stores, allowing them to offer efficient and modern shopping experiences at scale. It reduces technical complexity and makes it easy to deploy AI-driven self-checkout solutions across multiple locations. The system eliminates checkout delays, drives operational efficiencies by reducing labor costs, and enables continuous improvement through data insights, ensuring a consistent customer experience regardless of location.\"]]\n",
       "out.score: [[0.837221]]\n",
       "check_failures: [[0]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.infer(input_data, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afb80c",
   "metadata": {},
   "source": [
    "### Undeploy the Models\n",
    "\n",
    "With the tutorial complete, we undeploy the LLMs to return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce6b90ea-7b0c-4ef5-b16d-1ae2c322f2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s ..................................... ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>llm-summ-quality-1</td></tr><tr><th>created</th> <td>2024-05-23 20:01:16.874284+00:00</td></tr><tr><th>last_updated</th> <td>2024-05-23 20:01:16.935710+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>arch</th> <td>None</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>1c9e9ec2-3dc9-4ef1-94e1-e9e2b6266d2c, b7c2c259-f900-471c-9ccd-cf3f95085969</td></tr><tr><th>steps</th> <td>llamav3-llamacpp-passthrough-1</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'llm-summ-quality-1', 'create_time': datetime.datetime(2024, 5, 23, 20, 1, 16, 874284, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'llamav3-llamacpp-passthrough-1', 'version': '71993033-561b-455d-89ea-933f112eb523', 'sha': '54f3b58c3efb4bf1c02a144683dd6431fcb606fb884ce7b1d853f9bffb71b6b4'}]}}, {'ModelInference': {'models': [{'name': 'summquality', 'version': '14fca0ba-69d1-44b0-9fbb-ff39c07884b8', 'sha': 'c221cf1cab35c089847138aeac5a2e179430fa45fbddd281bcb1614876541c81'}]}}]\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca97293",
   "metadata": {},
   "source": [
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
