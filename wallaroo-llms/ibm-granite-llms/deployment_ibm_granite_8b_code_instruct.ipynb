{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cddd10-25e6-4891-bd6a-a14f81ac351e",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/ibm-granite-llms).\n",
    "\n",
    "## IBM Granite 8B Code Instruct Large Language Model (LLM) with GPU\n",
    "\n",
    "The following demonstrates deploying an IBM Granite 8B Code Instruct Large Language Model (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4ceab-98b1-4f2a-a0b5-680770727c32",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/ibm-granite-llms).\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "The following demonstrates deploying and inferencing with an IBM Granite 8B Code Instruct Large Language Model (LLM) in Wallaroo.\n",
    "\n",
    "This process shows how to:\n",
    "\n",
    "* Retrieve a previously uploaded [IBM Granite 8B Code Instruct LLM](https://huggingface.co/ibm-granite).\n",
    "* Deploy the LLM and allocate resources for its exclusive use.\n",
    "* Perform inference requests through the deployed LLM.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Wallaroo 2024.1 and above.\n",
    "* A cluster with GPUs.  See [Create GPU Nodepools](https://docs.wallaroo.ai/wallaroo-platform-operations/wallaroo-platform-operations-install/wallaroo-install-enterprise-environment/wallaroo-gpu-nodepools/) for instructions on adding a GPU enabled nodepool to a cluster hosting Wallaroo.\n",
    "* The IBM Granite 8B Code Instruct LLM contained in the [Wallaroo Custom Model aka BYOP (Bring Your Own Predict) framework](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "\n",
    "## Tutorial Steps\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the Python libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09da43e6-9e0d-483b-8855-61f22ede7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164734e0",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32932fea-5190-4e36-aca5-69ca0fc629a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033e290-d466-487f-93df-93af9d49bdfe",
   "metadata": {},
   "source": [
    "### Retrieve the LLM\n",
    "\n",
    "The Wallaroo SDK method `wallaroo.client.Client.list_models` returns a List of models previously uploaded to Wallaroo.  We then specify the most current model version to assign to our `model` variable for later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "333d3c67-3c3b-4630-8d02-933d8507a14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>byop-granite-instruct-8b-v2</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>4d3f402d-e242-409f-8678-29c18f59a4a8</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>byop_granite_8b_code_instruct.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>ffa1a170b0e1628924c18a96c44f43c8afef1e535e378c2eb071a61dd282c669</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.1.0-5330</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-22-Jul 12:52:47</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'byop-granite-instruct-8b-v2', 'version': '4d3f402d-e242-409f-8678-29c18f59a4a8', 'file_name': 'byop_granite_8b_code_instruct.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.1.0-5330', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2024, 7, 22, 12, 52, 47, 763960, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = wl.list_models()[0].versions()[-1]\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95fe555",
   "metadata": {},
   "source": [
    "### IBM Granite 8B Code Instruct BYOP Template\n",
    "\n",
    "[Wallaroo BYOP models](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/) use Python scripts combined with the LLM artifacts to deploy the target model and perform inference requests.\n",
    "\n",
    "Wallaroo BYOP models are composed of:\n",
    "\n",
    "* One or more Python scripts.\n",
    "* A `requirements.txt` file to specify the libraries.\n",
    "* Any model artifacts.  For this example, the [IBM Granite 8B Code Instruct LLM](https://huggingface.co/ibm-granite).\n",
    "\n",
    "The following template demonstrates the Python script used with the Wallaroo BYOP model accept inference requests, forwards them to the IBM Granite LLM, then returns the responses back to the requester.\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from mac.inference import Inference\n",
    "from mac.inference.creation import InferenceBuilder\n",
    "from mac.types import InferenceData\n",
    "from mac.config.inference import CustomInferenceConfig\n",
    "\n",
    "from typing import Any, Set\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class GraniteInference(Inference):\n",
    "    @property\n",
    "    def expected_model_types(self) -> Set[Any]:\n",
    "        return {pipeline}\n",
    "\n",
    "    @Inference.model.setter\n",
    "    def model(self, model) -> None:\n",
    "        # self._raise_error_if_model_is_wrong_type(model)\n",
    "        self._model = model\n",
    "\n",
    "    def _predict(self, input_data: InferenceData):\n",
    "        generated_texts = []\n",
    "        prompts = input_data[\"text\"].tolist()\n",
    "\n",
    "        for prompt in prompts:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "\n",
    "            generated_text = self.model(messages, max_new_tokens=1024, do_sample=True)[\n",
    "                0\n",
    "            ][\"generated_text\"][-1][\"content\"]\n",
    "            generated_texts.append(generated_text)\n",
    "\n",
    "        return {\"generated_text\": np.array(generated_texts)}\n",
    "\n",
    "\n",
    "class GraniteInferenceBuilder(InferenceBuilder):\n",
    "    @property\n",
    "    def inference(self) -> GraniteInference:\n",
    "        return GraniteInference()\n",
    "\n",
    "    def create(self, config: CustomInferenceConfig) -> GraniteInference:\n",
    "        inference = self.inference\n",
    "        model = self._load_model(config.model_path)\n",
    "        inference.model = model\n",
    "\n",
    "        return inference\n",
    "\n",
    "    def _load_model(self, model_path):\n",
    "        return pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=os.path.join(model_path, \"artifacts\", \"granite-8b-code-instruct\"),\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705dc16-f70b-4afc-a6c8-9f693c887385",
   "metadata": {},
   "source": [
    "### Deploy the LLM\n",
    "\n",
    "Deploying a model in Wallaroo takes the following steps:\n",
    "\n",
    "* Create the deployment configuration.  This sets the number of resources allocated from the cluster for the LLMs use.  For this example, the following resources are allocated:\n",
    "  * CPUs: 4\n",
    "  * RAM: 2 Gi\n",
    "  * GPUs: 1.  Note that when GPUs are allocated for LLMS deployed in Wallaroo, the `deployment_label` setting is **required** to specify the nodepool with the GPUs.\n",
    "* Assign the LLM to a Wallaroo pipeline as a **model step**, then deploy the pipeline with the deployment configuration.\n",
    "\n",
    "Once the deployment configuration is complete, the LLM is ready to accept inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "499212ba-7a6a-4f06-a59d-908c7fefe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the deployment configuration\n",
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(model, 4) \\\n",
    "    .sidekick_memory(model, '2Gi') \\\n",
    "    .sidekick_gpus(model, 1) \\\n",
    "    .deployment_label(\"wallaroo.ai/accelerator:a10040\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0aa40-d037-4874-b662-7950cc120270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline and add the LLM as a model step\n",
    "pipeline = wl.build_pipeline(\"granite-pipe-v2\")\n",
    "pipeline.add_model_step(model)\n",
    "\n",
    "# deploy the LLM with the deployment configuration\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b5c61",
   "metadata": {},
   "source": [
    "We verify the deployment status - once the status is `Running` the LLM is ready for inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4fca07c-1950-41cb-a81a-1f91281fd3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.240.5.6',\n",
       "   'name': 'engine-7bd8d4664d-69qfx',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'granite-pipe-v2',\n",
       "      'status': 'Running',\n",
       "      'version': 'c27736f6-0ee2-4ca0-9982-9845d2d5f756'}]},\n",
       "   'model_statuses': {'models': [{'name': 'byop-granite-instruct-8b-v2',\n",
       "      'sha': 'ffa1a170b0e1628924c18a96c44f43c8afef1e535e378c2eb071a61dd282c669',\n",
       "      'status': 'Running',\n",
       "      'version': '4d3f402d-e242-409f-8678-29c18f59a4a8'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.240.5.7',\n",
       "   'name': 'engine-lb-776bbf49b9-rb5mt',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.240.5.8',\n",
       "   'name': 'engine-sidekick-byop-granite-instruct-8b-v2-99-55d95d96f5-gjml9',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13f886",
   "metadata": {},
   "source": [
    "### Submit Inference Request to Deployed LLM\n",
    "\n",
    "Inference Requests to LLMs deployed in Wallaroo accept the following inputs:\n",
    "\n",
    "* pandas DataFrame\n",
    "* Apache Arrow Tables\n",
    "\n",
    "Inference Requests performed through the Wallaroo SDK returns inference results in the same format they were submitted in; if the request is in a pandas DataFrame, the response is returned in a pandas DataFrame.\n",
    "\n",
    "For this example, the inference request is submitted as a pandas DataFrame, with result returned in the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ef741c36-a98a-411b-a1a7-0d6a5bc28700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"text\": [\"Write a code to find the maximum value in a list of numbers.\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7c59f9e-30a8-42b0-b1d2-8e7f1a81348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.infer(data, timeout=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff8bcede-2c3d-4143-8199-fcce7c9fbd8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.text</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-22 14:21:36.748</td>\n",
       "      <td>Write a code to find the maximum value in a li...</td>\n",
       "      <td>You can use the `max()` function in Python to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time                                            in.text  \\\n",
       "0 2024-07-22 14:21:36.748  Write a code to find the maximum value in a li...   \n",
       "\n",
       "                                  out.generated_text  anomaly.count  \n",
       "0  You can use the `max()` function in Python to ...              0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a61b61",
   "metadata": {},
   "source": [
    "We isolate the `generated_text` output field so show the inference result generated from the IBM Granite LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e05d4c23-dd73-4d5c-8842-59a9eb9bc145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can use the `max()` function in Python to find the maximum value in a given list. The `max()` function takes an iterable (such as a list) and returns the largest element.\\n\\nHere is a Python code snippet that finds the maximum value in a list of numbers:\\n\\n```python\\ndef find_max(numbers):\\n    max_value = max(numbers)\\n    return max_value\\n```\\n\\nTo use the `find_max` function, you need to pass a list of numbers as an argument. The function will return the maximum value in the list.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"out.generated_text\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c82010",
   "metadata": {},
   "source": [
    "When complete, we undeploy the LLM and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "906949e3-b9f2-430f-920f-04caf8613011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s .................................... ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>granite-pipe-v1</td></tr><tr><th>created</th> <td>2024-07-22 12:09:21.555827+00:00</td></tr><tr><th>last_updated</th> <td>2024-07-22 12:09:21.605179+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>4b2d7802-e930-437f-a29e-05ced44eddd7, 798784e0-85d6-461a-b005-177340b48f5a</td></tr><tr><th>steps</th> <td>byop-granite-instruct-8b-v1</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'granite-pipe-v1', 'create_time': datetime.datetime(2024, 7, 22, 12, 9, 21, 555827, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'byop-granite-instruct-8b-v1', 'version': '41f31bb5-3566-4cda-814b-a8cfd9beca2d', 'sha': 'ac4a3713c232375282383e9a51191a13243f15c44bccafd7662365707e4c9cf7'}]}}]\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb9b4d-3cec-4292-a3b6-b8bb09472488",
   "metadata": {},
   "source": [
    "This sample notebook is available through the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/ibm-granite-llms).\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
