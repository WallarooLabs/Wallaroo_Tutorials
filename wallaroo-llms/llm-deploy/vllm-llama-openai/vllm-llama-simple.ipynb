{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9930eccf",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.1.1_tutorials/wallaroo-llms/llm-deploy/vllm-llama-openai).\n",
    "\n",
    "## Deploy Llama with OpenAI compatibility\n",
    "\n",
    "The following tutorial demonstrates deploying a Llama LLM in Wallaroo with OpenAI API compatibility enabled.  This allows developers to:\n",
    "\n",
    "* Take advantage of Wallaroo's inference optimization to increase inference response times with more efficient resource allocation.\n",
    "* Migrate existing OpenAI client code with a minimum of changes.\n",
    "\n",
    "Wallaroo supports OpenAI compatibility for LLMs through the following Wallaroo frameworks:\n",
    "\n",
    "* `wallaroo.framework.Framework.VLLM`:  Native async vLLM implementations.\n",
    "* `wallaroo.framework.Framework.CUSTOM`:  Wallaroo Custom Models provide greater flexibility through a lightweight Python interface.  This is typically used in the same pipeline as a native vLLM implementation to provide additional features such as Retrieval-Augmented Generation (RAG), monitoring, etc.\n",
    "\n",
    "A typical situation is to either deploy the native vLLM runtime as a single model in a Wallaroo pipeline, or both the Custom Model runtime and the native vLLM runtime together in the same pipeline to extend the LLMs capabilities.\n",
    "\n",
    "This example uses one LLM with OpenAI compatibility enabled.\n",
    "\n",
    "![Single model LLM](./images/reference/wallaroo-llms/openai/OpenAI-single-model-pipeline.svg)\n",
    "\n",
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "This tutorial demonstrates how to:\n",
    "\n",
    "* Upload a LLM with the Wallaroo native vLLM framework.\n",
    "* Configure the uploaded LLM to enable OpenAI API compatibility and set additional OpenAI parameters.\n",
    "* Set resource configurations and deploy the LLM in Wallaroo.\n",
    "* Submit inference request via:\n",
    "  * The Wallaroo SDK methods `completions` and `chat_completion` \n",
    "  * Wallaroo pipeline inference urls with OpenAI API endpoints extensions.\n",
    "\n",
    "### Tutorial Requirements\n",
    "\n",
    "The following tutorial requires the following:\n",
    "\n",
    "* Wallaroo version 2025.1 and above.\n",
    "* Tiny Llama model.  This is available from Wallaroo representatives upon request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea54115",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "The following libraries are used for this tutorial, primarily the Wallaroo SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5088f782-1309-4136-be12-033c901068b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Acceleration\n",
    "from wallaroo.openai_config import OpenaiConfig\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41db24",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is established via the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d38993-5c1e-4e13-bf4a-53da7e2ec404",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client(request_timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f01b42",
   "metadata": {},
   "source": [
    "### Create and Set the Current Workspace\n",
    "\n",
    "This steps creates the workspace.  Uploaded LLMs and pipeline deployments are set within this workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856f2843-8588-4d75-993e-8d2be0da95cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'vllm-openai-test', 'id': 1689, 'archived': False, 'created_by': 'younes.amar@wallaroo.ai', 'created_at': '2025-05-30T20:30:35.093295+00:00', 'models': [{'name': 'tinyllamaopenai', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 5, 30, 20, 32, 28, 757011, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 5, 30, 20, 32, 28, 757011, tzinfo=tzutc())}, {'name': 'tinyllamaopenaiyns1', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 3, 0, 31, 49, 205332, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 3, 0, 31, 49, 205332, tzinfo=tzutc())}, {'name': 'tinyllama', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 3, 0, 34, 0, 798254, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 3, 0, 34, 0, 798254, tzinfo=tzutc())}, {'name': 'ragstep1', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 3, 1, 46, 47, 430142, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 3, 1, 46, 47, 430142, tzinfo=tzutc())}, {'name': 'tinyllamaopenaiyns2', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 16, 16, 36, 23, 762501, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 16, 16, 36, 23, 762501, tzinfo=tzutc())}, {'name': 'tinyllamaopenaiyns-error', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 23, 15, 2, 59, 581760, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 23, 15, 2, 59, 581760, tzinfo=tzutc())}, {'name': 'tinyllamaopenaiyns-error1', 'versions': 1, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 24, 18, 39, 42, 466411, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 24, 18, 39, 42, 466411, tzinfo=tzutc())}, {'name': 'tinyllamarag', 'versions': 2, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 27, 17, 43, 44, 446012, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 3, 19, 25, 43, 437726, tzinfo=tzutc())}, {'name': 'ragstep', 'versions': 3, 'owner_id': '\"\"', 'last_update_time': datetime.datetime(2025, 6, 27, 17, 49, 39, 238424, tzinfo=tzutc()), 'created_at': datetime.datetime(2025, 6, 3, 0, 37, 17, 945954, tzinfo=tzutc())}], 'pipelines': [{'name': 'tinyllama-openai', 'create_time': datetime.datetime(2025, 5, 30, 20, 40, 46, 518566, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'tinyllama-openai-error', 'create_time': datetime.datetime(2025, 6, 23, 15, 13, 57, 625524, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'tinyllama-openai-error1', 'create_time': datetime.datetime(2025, 6, 24, 18, 43, 42, 785405, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'tinyllama-openai-rag-cb', 'create_time': datetime.datetime(2025, 6, 4, 18, 15, 37, 345076, tzinfo=tzutc()), 'definition': '[]'}, {'name': 'tinyllama-openai-rag', 'create_time': datetime.datetime(2025, 6, 3, 0, 43, 13, 169150, tzinfo=tzutc()), 'definition': '[]'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = wl.get_workspace(name='vllm-openai-test', create_if_not_exist=True)\n",
    "wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d7495e",
   "metadata": {},
   "source": [
    "### Upload the LLM\n",
    "\n",
    "The model is uploaded with the following parameters:\n",
    "\n",
    "* The model name\n",
    "* The file path to the model\n",
    "* The framework set to Wallaroo native vLLM runtime:  `wallaroo.framework.Framework.VLLM`\n",
    "* The input and output schemas are defined in Apache PyArrow format.  For OpenAI compatibility, this is left as an empty List.\n",
    "* Acceleration is set to NVIDIA CUDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aa6c20-f8bb-46eb-ae72-7c5ec9ab2fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10min.\n",
      "\n",
      "Model is pending loading to a container runtime...........................\n",
      "Model is attempting loading to a container runtime...................................\n",
      "Successful\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "model_step = wl.upload_model(\n",
    "    \"tinyllamaopenaiyns1\",\n",
    "    \"vllm-openai_tinyllama.zip\",\n",
    "    framework=Framework.VLLM,\n",
    "    input_schema=pa.schema([]),\n",
    "    output_schema=pa.schema([]),\n",
    "    convert_wait=True,\n",
    "    accel=Acceleration.CUDA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dbf7db",
   "metadata": {},
   "source": [
    "### Enable OpenAI Compatibility\n",
    "\n",
    "OpenAI compatibility is enabled via the **model configuration** from the class `wallaroo.openai_config.OpenaiConfig` includes the following main parameters.  The essential one is `enabled` - if OpenAI compatibility is **not** enabled, all other parameters are ignored.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| `enabled` | *Boolean* (*Default: False*) | If `True`, OpenAI compatibility is enabled.  If `False`, OpenAI compatibility is not enabled.  All other parameters are ignored if `enabled=False`. |\n",
    "| `completion_config` | *Dict* | The OpenAI API [`completion`](https://platform.openai.com/docs/api-reference/completions) parameters.  All `completion` parameters are available **except** `stream`; the `stream` parameter is **only** set at inference requests. |\n",
    "| `chat_completion_config` | *Dict* | The OpenAI API [`chat/completion`](https://platform.openai.com/docs/api-reference/chat/create) parameters.  All `completion` parameters are available **except** `stream`; the `stream` parameter is **only** set at inference requests. |\n",
    "\n",
    "With the `OpenaiConfig` object defined, it is when applied to the LLM configuration through the `openai_config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ccadc11-1ae1-4b08-acae-4c1cff4ec68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring as OpenAI\n",
    "\n",
    "openai_config = OpenaiConfig(enabled=True, chat_completion_config={\"temperature\": .3, \"max_tokens\": 200})\n",
    "model_step = model_step.configure(openai_config=openai_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96aa225",
   "metadata": {},
   "source": [
    "### Set the Deployment Configuration and Deploy\n",
    "\n",
    "The **deployment configuration** defines what resources are allocated to the LLM's exclusive use.  For this tutorial, the LLM is allocated:\n",
    "\n",
    "* 1 cpu\n",
    "* 8 Gi RAM\n",
    "* 1 GPU.  The GPU type is inherited from the model upload step.\n",
    "* The deployment label with the GPU resources used.\n",
    "\n",
    "Once the deployment configuration is set:\n",
    "\n",
    "* The pipeline is created and the LLM added as a **pipeline step**.\n",
    "* The pipeline is deployed with the deployment configuration.\n",
    "\n",
    "Once the deployment is complete, the LLM is ready to receive inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "161bad84-5049-4c5b-a3c4-61f475b1a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying\n",
    "\n",
    "deployment_config = wallaroo.DeploymentConfigBuilder() \\\n",
    "    .replica_count(1) \\\n",
    "    .cpus(.5) \\\n",
    "    .memory(\"1Gi\") \\\n",
    "    .sidekick_cpus(model_step, 1) \\\n",
    "    .sidekick_memory(model_step, '8Gi') \\\n",
    "    .sidekick_gpus(model_step, 1) \\\n",
    "    .deployment_label('wallaroo.ai/accelerator:l4') \\\n",
    "    .build()\n",
    "\n",
    "pipeline = wl.build_pipeline('tinyllama-openai')\n",
    "pipeline.clear()\n",
    "pipeline.add_model_step(model_step)\n",
    "pipeline.deploy(deployment_config = deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc59ccd",
   "metadata": {},
   "source": [
    "### Inference Requests on LLM with OpenAI Compatibility Enabled\n",
    "\n",
    "Inference requests on Wallaroo pipelines deployed with native vLLM runtimes or Wallaroo Custom with OpenAI compatibility enabled in Wallaroo are performed either through the Wallaroo SDK, or via OpenAPI endpoint requests.\n",
    "\n",
    "OpenAI API inference requests on models deployed with OpenAI compatibility enabled have the following conditions:\n",
    "\n",
    "* Parameters for `chat/completion` and `completion` **override** the existing OpenAI configuration options.\n",
    "* If the `stream` option is enabled:\n",
    "  * Outputs returned as list of chunks aka as an event stream.\n",
    "  * The request inference call completes when all chunks are returned.\n",
    "  * The response metadata includes `ttft`, `tps` and user-specified OpenAI request params **after** the last chunk is generated.\n",
    "\n",
    "#### OpenAI API Inference Requests via the Wallaroo SDK\n",
    "\n",
    "Inference requests with OpenAI compatible enabled models in Wallaroo via the Wallaroo SDK use the following methods:\n",
    "\n",
    "* `wallaroo.pipeline.Pipeline.openai_chat_completion`:  Submits an inference request using the OpenAI API `chat/completion` endpoint parameters.\n",
    "* `wallaroo.pipeline.Pipeline.openai_completion`: Submits an inference request using the OpenAI API `completion` endpoint parameters.\n",
    "\n",
    "The following demonstrates performing an inference request using `openai_chat_completion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2e6f0c0-0cae-41a8-8337-cdd5cbbaf531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Of course! Here's an updated version of the text with the added phrases:\\n\\nAs the sun rises over the horizon, the world awakens to a new day. The birds chirp and the birdsong fills the air, signaling the start of another beautiful day. The gentle breeze carries the scent of freshly cut grass and the promise of a new day ahead. The sun's rays warm the skin, casting a golden glow over everything in sight. The world awakens to a new day, a new chapter, a new beginning. The world is alive with energy and vitality, ready to take on the challenges of the day ahead. The birds chirp and the birdsong fills the air, signaling the start of another beautiful day. The gentle breeze carries the scent of freshly cut grass and the promise of a new day ahead. The sun's rays warm the skin\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing completions\n",
    "\n",
    "pipeline.openai_chat_completion(messages=[{\"role\": \"user\", \"content\": \"good morning\"}]).choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fa1b82",
   "metadata": {},
   "source": [
    "The following demonstrates performing an inference request using `openai_completion`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1b5f179-03d4-4e09-9484-ae74449cbef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', any first-person shooter game. Wallaroo is a comprehensive platform for building and tracking predictive models. This tool is really helpful in AI development. Wallaroo provides a unified platform for data and model developers to securely store or share data and access/optimize their AI models. It allows end-users to have a direct access to the development tools to customize and reuse code. Wallaroo has an intuitive User Interface that is easy to install and configure. Wallaroo handles entire the integration, deployment and infrastructure from data collection to dashboard visualisations. Can you provide some examples of how Wallaroo has been utilised in game development? Also, talk about the effectiveness of ML training using Wallaroo.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.openai_completion(prompt=\"tell me about wallaroo.AI\", max_tokens=200).choices[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef79b4",
   "metadata": {},
   "source": [
    "The following demonstrates performing an inference request using `openai_chat_completion` **with** token streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6b4e9f-0198-490b-af4a-809aeaeca81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a small village nestled in the heart of the countryside, there lived a young woman named Lily. Lily was a kind and gentle soul, always looking out for those in need. She had a heart full of love for her family and friends, and she was always willing to lend a helping hand.\n",
      "\n",
      "One day, Lily met a handsome young man named Jack. Jack was a charming and handsome man, with a"
     ]
    }
   ],
   "source": [
    "# Now with streaming\n",
    "\n",
    "for chunk in pipeline.openai_chat_completion(messages=[{\"role\": \"user\", \"content\": \"this is a short story about love\"}], max_tokens=100, stream=True):\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f1ff9b",
   "metadata": {},
   "source": [
    "The following demonstrates performing an inference request using `openai_completion` **with** token streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e63fd5aa-a3a7-4f82-b13c-56e1380c007e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\" this makes their life easier, but sometimes, when they have a story, they don't know how to tell it well. This frustrates them and makes their life even more difficult.\n",
      "\n",
      "b. Relaxation:\n",
      "protagonist: take a deep breath and let it out. Why not start with a song? \"Eyes full of longing, I need your music to embrace.\" this calms them down and lets them relax, giving them more patience to continue with their story.\n",
      "\n",
      "c. Inspirational quotes:\n",
      "protagonist: this quote from might jeffries helps me reflect on my beliefs and values: \"the mind is a powerful thing, it can change your destiny at any time. Fear no fear, only trust your divineline and reclaim your destiny.\" listening to this quote always helps me keep my thoughts in perspective, and gets me back to my story with renewed vigor."
     ]
    }
   ],
   "source": [
    "# Now with streaming\n",
    "\n",
    "for chunk in pipeline.openai_completion(prompt=\"tell me a short story\", max_tokens=300, stream=True):\n",
    "    print(chunk.choices[0].text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3967f9",
   "metadata": {},
   "source": [
    "#### OpenAI API Inference Requests via the OpenAPI Client Endpoints\n",
    "\n",
    "Wallaroo deployed pipelines provide a deployment inference URL for inference requests via API methods.  Pipelines deployed with LLMs with OpenAI API compatibility add OpenAI extensions to the inference URL for direct inference requests.\n",
    "\n",
    "The following examples demonstrate performing inference requests through the deployed pipeline's OpenAI API compatibility endpoint extensions.\n",
    "\n",
    "Note that the command `token = wl.auth.auth_header()['Authorization'].split()[1]` retrieves the authentication token used to authenticate to Wallaroo before performing the inference request via API calls.\n",
    "\n",
    "##### Connect to the OpenAI API Endpoint\n",
    "\n",
    "The following command connects the OpenAI client to the deployed pipeline's OpenAI endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a24d20-a70d-4bfb-87f7-65efe4b08d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now using the OpenAI client\n",
    "\n",
    "token = wl.auth.auth_header()['Authorization'].split()[1]\n",
    "\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url='https://autoscale-uat-gcp.wallaroo.dev/v1/api/pipelines/infer/tinyllama-openai-414/tinyllama-openai/openai/v1',\n",
    "    api_key=token\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0945f7",
   "metadata": {},
   "source": [
    "##### OpenAI API Inference Request Examples\n",
    "\n",
    "The following demonstrates performing an inference request using the `chat/completions` endpoint with token streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc238ea-241f-48a3-a4d6-ef5eb09d8ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a warm summer evening, and the sun was setting over the city. A young couple, Alex and Emily, had just walked out of a coffee shop, hand in hand. They were laughing and chatting, enjoying the last few moments of their day.\n",
      "\n",
      "As they walked down the street, Alex turned to Emily and said, \"I love you, Emily.\"\n",
      "\n",
      "Emily's eyes widened in surprise, and she smiled. \"I love you too, Alex.\"\n",
      "\n",
      "They walked for a few more blocks, hand in hand, and finally, they arrived at a park. They sat down on a bench, and Alex took Emily's hand.\n",
      "\n",
      "\"I know this is a little sudden,\" Alex said, \"but I feel like we've been together for a while now. I want to spend the rest of my life with you.\"\n",
      "\n",
      "Emily looked at him, her eyes filled with tears. \"I feel the same way, Alex. I love you more than anything in this world.\"\n",
      "\n",
      "They sat there, holding hands, for what felt like hours. They talked about everything and anything, their hearts beating in unison.\n",
      "\n",
      "As the sun began to set, Alex and Emily stood up, and they walked back to the coffee shop. They hugged each other tightly, tears streaming down their faces.\n",
      "\n",
      "\"I love you, Alex,\" Emily said, her voice shaking.\n",
      "\n",
      "Alex smiled, and he said, \"I love you too, Emily.\"\n",
      "\n",
      "They walked back to their apartment, hand in hand, and spent the rest of the night talking and laughing.\n",
      "\n",
      "Over the next few weeks, Alex and Emily's love grew stronger. They spent every moment they could together, exploring the city, going on walks, and enjoying each other's company.\n",
      "\n",
      "One day, they decided to take a walk in a nearby park. As they walked, they talked about everything and anything, their hearts beating in unison.\n",
      "\n",
      "As they reached the end of the path, Alex turned to Emily and said, \"I love you, Emily. I know we've only been together for a short time, but I feel like we've been through so much together. I want to spend the rest of my life with you.\"\n",
      "\n",
      "Emily looked at him, her eyes filled with tears. \"I feel the same way, Alex. I love you more than anything in this world.\"\n",
      "\n",
      "They stood there, holding hands, and Alex said, \"I love you, Emily. I want to spend the rest of my life with you.\"\n",
      "\n",
      "Emily smiled, tears streaming down her face. \"I love you too, Alex. I know this is a little sudden, but I feel like we've been together for a while now. I want to spend the rest of my life with you.\"\n",
      "\n",
      "They walked back to their apartment, hand in hand, and spent the rest of the night talking and laughing.\n",
      "\n",
      "From that day on, Alex and Emily's love grew stronger. They spent every moment they could together, exploring the city, going on walks, and enjoying each other's company.\n",
      "\n",
      "Years later, they were married, and they had two beautiful children. They knew that their love had been a little sudden, but they knew that it was worth it. They knew that they had found each other, and they knew that they would spend the rest of their lives together, loving and cherishing each other."
     ]
    }
   ],
   "source": [
    "for chunk in client.chat.completions.create(model=\"dummy\", \n",
    "                                            messages=[{\"role\": \"user\", \"content\": \"this is a short story about love\"}], \n",
    "                                            max_tokens=1000, \n",
    "                                            stream=True):\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb86b33",
   "metadata": {},
   "source": [
    "The following demonstrates performing an inference request using the `completions` endpoint with token streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25497647-f6d6-4518-88a5-d1f8cadcca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'s robotic fabrication technology: can you provide some examples of products that have been milled using wallarooâ€™s robots?"
     ]
    }
   ],
   "source": [
    "for chunk in client.completions.create(model=\"dummy\", prompt=\"tell me about wallaroo.AI\", max_tokens=1000, stream=True):\n",
    "    print(chunk.choices[0].text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff85047-aef2-455c-b33d-aa66f68c4c8b",
   "metadata": {},
   "source": [
    "### Publish Pipeline for Edge Deployment\n",
    "\n",
    "Wallaroo pipelines are published to Open Container Initiative (OCI) Registries for remote/edge deployments via the `wallaroo.pipeline.Pipeline.publish(deployment_config)` command.  This uploads the following artifacts to the OCI registry:\n",
    "\n",
    "* The native vLLM runtimes or custom models with OpenAI compatibility enabled.\n",
    "* If specified, the deployment configuration.\n",
    "* The Wallaroo engine for the architecture and AI accelerator, both inherited from the model settings at model upload.\n",
    "\n",
    "Once the publish process is complete, the pipeline can be deployed to one or more edge/remote environments.\n",
    "\n",
    "The following demonstrates publishing the RAG Llama pipeline created and tested in the previous steps.  Once published, it can be deployed to edge locations with the required resources matching the deployment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "645e187f-80a8-4569-9381-40c874cfaa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for pipeline publish... It may take up to 600 sec.\n",
      "................................. Published.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "          <table>\n",
       "              <tr><td>ID</td><td>72</td></tr>\n",
       "              <tr><td>Pipeline Name</td><td>tinyllama-openai</td></tr>\n",
       "              <tr><td>Pipeline Version</td><td>56b2cebd-fdc7-4c68-a081-837585df6a61</td></tr>\n",
       "              <tr><td>Status</td><td>Published</td></tr>\n",
       "              <tr><td>Workspace Id</td><td>1689</td></tr>\n",
       "              <tr><td>Workspace Name</td><td>vllm-openai-test</td></tr>\n",
       "              <tr><td>Edges</td><td></td></tr>\n",
       "              <tr><td>Engine URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-6232'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-6232</a></td></tr>\n",
       "              <tr><td>Pipeline URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/tinyllama-openai:56b2cebd-fdc7-4c68-a081-837585df6a61'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/tinyllama-openai:56b2cebd-fdc7-4c68-a081-837585df6a61</a></td></tr>\n",
       "              <tr><td>Helm Chart URL</td><td>oci://<a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/tinyllama-openai'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/tinyllama-openai</a></td></tr>\n",
       "              <tr><td>Helm Chart Reference</td><td>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:74fe1ea0410b4dad90dbda3db10904728c5a5c0c2ea2b60d0d8e889e4617347b</td></tr>\n",
       "              <tr><td>Helm Chart Version</td><td>0.0.1-56b2cebd-fdc7-4c68-a081-837585df6a61</td></tr>\n",
       "              <tr><td>Engine Config</td><td>{'engine': {'resources': {'limits': {'cpu': 0.5, 'memory': '1Gi'}, 'requests': {'cpu': 0.5, 'memory': '1Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none', 'cpu_utilization': 50.0}, 'images': {'tinyllamaopenaiyns1-766': {'resources': {'limits': {'cpu': 1.0, 'memory': '8Gi'}, 'requests': {'cpu': 1.0, 'memory': '8Gi'}, 'accel': 'none', 'arch': 'x86', 'gpu': True}}}}}</td></tr>\n",
       "              <tr><td>User Images</td><td>[]</td></tr>\n",
       "              <tr><td>Created By</td><td>john.hummel@wallaroo.ai</td></tr>\n",
       "              <tr><td>Created At</td><td>2025-07-01 17:27:42.112064+00:00</td></tr>\n",
       "              <tr><td>Updated At</td><td>2025-07-01 17:27:42.112064+00:00</td></tr>\n",
       "              <tr><td>Replaces</td><td></td></tr>\n",
       "              <tr>\n",
       "                  <td>Docker Run Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">docker run \\\n",
       "    -p $EDGE_PORT:8080 \\\n",
       "    -e OCI_USERNAME=$OCI_USERNAME \\\n",
       "    -e OCI_PASSWORD=$OCI_PASSWORD \\\n",
       "    -e PIPELINE_URL=us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/tinyllama-openai:56b2cebd-fdc7-4c68-a081-837585df6a61 \\\n",
       "    -e CONFIG_CPUS=1.0 --gpus all --cpus=1.5 --memory=9g \\\n",
       "    us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-6232</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>EDGE_PORT</code>, <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              <tr>\n",
       "                  <td>Helm Install Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">helm install --atomic $HELM_INSTALL_NAME \\\n",
       "    oci://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/tinyllama-openai \\\n",
       "    --namespace $HELM_INSTALL_NAMESPACE \\\n",
       "    --version 0.0.1-56b2cebd-fdc7-4c68-a081-837585df6a61 \\\n",
       "    --set ociRegistry.username=$OCI_USERNAME \\\n",
       "    --set ociRegistry.password=$OCI_PASSWORD</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>HELM_INSTALL_NAME</code>, <code>HELM_INSTALL_NAMESPACE</code>,\n",
       "                          <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              \n",
       "          </table>\n",
       "        "
      ],
      "text/plain": [
       "PipelinePublish(created_at=datetime.datetime(2025, 7, 1, 17, 27, 42, 112064, tzinfo=tzutc()), docker_run_variables={'PIPELINE_URL': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/tinyllama-openai:56b2cebd-fdc7-4c68-a081-837585df6a61'}, engine_config={'engine': {'resources': {'limits': {'cpu': 0.5, 'memory': '1Gi'}, 'requests': {'cpu': 0.5, 'memory': '1Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none', 'cpu_utilization': 50.0}, 'images': {'tinyllamaopenaiyns1-766': {'resources': {'limits': {'cpu': 1.0, 'memory': '8Gi'}, 'requests': {'cpu': 1.0, 'memory': '8Gi'}, 'accel': 'none', 'arch': 'x86', 'gpu': True}}}}}, id=72, pipeline_name='tinyllama-openai', pipeline_version_id=1795, replaces=[], status='Published', updated_at=datetime.datetime(2025, 7, 1, 17, 27, 42, 112064, tzinfo=tzutc()), user_images=[], created_by='7d603858-88e0-472e-8f71-e41094afd7ec', created_on_version='2025.1.1', edge_bundles=<wallaroo.wallaroo_ml_ops_api_client.types.Unset object at 0x7bdff20f3070>, engine_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-6232', error=None, helm={'reference': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:74fe1ea0410b4dad90dbda3db10904728c5a5c0c2ea2b60d0d8e889e4617347b', 'values': {}, 'chart': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/tinyllama-openai', 'version': '0.0.1-56b2cebd-fdc7-4c68-a081-837585df6a61'}, pipeline_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/tinyllama-openai:56b2cebd-fdc7-4c68-a081-837585df6a61', pipeline_version_name='56b2cebd-fdc7-4c68-a081-837585df6a61', workspace_id=1689, workspace_name='vllm-openai-test', additional_properties={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.publish(deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4892446",
   "metadata": {},
   "source": [
    "### Undeploy\n",
    "\n",
    "With the tutorial complete, the pipeline is undeployed to return the resources from the LLM's exclusive use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ace57433-04ee-40a5-93e0-e8ed75bc5316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 600s .................................... ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>tinyllama-openai</td></tr><tr><th>created</th> <td>2025-05-30 20:40:46.518566+00:00</td></tr><tr><th>last_updated</th> <td>2025-05-30 21:15:17.806262+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>1689</td></tr><tr><th>workspace_name</th> <td>vllm-openai-test</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>cuda</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>c594f433-eaa7-45d9-903a-270314c1e3aa, 0017f356-8104-4708-ad73-70b5f93201d1, 239cf2e0-7e2c-4fe1-95fd-39aeacc559e8, e0337b32-0ff2-43e7-86b6-ecce344e326c</td></tr><tr><th>steps</th> <td>tinyllamaopenai</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'tinyllama-openai', 'create_time': datetime.datetime(2025, 5, 30, 20, 40, 46, 518566, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'tinyllamaopenai', 'version': 'a1af7271-05f5-4032-8434-4623fdc1b6c2', 'sha': 'db68af9c290cdc8d047b7ac70f5acbd446435d2767ac4dfd51509b750a78bdd0'}]}}]\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5df58b6-eb02-4200-adc1-58c00b83e650",
   "metadata": {},
   "source": [
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
