{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f999ec",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/llm-deploy/power10-deploy-llamacpp).\n",
    "\n",
    "## Llamacpp Deploy on IBM Power10 Tutorial\n",
    "\n",
    "The following demonstrates deploying a Llama V3 8B quantized with llama-cpp encapsulated in the Wallaroo Custom Model aka BYOP Framework on IBM Power10 architecture.\n",
    "\n",
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "Deploying on IBM Power10 is made easy with Wallaroo.  Models are defined with an **architecture** at the upload stage, allowing the deployment of the same model on different architectures.\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial demonstrates using Wallaroo to:\n",
    "\n",
    "* Upload a model and define the deployment architecture as `Power10`.\n",
    "* Deploy the model on a node with the Power10 chips and perform a sample inference.\n",
    "* Publish the model to an OCI registry for deployment on edge and multi-cloud environments with Power10 chips.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The following tutorial requires the following:\n",
    "\n",
    "* Wallaroo version 2024.4 and above.\n",
    "* At least one Power10 node deployed in the cluster.\n",
    "* Llama V3 8B quantized with llama-cpp encapsulated in the Wallaroo Custom Model aka BYOP Framework.  This is available through a Wallaroo representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c2ad60",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca0bc3e-9a4a-4b34-bb17-bd0fffb6a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "from wallaroo.engine_config import Acceleration\n",
    "from wallaroo.dynamic_batching_config import DynamicBatchingConfig\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34701937",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is established via the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6712f9-871f-4224-8a9e-d44520a25628",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558ea37-f227-45dc-a637-c8a7a0e552db",
   "metadata": {},
   "source": [
    "### Upload Model for Power10 Deployment\n",
    "\n",
    "The model **architecture** is specified during the model upload process.  By default this is set to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a6693e0-f96b-45e5-8253-437c9ca28bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string())\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field(\"generated_text\", pa.string())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ceec20",
   "metadata": {},
   "source": [
    "#### Upload the Model and Set Architecture to Power10\n",
    "\n",
    "We upload the model, and specify the architecture as `wallaroo.engine_config.Architecture.Power10`.  Once the upload is complete, it will be ready for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8718e6-3475-42d1-b5c5-e3a57fc54b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime...\n",
      "Model is attempting loading to a container runtime....................................................................................................."
     ]
    }
   ],
   "source": [
    "model = wl.upload_model('llama-cpp-sdk-power2', \n",
    "    'byop_llamacpp_power.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema,\n",
    "    arch=Architecture.Power10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0767f9-8b74-4025-831e-49ebb06599b6",
   "metadata": {},
   "source": [
    "### Deployment on Power10\n",
    "\n",
    "To deploy the model, we do the following:\n",
    "\n",
    "* Specify the **deployment configuration**.  This defines the resources allocated from the cluster for the model's use.  Notice we do **not** specify the architecture in this step - this is inherited from the model.  For this deployment, the model will be allocated from the Power10 node:\n",
    "  * CPUS: 4\n",
    "  * Memory:  10Gi\n",
    "* Build the Pipeline and model steps:  The Wallaroo pipeline defines the models and order that receive inference data.\n",
    "* Deploy the pipeline with the deployment configuration:  This step deploys the model with the specified resources.\n",
    "\n",
    "Once complete, the model is ready for inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e7a7cf-5fc8-44ec-b16b-d0bd82bb22e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(model, 4) \\\n",
    "    .sidekick_memory(model, '10Gi') \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0c6c5-bee6-4be3-9fd3-67f30532a0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"llamacpp-pipeyns-power\")\n",
    "pipeline.add_model_step(model)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1e7b8b-f92e-4caf-837c-4444061fdbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>llamacpp-pipeyns-power</td></tr><tr><th>created</th> <td>2024-12-24 19:47:42.761843+00:00</td></tr><tr><th>last_updated</th> <td>2024-12-24 19:47:43.105666+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>22</td></tr><tr><th>workspace_name</th> <td>younes.amar@wallaroo.ai - Default Workspace</td></tr><tr><th>arch</th> <td>power10</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>f19ae86a-b7de-4922-9dc8-c52af0de3f21, a1dee100-a225-4633-9541-7daed74b541c</td></tr><tr><th>steps</th> <td>llama-cpp-sdk-power2</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'llamacpp-pipeyns-power', 'create_time': datetime.datetime(2024, 12, 24, 19, 47, 42, 761843, tzinfo=tzutc()), 'definition': '[]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline=wl.get_pipeline(\"llamacpp-pipeyns-power\")\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de95ca9-0a64-4d8f-b91f-60b1527e26f1",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Once the LLM is deployed, we'll perform an inference with the `wallaroo.pipeline.Pipeline.infer` method, which accepts either a pandas DataFrame or an Apache Arrow table.\n",
    "\n",
    "For this example, we'll create a pandas DataFrame with a text query and submit that for our inference request, then display the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c468b4da-ff9f-4639-b972-c8d7cd68da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': ['Describe what roland garros is']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c9feef8-ce6d-4aca-9f2e-2105a8efd79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Roland-Garros, also known as the French Open, is a tennis tournament held annually in Paris, France. It is one of the four Grand Slam tennis tournaments, the most prestigious and important events in tennis. The tournament is played on clay courts, which are slower than grass or hard courts, and it is typically held in late May and early June. The tournament was founded in 1928 and is named after the French tennis player Richard \"Dick\" Roland-Garros, who was a prominent player in the 1920s and 1930s. The tournament has been played at the Stade Roland-Garros in Paris since 1928.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=pipeline.infer(data, timeout=10000)\n",
    "result[\"out.generated_text\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9588d038-84e0-4174-a05d-342e7f14586b",
   "metadata": {},
   "source": [
    "### Undeploy the Model\n",
    "\n",
    "With the inference example complete, we undeploy the model and resource the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f083e08",
   "metadata": {},
   "source": [
    "### Publish the Model\n",
    "\n",
    "Publishing the model takes the model, pipeline, and Wallaroo engine and puts them into an OCI compliant registry for later deployment on Edge and multi-cloud environments.\n",
    "\n",
    "Publishing the pipeline uses the pipeline `wallaroo.pipeline.Pipeline.publish()` command.  This requires that the Wallaroo Ops instance have [Edge Registry Services](https://docs.wallaroo.ai/wallaroo-platform-operations/wallaroo-platform-operations-configure/wallaroo-edge-deployment/#enable-wallaroo-edge-deployment-registry) enabled.\n",
    "\n",
    "When publishing, we specify the pipeline deployment configuration through the `wallaroo.DeploymentConfigBuilder`.  For our example, we do **not** specify the architecture; the architecture and is inherited from the model.\n",
    "\n",
    "The following publishes the pipeline to the OCI registry and displays the container details.  For more information, see [Wallaroo SDK Essentials Guide: Pipeline Edge Publication](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline-publication/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6cb5e52-6c24-4a85-ab62-30875554c6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for pipeline publish... It may take up to 600 sec.\n",
      "Pipeline is publishing..................... Published.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "          <table>\n",
       "              <tr><td>ID</td><td>1</td></tr>\n",
       "              <tr><td>Pipeline Name</td><td>llamacpp-pipeyns-power</td></tr>\n",
       "              <tr><td>Pipeline Version</td><td>4e5f120a-89ad-4233-9aac-cc25158d232e</td></tr>\n",
       "              <tr><td>Status</td><td>Published</td></tr>\n",
       "              <tr><td>Engine URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-ppc64le:v2024.4.0-5866'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-ppc64le:v2024.4.0-5866</a></td></tr>\n",
       "              <tr><td>Pipeline URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llamacpp-pipeyns-power:4e5f120a-89ad-4233-9aac-cc25158d232e'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llamacpp-pipeyns-power:4e5f120a-89ad-4233-9aac-cc25158d232e</a></td></tr>\n",
       "              <tr><td>Helm Chart URL</td><td>oci://<a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llamacpp-pipeyns-power'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llamacpp-pipeyns-power</a></td></tr>\n",
       "              <tr><td>Helm Chart Reference</td><td>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:b99e8b7e75bd714dd4820a18e7d6850496bd34b25a9e43e3aac19db9c4ae1a04</td></tr>\n",
       "              <tr><td>Helm Chart Version</td><td>0.0.1-4e5f120a-89ad-4233-9aac-cc25158d232e</td></tr>\n",
       "              <tr><td>Engine Config</td><td>{'engine': {'resources': {'limits': {'cpu': 1.0, 'memory': '2Gi'}, 'requests': {'cpu': 1.0, 'memory': '2Gi'}, 'accel': 'none', 'arch': 'power10', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none'}, 'images': {'llama-cpp-sdk-power2-62': {'resources': {'limits': {'cpu': 4.0, 'memory': '10Gi'}, 'requests': {'cpu': 4.0, 'memory': '10Gi'}, 'accel': 'none', 'arch': 'power10', 'gpu': False}}}}}</td></tr>\n",
       "              <tr><td>User Images</td><td>[]</td></tr>\n",
       "              <tr><td>Created By</td><td>younes.amar@wallaroo.ai</td></tr>\n",
       "              <tr><td>Created At</td><td>2024-12-24 21:25:18.860069+00:00</td></tr>\n",
       "              <tr><td>Updated At</td><td>2024-12-24 21:25:18.860069+00:00</td></tr>\n",
       "              <tr><td>Replaces</td><td></td></tr>\n",
       "              <tr>\n",
       "                  <td>Docker Run Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">docker run \\\n",
       "    -p $EDGE_PORT:8080 \\\n",
       "    -e OCI_USERNAME=$OCI_USERNAME \\\n",
       "    -e OCI_PASSWORD=$OCI_PASSWORD \\\n",
       "    -e PIPELINE_URL=us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llamacpp-pipeyns-power:4e5f120a-89ad-4233-9aac-cc25158d232e \\\n",
       "    -e CONFIG_CPUS=1.0 --cpus=5.0 --memory=12g \\\n",
       "    us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-ppc64le:v2024.4.0-5866</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>EDGE_PORT</code>, <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              <tr>\n",
       "                  <td>Helm Install Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">helm install --atomic $HELM_INSTALL_NAME \\\n",
       "    oci://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llamacpp-pipeyns-power \\\n",
       "    --namespace $HELM_INSTALL_NAMESPACE \\\n",
       "    --version 0.0.1-4e5f120a-89ad-4233-9aac-cc25158d232e \\\n",
       "    --set ociRegistry.username=$OCI_USERNAME \\\n",
       "    --set ociRegistry.password=$OCI_PASSWORD</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>HELM_INSTALL_NAME</code>, <code>HELM_INSTALL_NAMESPACE</code>,\n",
       "                          <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              \n",
       "          </table>\n",
       "        "
      ],
      "text/plain": [
       "PipelinePublish(created_at=datetime.datetime(2024, 12, 24, 21, 25, 18, 860069, tzinfo=tzutc()), docker_run_variables={'PIPELINE_URL': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llamacpp-pipeyns-power:4e5f120a-89ad-4233-9aac-cc25158d232e'}, engine_config={'engine': {'resources': {'limits': {'cpu': 1.0, 'memory': '2Gi'}, 'requests': {'cpu': 1.0, 'memory': '2Gi'}, 'accel': 'none', 'arch': 'power10', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none'}, 'images': {'llama-cpp-sdk-power2-62': {'resources': {'limits': {'cpu': 4.0, 'memory': '10Gi'}, 'requests': {'cpu': 4.0, 'memory': '10Gi'}, 'accel': 'none', 'arch': 'power10', 'gpu': False}}}}}, id=1, pipeline_name='llamacpp-pipeyns-power', pipeline_version_id=105, replaces=[], status='Published', updated_at=datetime.datetime(2024, 12, 24, 21, 25, 18, 860069, tzinfo=tzutc()), user_images=[], created_by='58dcb599-1a7d-4b4f-9633-8aaf7d6f0aeb', created_on_version='2024.4.0', edge_bundles=<wallaroo.wallaroo_ml_ops_api_client.types.Unset object at 0x76b859f30370>, engine_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-ppc64le:v2024.4.0-5866', error=None, helm={'reference': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:b99e8b7e75bd714dd4820a18e7d6850496bd34b25a9e43e3aac19db9c4ae1a04', 'values': {}, 'chart': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llamacpp-pipeyns-power', 'version': '0.0.1-4e5f120a-89ad-4233-9aac-cc25158d232e'}, pipeline_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llamacpp-pipeyns-power:4e5f120a-89ad-4233-9aac-cc25158d232e', pipeline_version_name='4e5f120a-89ad-4233-9aac-cc25158d232e', additional_properties={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.publish(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc097bfe-e91d-4c25-8ac3-38a346f16afd",
   "metadata": {},
   "source": [
    "### Edge Deployment\n",
    "\n",
    "Included with the publish details include instructions on deploying the model via `Docker Run` and `Helm Install` commands with the defined deployment configuration on the Power10 architecture.\n",
    "\n",
    "## Contact Us\n",
    "\n",
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
