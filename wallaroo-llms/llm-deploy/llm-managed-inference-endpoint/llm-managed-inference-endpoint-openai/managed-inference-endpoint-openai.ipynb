{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edf484df-a53a-4868-bff6-39065c582559",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.4_tutorials/wallaroo-llms/llm-deploy/llm-managed-inference-endpoint/llm-managed-inference-endpoint-openai).\n",
    "\n",
    "## Wallaroo Deployment of Managed Inference Endpoint Models with OpenAI\n",
    "\n",
    "The following tutorial demonstrates uploading, deploying, inferencing and monitoring a [LLM with Managed Inference Endpoints](https://docs.wallaroo.ai/wallaroo-llm/wallaroo-llm-package-deployment/wallaroo-llm-monitoring-external-endpoints/).\n",
    "\n",
    "These models leverage LLMs deployed in other services, with Wallaroo providing a single source for inference requests, logging results, monitoring for hate/abuse/racism and other factors, and tracking model drift through Wallaroo assays.\n",
    "\n",
    "## Provided Models\n",
    "\n",
    "The following models are provided:\n",
    "\n",
    "* `gpt35.zip`: A [Wallaroo BYOP](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/) model that uses OpenAI as a Managed Inference Endpoint.\n",
    "* `summarization_quality.zip`: A [Wallaroo BYOP](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/) model that scores the quality of the LLM response.\n",
    "\n",
    "### OpenAI Python Library Requirements\n",
    "\n",
    "BYOP models artifacts include an optional `requirements.txt` file to set the Python libraries used when deploying the model.  For Wallaroo 2024.4, the following are **required** for LLMs with Managed Inference Endpoints with OpenAI contained within the BYOP framework.\n",
    "\n",
    "| Library | Required Version |\n",
    "|---|---|\n",
    "| [httpx](https://pypi.org/project/httpx/) | `httpx==0.27.2` |\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This tutorial requires:\n",
    "\n",
    "* Wallaroo 2024.1 and above\n",
    "* Credentials for authenticating to OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dbbd01-2c72-4f74-aad7-d99f60ef3d13",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Library\n",
    "\n",
    "The following libraries are used to upload and perform inferences on the LLM with Managed Inference Endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30cd4413-2433-4665-8b95-44fe6dda9557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.framework import Framework\n",
    "import pyarrow as pa\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f1e2a6",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is opened through the Wallaroo SDK client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/).\n",
    "\n",
    "The `request_timeout` flag is used for Wallaroo BYOP models where the file size may require additional time to complete the upload process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e123d6a5-f612-4de6-9edc-7bda2d334394",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client(request_timeout=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f7713",
   "metadata": {},
   "source": [
    "### Set Workspace\n",
    "\n",
    "The following creates or connects to an existing workspace based on the variable `workspace_name`, and sets it as the current workspace.  For more details on Wallaroo workspaces, see [Wallaroo Workspace Management Guide](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-optimize/wallaroo-workspace-management/).\n",
    "\n",
    "For this tutorial, the workspace name `openai` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = wl.get_workspace('openai', create_if_not_exist=True)\n",
    "_ = wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00216320",
   "metadata": {},
   "source": [
    "### LLM with Managed Inference Endpoint Model Code\n",
    "\n",
    "The Wallaroo BYOP model `gpt35.zip` contains the following artifacts:\n",
    "\n",
    "* `main.py`: Python script that controls the behavior of the model.\n",
    "* `requirements.txt`:  Python requirements file that sets the Python libraries used.\n",
    "* `secret_key.json`: Secret key that contains the [OpenAI API key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key).\n",
    "\n",
    "The model performs the following.\n",
    "\n",
    "1. Accepts the inference request from the requester.\n",
    "2. Connect to OpenAI using the OpenAI API Key.\n",
    "\n",
    "    ```python\n",
    "    def create(self, config: CustomInferenceConfig) -> GPTInference:\n",
    "        inference = self.inference\n",
    "        \n",
    "        with open(os.path.join(config.model_path, 'secret_key.json')) as file:\n",
    "            auth = json.load(file)\n",
    "        \n",
    "        inference.model = OpenAI(api_key=auth['API_SECRET'])\n",
    "\n",
    "        return inference\n",
    "    ```\n",
    "\n",
    "3. Take the inference request, connect to OpenAI and submit the request.  The inference result is returned to the BYOP model, which is then returned.\n",
    "\n",
    "    ```python\n",
    "    def _predict(self, input_data: InferenceData):\n",
    "        generated_texts = []\n",
    "        prompts = input_data[\"text\"].tolist()\n",
    "\n",
    "        for prompt in prompts:\n",
    "            result = self.model.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo-1106\",\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=1,\n",
    "                max_tokens=256,\n",
    "                top_p=1,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0\n",
    "            )\n",
    "            \n",
    "            generated_texts.append(result.choices[0].message.content)\n",
    "\n",
    "        prompt = np.array([str(x) for x in input_data[\"text\"]])\n",
    "        \n",
    "        return {\"text\": prompt, \"generated_text\": np.array(generated_texts)}\n",
    "    ```\n",
    "\n",
    "This model is contained in a Wallaroo pipeline which accepts the inference request, and returns the final result back to the requester."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b2add-3e83-4ae9-a800-583cd8bac32a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload LLM with Managed Inference Endpoint Model\n",
    "\n",
    "Uploading models uses the Wallaroo Client `upload_model` method, which takes the following parameters:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| `name` | `string` (*Required*) | The name of the model.  Model names are unique **per workspace**.  Models that are uploaded with the same name are assigned as a new **version** of the model. |\n",
    "| `path` | `string` (*Required*) | The path to the model file being uploaded. |\n",
    "| `framework` |`string` (*Required*) | The framework of the model from `wallaroo.framework`. |\n",
    "| `input_schema` | `pyarrow.lib.Schema` (*Required*) | The input schema in Apache Arrow schema format. |\n",
    "| `output_schema` | `pyarrow.lib.Schema` (*Required*) | The output schema in Apache Arrow schema format. |\n",
    "\n",
    "The following shows the upload parameters for the `gpt35.zip` Wallaroo BYOP model with the following input and output schema:\n",
    "\n",
    "* Input:\n",
    "  * `text` (*String*):  The input text.\n",
    "* Output:\n",
    "  * `text` (*String*): The original input text.\n",
    "  * `generated_text` (*String*): The result returned from the GPT 3.5 model as a Managed Inference Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200d3894-acd0-49e6-8919-7fe2abc61a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10.0min.\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime..............successful\n",
      "\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('text', pa.string()),\n",
    "]) \n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field('text', pa.string()),\n",
    "    pa.field('generated_text', pa.string()),\n",
    "])\n",
    "\n",
    "gpt = wl.upload_model('gpt-35', \n",
    "                      r'models/gpt35.zip',\n",
    "                      framework=Framework.CUSTOM,\n",
    "                      input_schema=input_schema,\n",
    "                      output_schema=output_schema,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9a978b-57ca-4efb-8932-df2ef9e3e96f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set Deployment Configuration\n",
    "\n",
    "The deployment configuration sets the resources assigned to the LLM and the LLM Validation Listener model.  For this example, following resources are applied.\n",
    "\n",
    "* `gpt35.zip`: 0.5 cpus, 1 Gi RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec4d3771-3a25-4ff5-a385-72a60dd5bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder() \\\n",
    "    .cpus(1) \\\n",
    "    .memory('1Gi') \\\n",
    "    .sidekick_cpus(gpt, 0.5) \\\n",
    "    .sidekick_memory(gpt, '1Gi') \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58efcd-9fbd-4e8e-a724-626fe6843b8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy Model\n",
    "\n",
    "To deploy the model:\n",
    "\n",
    "1. We build a Wallaroo pipeline and assign the model as a pipeline step.  For this tutorial it is called `gpt35-wrapper-pipeline`.\n",
    "2. The pipeline is deployed with the deployment configuration.\n",
    "3. Once the resources allocation is complete, the model is ready for inferencing.\n",
    "\n",
    "See [Model Deploy](https://docs.wallaroo.ai/wallaroo-llm/wallaroo-llm-package-deployment/) for more details on deploying LLMs in Wallaroo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37c59d1d-2564-4172-9491-11460c144455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 480s ..................................... ok\n"
     ]
    }
   ],
   "source": [
    "gpt_pipeline = wl.build_pipeline(\"gpt35-wrapper-pipeline\") \\\n",
    "    .add_model_step(gpt) \\\n",
    "    .deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b82c43-205b-4fb4-ba15-7e96a001c13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate Inference Request\n",
    "\n",
    "The inference request will be submitted as a pandas DataFrame as a text entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eede1f00-513c-4152-9758-3a879d15b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = '''Please summarize this text in one sentence: \n",
    "\n",
    "Simplify production AI for seamless self-checkout or cashierless experiences at scale, enabling any retail store to offer a modern shopping journey. \n",
    "We reduce the technical overhead and complexity for delivering a checkout experience that’s easy and efficient no matter where your stores are located. \n",
    "Eliminate Checkout Delays: Easy and fast model deployment for a smooth self-checkout process, allowing customers to enjoy faster, hassle-free shopping experiences. \n",
    "Drive Operational Efficiencies: Simplifying the process of scaling AI-driven self-checkout solutions to multiple retail locations ensuring uniform customer experiences no matter the location of the store while reducing in-store labor costs. \n",
    "Continuous Improvement: Enabling integrated data insights for informing self-checkout improvements across various locations, ensuring the best customer experience, regardless of where they shop.\n",
    "'''\n",
    "\n",
    "data = pd.DataFrame({\"text\": [text]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30f05bb",
   "metadata": {},
   "source": [
    "### Submit Inference Request\n",
    "\n",
    "The inference request is submitted to the pipeline with the `infer` method, which accepts either:\n",
    "\n",
    "* pandas DataFrame\n",
    "* Apache Arrow Table\n",
    "\n",
    "The results are returned in the same format as submitted.  For this example, a pandas DataFrame is submitted, so a pandas DataFrame is returned.  The final generated text is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a40889-c8ad-4e6c-b4d1-8033b3a17349",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = gpt_pipeline.infer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae799ae9-f37d-4bef-b855-8b493821d37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This text outlines the benefits of simplifying production AI for self-checkout experiences at retail stores, including reducing technical overhead, eliminating checkout delays, driving operational efficiencies, and enabling continuous improvement for a seamless and efficient shopping journey.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['out.generated_text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d130dc-ff57-4682-bd92-8630390b7462",
   "metadata": {
    "tags": []
   },
   "source": [
    "### LLM Listener with OpenAI Managed Inference Endpoint LLM\n",
    "\n",
    "The results of the BYOP with Managed Inference Endpoint are scored with a [in-line LLM Validation Listener](https://docs.wallaroo.ai/wallaroo-llm/wallaroo-llm-monitoring/wallaroo-llm-monitoring-in-line/) or an [offline LLM Monitoring Listener](https://docs.wallaroo.ai/wallaroo-llm/wallaroo-llm-monitoring/wallaroo-llm-monitoring-listeners/).  The following demonstrates using a LLM Listener to evaluate the outputs of the BYOP with Managed Inference Endpoint and score it.\n",
    "\n",
    "This demonstration uses the model `summarization_quality.zip` model, a BYOP model which takes the generated text and returns a scored output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd745f20-d4b6-4c30-af41-01578a22038e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Upload Summarization LLM Listener\n",
    "\n",
    "To upload the Summarization LLM Listener, we set the input and output schema as follows.\n",
    "\n",
    "* Inputs\n",
    "  * `text` (*String*): The original inference request\n",
    "  * `generated_text` (*String*): The text returned from the Managed Inference Endpoint.\n",
    "* Outputs\n",
    "  * `generated_text` (*String*): The text returned from the Managed Inference Endpoint.\n",
    "  * `score` (*Float64*): The total score based on the generated_text field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1167e0-cc37-46aa-8a77-ebf65e3fe0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('text', pa.string()),\n",
    "    pa.field('generated_text', pa.string())\n",
    "]) \n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string()),\n",
    "    pa.field('score', pa.list_(pa.float64())),\n",
    "])\n",
    "\n",
    "summarizer_listener = wl.upload_model('summarizer_listener', \n",
    "                                      r'models/summarization_quality.zip',\n",
    "                                      framework=Framework.CUSTOM,\n",
    "                                      input_schema=input_schema,\n",
    "                                      output_schema=output_schema\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fcb884-28c2-45d9-8dd8-55bd068d5e49",
   "metadata": {},
   "source": [
    "### Deploy Managed Inference Endpoint LLM with Summarizer Score\n",
    "\n",
    "To deploy the Managed Inference Endpoint with the Summarizer Score model:\n",
    "\n",
    "1. Set the deployment configuration.  For this example, we allocate the following resources per model:\n",
    "  * `gpt`: 0.5 cpus, 1 Gi RAM.\n",
    "  * `summarizer_listener`: 2 cpus, 8 Gi RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7176583c-8533-452c-b1ac-e549e41cf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder() \\\n",
    "                    .cpus(1) \\\n",
    "                    .memory('1Gi') \\\n",
    "                    .sidekick_cpus(gpt, 0.5) \\\n",
    "                    .sidekick_memory(gpt, '1Gi') \\\n",
    "                    .sidekick_cpus(summarizer_listener, 2) \\\n",
    "                    .sidekick_memory(summarizer_listener, '8Gi') \\\n",
    "                    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fde099a-06aa-4c42-a425-c6b769c65f57",
   "metadata": {},
   "source": [
    "We then build the pipeline add both the GPT model and the summarizer listener to the pipeline.  For additional context, we'll add a [Anomaly Detection](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-observe/wallaroo-model-anomaly-detection/) that will detect any scores that are less than `0.75`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf7e628f-be61-4548-bcd0-9ad1ce21e352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 480s ..................................................................................................................................................................... ok\n"
     ]
    }
   ],
   "source": [
    "listener_pipeline = wl.build_pipeline('summarizer-listener') \\\n",
    "            .add_model_step(gpt) \\\n",
    "            .add_model_step(summarizer_listener) \\\n",
    "            .add_validations(incorrect_summary = pl.col('out.score').list.get(0) < 0.75) \\\n",
    "            .deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7421204-f7e6-42a4-9746-2b0a042a2cda",
   "metadata": {},
   "source": [
    "### Generate Inference and Score Text\n",
    "\n",
    "We now perform the same inference as before, this time with an added LLM Listener to provide the score based on the summarized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "459ced1f-baf5-4fd5-aa09-cb5273fcb5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Please summarize this text in 5 words: \n",
    "\n",
    "Simplify production AI for seamless self-checkout or cashierless experiences at scale, enabling any retail store to offer a modern shopping journey. \n",
    "We reduce the technical overhead and complexity for delivering a checkout experience that’s easy and efficient no matter where your stores are located. \n",
    "Eliminate Checkout Delays: Easy and fast model deployment for a smooth self-checkout process, allowing customers to enjoy faster, hassle-free shopping experiences. \n",
    "Drive Operational Efficiencies: Simplifying the process of scaling AI-driven self-checkout solutions to multiple retail locations ensuring uniform customer experiences no matter the location of the store while reducing in-store labor costs. \n",
    "Continuous Improvement: Enabling integrated data insights for informing self-checkout improvements across various locations, ensuring the best customer experience, regardless of where they shop.\n",
    "'''\n",
    "\n",
    "data = pd.DataFrame({\"text\": [text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3bb889a1-5e0b-4ce5-83f8-f403df8edf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.text</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>out.score</th>\n",
       "      <th>anomaly.count</th>\n",
       "      <th>anomaly.incorrect_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-07-16 18:52:35.226</td>\n",
       "      <td>Please summarize this text in 5 words: \\n\\nSim...</td>\n",
       "      <td>AI simplifies self-checkout for retailers.</td>\n",
       "      <td>[0.7120675]</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time                                            in.text  \\\n",
       "0 2024-07-16 18:52:35.226  Please summarize this text in 5 words: \\n\\nSim...   \n",
       "\n",
       "                           out.generated_text    out.score  anomaly.count  \\\n",
       "0  AI simplifies self-checkout for retailers.  [0.7120675]              1   \n",
       "\n",
       "   anomaly.incorrect_summary  \n",
       "0                       True  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = listener_pipeline.infer(data, timeout=10000)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ca8e3",
   "metadata": {},
   "source": [
    "### Undeploy the Model\n",
    "\n",
    "With the tutorial complete, we undeploy the model and return the resources back to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b1f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "listener_pipeline.undeploy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2024.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
