{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ade7e87",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.1.1_tutorials/wallaroo-llms/llm-deploy/llm-deploy-qaic/llm-deploy-qaic-llama-qaic-rag-openai).\n",
    "\n",
    "## Deploy Custom LLM using QAIC Acceleration with a MongoDB Vector Database Connection for RAG with OpenAI API Compatibility\n",
    "\n",
    "The following tutorial demonstrates deploying a Llama LLM using QAIC Acceleration with Retrieval-Augmented Generation (RAG) in Wallaroo with OpenAI API compatibility enabled.  This allows developers to:\n",
    "\n",
    "* Take advantage of Wallaroo's inference optimization to increase inference response times with more efficient resource allocation.\n",
    "* Increase the speed of LLM inferences with QAIC's AI acceleration with lower power costs.\n",
    "* Migrate existing OpenAI client code with a minimum of changes.\n",
    "* Extend their LLMs capabilities with the Wallaroo Custom Model framework to add RAG functionality to an existing LLM.\n",
    "\n",
    "Wallaroo supports OpenAI compatibility for LLMs through the following Wallaroo frameworks:\n",
    "\n",
    "* `wallaroo.framework.Framework.VLLM`:  Native async vLLM implementations.\n",
    "* `wallaroo.framework.Framework.CUSTOM`:  Wallaroo Custom Models provide greater flexibility through a lightweight Python interface.  This is typically used in the same pipeline as a native vLLM implementation to provide additional features such as Retrieval-Augmented Generation (RAG), monitoring, etc.\n",
    "\n",
    "A typical situation is to either deploy the native vLLM runtime as a single model in a Wallaroo pipeline, or both the Custom Model runtime and the native vLLM runtime together in the same pipeline to extend the LLMs capabilities.  In this tutorial, RAG is added to improve the context of inference requests to provide better responses and prevent AI hallucinations.\n",
    "\n",
    "This example uses one model for RAG, and one LLM with OpenAI compatibility enabled.\n",
    "\n",
    "![Single model LLM](./images/reference/wallaroo-llms/openai/OpenAI-multiple-models-pipeline.svg)\n",
    "\n",
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "This tutorial demonstrates how to:\n",
    "\n",
    "* Upload a LLM with the Wallaroo native vLLM framework and a Wallaroo Custom Model with the Custom Model framework, with QAIC acceleration enabled.\n",
    "* Configure the uploaded LLM to enable OpenAI API compatibility and set additional OpenAI parameters.\n",
    "* Set resource configurations for allocating cpus, memory, etc.\n",
    "* Set the Custom Model runtime and native vLLM runtime as pipeline steps and deploy in Wallaroo.\n",
    "* Submit inference request via:\n",
    "  * The Wallaroo SDK methods `completions` and `chat_completion` \n",
    "  * Wallaroo pipeline inference urls with OpenAI API endpoints extensions.\n",
    "\n",
    "### Tutorial Requirements\n",
    "\n",
    "The following tutorial requires the following:\n",
    "\n",
    "* Wallaroo version 2025.1 and above.\n",
    "* Tiny Llama model and the Wallaroo RAG Custom Model.  These are available from Wallaroo representatives upon request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4b682",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "The following libraries are used for this tutorial, primarily the Wallaroo SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30da7a77-1c18-46ac-8ff1-7108592aaf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64 \n",
    "import json\n",
    "import os\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.framework import CustomConfig, VLLMConfig\n",
    "from wallaroo.engine_config import QaicConfig\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.engine_config import Acceleration\n",
    "from wallaroo.continuous_batching_config import ContinuousBatchingConfig\n",
    "from wallaroo.openai_config import OpenaiConfig\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3c684",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is established via the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://staging.docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f98ee8-29c4-4175-b925-aa47fe42f4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd97c3d",
   "metadata": {},
   "source": [
    "### Upload the Wallaroo Native vLLM Runtime\n",
    "\n",
    "The model is uploaded with the following parameters:\n",
    "\n",
    "* The model name.\n",
    "* The file path to the model.\n",
    "* The framework set to Wallaroo native vLLM runtime:  `wallaroo.framework.Framework.VLLM`\n",
    "* The input and output schemas are defined in Apache PyArrow format.  For OpenAI compatibility, this is left as an empty List.\n",
    "* Acceleration is set to Qualcomm QAIC for the LLM.  In this example, an acceleration configuration is applied with `Acceleration.QAIC.with_config` to find tune hardware performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e346b03e-54b2-403e-8d6c-a9664292092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10min.\n",
      "\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime...................................................................................................................................................................................................................................................\n",
      "Successful\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "qaic_config = QaicConfig(\n",
    "    num_devices=4, \n",
    "    full_batch_size=16, \n",
    "    ctx_len=1024, \n",
    "    prefill_seq_len=128, \n",
    "    mxfp6_matmul=True, \n",
    "    mxint8_kv_cache=True\n",
    ")\n",
    "\n",
    "llama = wl.upload_model(\n",
    "    \"llama-qaic-openai\", \n",
    "    \"llama-31-8b.zip\", \n",
    "    framework=Framework.VLLM,\n",
    "    framework_config=VLLMConfig(\n",
    "        max_num_seqs=16,\n",
    "        max_model_len=1024,\n",
    "        max_seq_len_to_capture=128, \n",
    "        quantization=\"mxfp6\",\n",
    "        kv_cache_dtype=\"mxint8\", \n",
    "        gpu_memory_utilization=1\n",
    "    ),\n",
    "    input_schema=pa.schema([]),\n",
    "    output_schema=pa.schema([]), \n",
    "    accel=Acceleration.QAIC.with_config(qaic_config)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4dfcff-ad0e-46f4-8c45-3149f3c3f26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = wl.get_model(\"llama-qaic-openai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3d43b",
   "metadata": {},
   "source": [
    "### Enable OpenAI Compatibility and Continuous Batch Config\n",
    "\n",
    "OpenAI compatibility and continuous batch config options are enabled at the **model configuration** after the model is uploaded.\n",
    "\n",
    "OpenAI compatibility is enabled via the **model configuration** from the class `wallaroo.openai_config.OpenaiConfig` includes the following main parameters.  The essential one is `enabled` - if OpenAI compatibility is **not** enabled, all other parameters are ignored.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| `enabled` | *Boolean* (*Default: False*) | If `True`, OpenAI compatibility is enabled.  If `False`, OpenAI compatibility is not enabled.  All other parameters are ignored if `enabled=False`. |\n",
    "| `completion_config` | *Dict* | The OpenAI API [`completion`](https://platform.openai.com/docs/api-reference/completions) parameters.  All `completion` parameters are available **except** `stream`; the `stream` parameter is **only** set at inference requests. |\n",
    "| `chat_completion_config` | *Dict* | The OpenAI API [`chat/completion`](https://platform.openai.com/docs/api-reference/chat/create) parameters.  All `completion` parameters are available **except** `stream`; the `stream` parameter is **only** set at inference requests. |\n",
    "\n",
    "With the `OpenaiConfig` object defined, it is when applied to the LLM configuration through the `openai_config` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23f75de8-9b6e-4295-8634-b10a60c08ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>llama-qaic-openai</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>0c97b5ba-daac-4688-8d8e-fc1f0bcd9b9d</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy-qaic-vllm:v2025.1.0-6231</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>{'qaic': {'ctx_len': 1024, 'num_cores': 16, 'num_devices': 4, 'mxfp6_matmul': True, 'full_batch_size': 16, 'mxint8_kv_cache': True, 'prefill_seq_len': 128, 'aic_enable_depth_first': False}}</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-02-Jul 17:54:00</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>9</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'llama-qaic-openai', 'version': '0c97b5ba-daac-4688-8d8e-fc1f0bcd9b9d', 'file_name': 'llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy-qaic-vllm:v2025.1.0-6231', 'arch': 'x86', 'accel': {'qaic': {'ctx_len': 1024, 'num_cores': 16, 'num_devices': 4, 'mxfp6_matmul': True, 'full_batch_size': 16, 'mxint8_kv_cache': True, 'prefill_seq_len': 128, 'aic_enable_depth_first': False}}, 'last_update_time': datetime.datetime(2025, 7, 2, 17, 54, 0, 554706, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbc = ContinuousBatchingConfig(max_concurrent_batch_size = 100)\n",
    "\n",
    "openai_config = OpenaiConfig(\n",
    "    enabled=True,\n",
    "    completion_config={\n",
    "        \"temperature\": .3,\n",
    "        \"max_tokens\": 200\n",
    "    },\n",
    "    chat_completion_config={\n",
    "        \"temperature\": .3,\n",
    "        \"max_tokens\": 200,\n",
    "        \"chat_template\": \"\"\"\n",
    "        {% for message in messages %}\n",
    "            {% if message['role'] == 'user' %}\n",
    "                {{ '<|user|>\\n' + message['content'] + eos_token }}\n",
    "            {% elif message['role'] == 'system' %}\n",
    "                {{ '<|system|>\\n' + message['content'] + eos_token }}\n",
    "            {% elif message['role'] == 'assistant' %}\n",
    "                {{ '<|assistant|>\\n'  + message['content'] + eos_token }}\n",
    "            {% endif %}\n",
    "            \n",
    "            {% if loop.last and add_generation_prompt %}\n",
    "                {{ '<|assistant|>' }}\n",
    "            {% endif %}\n",
    "        {% endfor %}\"\"\"\n",
    "    })\n",
    "llama = llama.configure(continuous_batching_config=cbc,\n",
    "                        openai_config=openai_config)\n",
    "llama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3050c",
   "metadata": {},
   "source": [
    "### Upload Embedder Model\n",
    "\n",
    "The RAG embedder model is uploaded with the Wallaroo Custom Model framework.  This allows for flexibility with Python scripts to handle requesting the context from the Mongo database through a vector query.  Once uploaded, the configuration is updated to include OpenAI compatibility.\n",
    "\n",
    "<details>\n",
    "<summary><h4>Custom Model Framework</h4></summary>\n",
    "\n",
    "The embedder model includes the following artifacts:\n",
    "\n",
    "* `requirements.txt`: Sets what Python libraries are used.\n",
    "* `{python script}.py`:  Any python script that extends the Wallaroo classes for Custom frameworks.  For more details, see [Wallaroo SDK Essentials Guide: Model Uploads and Registrations: Custom Model](https://staging.docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "\n",
    "In this example, the `requirements.txt` file is:\n",
    "\n",
    "```python\n",
    "sentence_transformers==4.1.0\n",
    "pymongo==4.7.1\n",
    "```\n",
    "\n",
    "The script for our `openai_step.py` file is as follows:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pymongo\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en\")  # runs on CPU by default\n",
    "client = pymongo.MongoClient(\"mongodb+srv://wallaroo_user:random123@example.wallaroo.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\")\n",
    "db = client.sample_mflix\n",
    "collection = db.movies\n",
    "\n",
    "def lookup_context(text: str):\n",
    "    embedding = model.encode(\n",
    "        text,\n",
    "        normalize_embeddings=True,\n",
    "        convert_to_numpy=True,\n",
    "    ).tolist()\n",
    "\n",
    "    query_results = collection.aggregate(\n",
    "            [\n",
    "                {\n",
    "                    \"$vectorSearch\": {\n",
    "                        \"queryVector\": embedding,\n",
    "                        \"path\": \"plot_embedding_hf\",\n",
    "                        \"numCandidates\": 50,\n",
    "                        \"limit\": 10,\n",
    "                        \"index\": \"PlotSemanticSearch\",\n",
    "                    }\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    context = \" \".join([result[\"plot\"] for result in query_results])\n",
    "    return context[:100]\n",
    "\n",
    "\n",
    "def handle_chat_completion(request: dict) -> dict:\n",
    "    messages = request[\"messages\"]\n",
    "    \n",
    "    # Extract last 3 user messages\n",
    "    user_text = \"\\n\".join([m[\"content\"] for m in messages if m.get(\"role\") == \"user\"][-3:])\n",
    "    context = lookup_context(user_text)\n",
    "    \n",
    "    # Inject as system message at the top\n",
    "    context_msg = {\"role\": \"system\", \"content\": f\"Context: {context}\"}\n",
    "    request[\"messages\"] = [context_msg] + messages\n",
    "    return request\n",
    "\n",
    "\n",
    "def handle_completion(request: dict) -> dict:\n",
    "    prompt = request.get(\"prompt\", \"\")\n",
    "    context = lookup_context(prompt)\n",
    "    request[\"prompt\"] = f\"Context: {context}\\n\\n{prompt}\"\n",
    "    return request\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8dab23a-5113-43da-a186-cfc77fdc016c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10min.\n",
      "\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime...........\n",
      "Successful\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "# Uploading the model\n",
    "\n",
    "rag_step = wl.upload_model(\n",
    "    \"ragstep\",\n",
    "    \"rag_step.zip\",\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=pa.schema([]),\n",
    "    output_schema=pa.schema([]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "895aabad-9315-460e-ac18-7e16cae71a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_step = wl.get_model(\"ragstep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27f6c8b3-c09e-4e97-856e-cb7011fa7cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>ragstep</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>161d2b87-ffa4-4bbf-b5ce-036e5dcd1db4</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>rag_step.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>5d47b8229b4410b63eb52af11f91a6c6e45eaa681e765624be2adec339f427a3</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-6231</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>none</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-02-Jul 18:01:55</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>9</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'ragstep', 'version': '161d2b87-ffa4-4bbf-b5ce-036e5dcd1db4', 'file_name': 'rag_step.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-6231', 'arch': 'x86', 'accel': 'none', 'last_update_time': datetime.datetime(2025, 7, 2, 18, 1, 55, 791319, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_config_rag = OpenaiConfig(enabled=True)\n",
    "rag_step = rag_step.configure(openai_config=openai_config_rag)\n",
    "rag_step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468f3227",
   "metadata": {},
   "source": [
    "### Set the Deployment Configuration and Deploy\n",
    "\n",
    "The **deployment configuration** defines what resources are allocated to the LLM's exclusive use.  For this tutorial, the LLM is allocated:\n",
    "\n",
    "* Llama LLM:\n",
    "  * 4 cpus\n",
    "  * 12 Gi RAM\n",
    "  * 4 GPU.  The GPU type is inherited from the model upload step.  For QAIC, each deployment configuration `gpu` values is the number of **System-on-Chip (SoC)** to use.\n",
    "* RAG Model:\n",
    "  * 1 cpu\n",
    "  * 2 Gi RAM\n",
    "\n",
    "Once the deployment configuration is set:\n",
    "\n",
    "* The pipeline is created.\n",
    "* The RAG model and the LLM added as a **pipeline steps**.\n",
    "* The pipeline is deployed with the deployment configuration.\n",
    "\n",
    "Once the deployment is complete, the LLM is ready to receive inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ff66b24-6a61-4cac-bde0-b2aa87ec9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('1Gi') \\\n",
    "    .sidekick_cpus(rag_step, 1) \\\n",
    "    .sidekick_memory(rag_step, '2Gi') \\\n",
    "    .sidekick_cpus(llama, 4) \\\n",
    "    .sidekick_memory(llama, '12Gi') \\\n",
    "    .sidekick_gpus(llama, 4) \\\n",
    "    .deployment_label(\"kubernetes.io/os:linux\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905efcee-7544-493e-b1bc-ce72be9d2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline('llama-openai-ragyns')\n",
    "pipeline.undeploy()\n",
    "pipeline.clear()\n",
    "pipeline.add_model_step(rag_step)\n",
    "pipeline.add_model_step(llama)\n",
    "pipeline.deploy(deployment_config = deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d57750f-b49e-46ef-a197-7f0541e421b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.244.69.143',\n",
       "   'name': 'engine-7fb7bcb47d-ssfxj',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'llama-openai-ragyns',\n",
       "      'status': 'Running',\n",
       "      'version': '1c8d179a-8dea-4e7b-8ffd-5d57b1707d6c'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 113,\n",
       "      'name': 'llama-qaic-openai',\n",
       "      'sha': '62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838',\n",
       "      'status': 'Running',\n",
       "      'version': '0c97b5ba-daac-4688-8d8e-fc1f0bcd9b9d'},\n",
       "     {'model_version_id': 114,\n",
       "      'name': 'ragstep',\n",
       "      'sha': '5d47b8229b4410b63eb52af11f91a6c6e45eaa681e765624be2adec339f427a3',\n",
       "      'status': 'Running',\n",
       "      'version': '161d2b87-ffa4-4bbf-b5ce-036e5dcd1db4'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.244.69.152',\n",
       "   'name': 'engine-lb-7765599d45-9k6f9',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.244.69.130',\n",
       "   'name': 'engine-sidekick-ragstep-114-6b7456f84c-wv2gq',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'},\n",
       "  {'ip': '10.244.69.132',\n",
       "   'name': 'engine-sidekick-llama-qaic-openai-113-5d9945ffd8-9jlfc',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b18e9d",
   "metadata": {},
   "source": [
    "### Inference Requests on LLM with OpenAI Compatibility Enabled\n",
    "\n",
    "Inference requests on Wallaroo pipelines deployed with native vLLM runtimes or Wallaroo Custom with OpenAI compatibility enabled in Wallaroo are performed either through the Wallaroo SDK, or via OpenAPI endpoint requests.\n",
    "\n",
    "OpenAI API inference requests on models deployed with OpenAI compatibility enabled have the following conditions:\n",
    "\n",
    "* Parameters for `chat/completion` and `completion` **override** the existing OpenAI configuration options.\n",
    "* If the `stream` option is enabled:\n",
    "  * Outputs returned as list of chunks aka as an event stream.\n",
    "  * The request inference call completes when all chunks are returned.\n",
    "  * The response metadata includes `ttft`, `tps` and user-specified OpenAI request params **after** the last chunk is generated.\n",
    "\n",
    "#### OpenAI API Inference Requests via the Wallaroo SDK and Inference Result Logs\n",
    "\n",
    "Inference requests with OpenAI compatible enabled models in Wallaroo via the Wallaroo SDK use the following methods:\n",
    "\n",
    "* `wallaroo.pipeline.Pipeline.openai_chat_completion`:  Submits an inference request using the OpenAI API `chat/completion` endpoint parameters.\n",
    "* `wallaroo.pipeline.Pipeline.openai_completion`: Submits an inference request using the OpenAI API `completion` endpoint parameters.\n",
    "\n",
    "The OpenAI metrics are provided as part of the pipeline inference logs and include the following values:\n",
    "\n",
    "* `ttft`\n",
    "* `tps`\n",
    "* The OpenAI request parameter values set during the inference request.\n",
    "\n",
    "The method `wallaroo.pipeline.Pipeline.logs` returns a pandas DataFrame by default, with the output fields labeled `out.{field}`.  For OpenAI inference requests, the OpenAI metrics output field is `out.json`.  The following demonstrates retrieving the inference results log and displaying the `out.json` field, which includes the `tps` and `ttft` fields.\n",
    "\n",
    "#### OpenAI API Inference Requests via Pipeline Deployment URLs with OpenAI Extensions\n",
    "\n",
    "Native vLLM runtimes and Wallaroo Custom Models with OpenAI enabled perform inference requests via the OpenAI API Client use the pipeline's **deployment inference endpoint** with the OpenAI API endpoints extensions.  For deployments with OpenAI compatibility enabled, the following additional endpoints are provided:\n",
    "\n",
    "* `{Deployment inference endpoint}/openai/v1/completions`:  Compatible with the OpenAI API endpoint `completion`.\n",
    "* `{Deployment inference endpoint}/openai/v1/chat/completions`:  Compatible with the OpenAI API endpoint `chat/completion`.\n",
    "\n",
    "These requests require the following:\n",
    "\n",
    "* A Wallaroo pipeline deployed with Wallaroo native vLLM runtime or Wallaroo Custom Models with OpenAI compatibility enabled.\n",
    "* Authentication to the Wallaroo MLOps API.  For more details, see the [Wallaroo API Connection Guide]({{<ref \"wallaroo-mlops-connection-guide\">}}).\n",
    "* Access to the deployed pipeline's OpenAPI API endpoints.\n",
    "\n",
    "\n",
    "#### Inference and Inference Results Logs Examples\n",
    "\n",
    "The following demonstrates performing an inference request using `openai_chat_completion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41fde315-1c45-4ebd-8fbb-ad48a1e4bc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The closest movie title to \"good morning\" is likely \"Good Morning, Vietnam\" (1987), a comedy-drama film directed by Barry Levinson and starring Robin Williams.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.openai_chat_completion(messages=[{\"role\": \"user\", \"content\": \"closest movie title to good morning\"}]).choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04951a3b",
   "metadata": {},
   "source": [
    "The following demonstrates performing an inference request using `openai_chat_completion` with token streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ceca0ab3-31ea-4d38-b9f9-fcf45fe28369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " that fits this description.\n",
      "\n",
      "## Step 1: Identify key elements of the movie description\n",
      "The movie is told in flashbacks, it's about an older man's obsession for a woman who can belong to no-one, and it's from the 1990's.\n",
      "\n",
      "## Step 2: Consider movies from the 1990's that fit the description\n",
      "One movie that fits this description is \"The English Patient\" (1996), but it's not the only one. Another movie that fits is \"The Piano\" (1993), but it's not the one I'm thinking of.\n",
      "\n",
      "## Step 3: Think of another movie that fits the description\n",
      "A movie that fits the description is \"The Piano\" is not it, but \"The Piano\" is a good guess, another movie that fits is \"The English Patient\" is not it, but \"The English Patient\" is a good guess, but the movie I'm thinking of is \"The Piano\" is not it, but \"The English Patient\" is not it, but \"The Piano\" is a good guess, but the movie I'm thinking of is \"The English Patient\" is not it, but I think I have it.\n",
      "\n",
      "## Step 4: Identify the movie\n",
      "The movie I'm thinking of is \"The Piano\" is not it, but I think I have it, the movie is \"The English Patient\" is not it, but I think I have it, the movie is \"The Piano\" is not it, but I think I have it, the movie is \"The English Patient\" is not it, but I think I have it, the movie is \"The Piano\" is not it, but I think I have it, the movie is \"The English Patient\" is not it, but I think I have it, the movie is \"The Piano\" is not it, but I think I have it, the movie is \"The English Patient\" is not it, but I think I have it, the movie is \"The Piano\" is not it, but I think I have it, the movie is \"The English Patient\" is not it, but I think I have it, the movie is \"The Piano\" is not it, but I think I have it, the movie is \"The English Patient-shift\" no, I have it, the movie is \"The Piano\" is not it, but I think I have it, the movie is \"The English Patient"
     ]
    }
   ],
   "source": [
    "for chunk in pipeline.openai_completion(prompt=\"Give me the title of a good movie from the 1990's\", max_tokens=500, stream=True):\n",
    "    print(chunk.choices[0].text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc3085",
   "metadata": {},
   "source": [
    "The following demonstrates using the pipeline infernence url with the OpenAI extension endpoints for `completions`.  First the authentication token is retrieved, then the inference request made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04599a5a-4836-4f4c-b71b-9ce663e88c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc123'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = wl.auth.auth_header()['Authorization'].split()[1]\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bc99f46-6311-4de9-80ba-b9cbd7d037e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: {\"id\":\"cmpl-12e8fff796b44a47a16fb74eff83e468\",\"created\":1751480108,\"model\":\"llama-31-8b.zip\",\"choices\":[{\"text\":\",\",\"index\":0,\"logprobs\":null,\"finish_reason\":null,\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":42,\"completion_tokens\":1,\"total_tokens\":43,\"ttft\":0.10214721,\"tps\":9.789792594433074}}\n",
      "\n",
      "data: {\"id\":\"cmpl-12e8fff796b44a47a16fb74eff83e468\",\"created\":1751480108,\"model\":\"llama-31-8b.zip\",\"choices\":[{\"text\":\" and\",\"index\":0,\"logprobs\":null,\"finish_reason\":null,\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":42,\"completion_tokens\":2,\"total_tokens\":44,\"ttft\":0.10214721,\"tps\":14.570481440179023}}\n",
      "\n",
      "data: {\"id\":\"cmpl-12e8fff796b44a47a16fb74eff83e468\",\"created\":1751480108,\"model\":\"llama-31-8b.zip\",\"choices\":[{\"text\":\" I\",\"index\":0,\"logprobs\":null,\"finish_reason\":null,\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":42,\"completion_tokens\":3,\"total_tokens\":45,\"ttft\":0.10214721,\"tps\":13.070685403999592}}\n",
      "\n",
      "...\n",
      "\n",
      "data: {\"id\":\"cmpl-12e8fff796b44a47a16fb74eff83e468\",\"created\":1751480108,\"model\":\"llama-31-8b.zip\",\"choices\":[{\"text\":\" from\",\"index\":0,\"logprobs\":null,\"finish_reason\":null,\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":42,\"completion_tokens\":99,\"total_tokens\":141,\"ttft\":0.10214721,\"tps\":9.90478908735264}}\n",
      "\n",
      "data: {\"id\":\"cmpl-12e8fff796b44a47a16fb74eff83e468\",\"created\":1751480108,\"model\":\"llama-31-8b.zip\",\"choices\":[{\"text\":\" the\",\"index\":0,\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":42,\"completion_tokens\":100,\"total_tokens\":142,\"ttft\":0.10214721,\"tps\":9.905616236577528}}\n",
      "\n",
      "data: {\"id\":\"cmpl-12e8fff796b44a47a16fb74eff83e468\",\"created\":1751480108,\"model\":\"llama-31-8b.zip\",\"choices\":[],\"usage\":{\"prompt_tokens\":42,\"completion_tokens\":100,\"total_tokens\":142,\"ttft\":0.10214721,\"tps\":9.905460621690295}}\n",
      "\n",
      "data: [DONE]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streaming: Completion\n",
    "!curl -X POST \\\n",
    "  -H \"Authorization: Bearer abc123\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"model\": \"\", \"prompt\": \"Give me the title of a good movie from the 1990s\", \"max_tokens\": 100, \"stream\": true, \"stream_options\": {\"include_usage\": true}}' \\\n",
    "  https://example.wallaroo.ai/v1/api/pipelines/infer/llama-openai-ragyns-63/llama-openai-ragyns/openai/v1/completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e761a",
   "metadata": {},
   "source": [
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
