{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d2f4fa",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.1.1_tutorials/wallaroo-llms/llm-deploy/llm-deploy-qaic/llm-deploy-qaic-llama-qaic-async).\n",
    "\n",
    "## Deploy Llama with Continuous Batching Using Native vLLM Framework and QAIC AI Acceleration\n",
    "\n",
    "The following tutorial demonstrates deploying the Llama LLM with the following enhancements:\n",
    "\n",
    "* The Wallaroo Native vLLM Framework: Provide performance optimizations with framework configuration options.\n",
    "* Continuous Batching: Configurable batch sizes balance latency and throughput use.\n",
    "* QAIC AI Acceleration:  x86 compatible architecture at low power with AI acceleration.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM deployment with QAIC acceleration, continuous batching, and other features:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "### Tutorial Goals\n",
    "\n",
    "This tutorial demonstrates the following procedure:\n",
    "\n",
    "* Upload a Llama LLM with:\n",
    "  * The Wallaroo Native vLLM runtime\n",
    "  * QAIC AI Acceleration enabled\n",
    "  * Framework configuration options to enhance performance\n",
    "* Configure continuous batching as a model configuration option.\n",
    "* Set a deployment configuration to allocate hardware resources and deploy the LLM.\n",
    "* Perform sample inferences and show both the inference results and the inference result logs.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Wallaroo 2025.1 and above.\n",
    "* A cluster with [Qualcomm Cloud AI](https://www.qualcomm.com/products/technology/processors/cloud-artificial-intelligence) hardware.\n",
    "\n",
    "## Tutorial Steps\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the Python libraries required, mainly the Wallaroo SDK.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42912a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Acceleration\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.engine_config import QaicConfig\n",
    "from wallaroo.framework import VLLMConfig\n",
    "import pyarrow as pa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5eeb5",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "Next connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a073475",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e60d6",
   "metadata": {},
   "source": [
    "### LLM Upload\n",
    "\n",
    "Uploading the LLM takes the following steps:\n",
    "\n",
    "* Define Schemas:  The input and output schemas are defined in Apache PyArrow format.  For this tutorial, they are converted to base64 strings used for uploading through the Wallaroo MLOps API.\n",
    "* Upload the model via either the Wallaroo SDK or the Wallaroo MLOps API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aca7b37",
   "metadata": {},
   "source": [
    "#### Define Schemas\n",
    "\n",
    "The schemas are defined in Apache PyArrow format for the inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9ea3d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('prompt', pa.string()),\n",
    "    pa.field('max_tokens', pa.int64()),\n",
    "])\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string()),\n",
    "    pa.field('num_output_tokens', pa.int64())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d956fb7c",
   "metadata": {},
   "source": [
    "Each is then converted to base64 strings that are later used for uploading via the Wallaroo MLops API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b10746",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64.b64encode(\n",
    "                bytes(input_schema.serialize())\n",
    "            ).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64.b64encode(\n",
    "                bytes(output_schema.serialize())\n",
    "            ).decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572af4eb",
   "metadata": {},
   "source": [
    "#### Upload LLM\n",
    "\n",
    "LLM uploads to Wallaroo are either via the Wallaroo SDK or the Wallaroo MLOps API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a352c9",
   "metadata": {},
   "source": [
    "The following demonstrates uploading the LLM via the SDK.  In this example the QAIC acceleration configuration is defined.  This is an **optional** step that fine tunes the QAIC AI Acceleration hardware performance to best fit the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "911786c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qaic_config = QaicConfig(\n",
    "    num_devices=4, \n",
    "    full_batch_size=16, \n",
    "    ctx_len=256, \n",
    "    prefill_seq_len=128, \n",
    "    mxfp6_matmul=True, \n",
    "    mxint8_kv_cache=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb969af",
   "metadata": {},
   "source": [
    "LLMs are uploaded with the Wallaroo SDK method `wallaroo.client.Client.upload_model`.  This this step, the following options are configured:\n",
    "\n",
    "* The model name and file path.\n",
    "* The framework, in this case the native vLLM runtime.\n",
    "* The optional framework configuration, which sets specific options for the LLM's performance.\n",
    "* The input and output schemas.\n",
    "* The hardware acceleration set to `wallaroo.engine_config.Acceleration.QAIC.with_config`.  The addition `with_config` accepts the hardware configuration options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d31eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10min.\n",
      "\n",
      "Model is pending loading to a container runtime..\n",
      "Model is attempting loading to a container runtime......................................................................................................................................................................................................................................\n",
      "Successful\n",
      "Ready\n"
     ]
    }
   ],
   "source": [
    "llm = wl.upload_model(\n",
    "    \"llama-31-8b-qaic\", \n",
    "    \"llama-31-8b.zip\", \n",
    "    framework=Framework.VLLM,\n",
    "    framework_config=VLLMConfig(\n",
    "        max_num_seqs=16,\n",
    "        max_model_len=256,\n",
    "        max_seq_len_to_capture=128, \n",
    "        quantization=\"mxfp6\",\n",
    "        kv_cache_dtype=\"mxint8\", \n",
    "        gpu_memory_utilization=1\n",
    "    ),\n",
    "    input_schema=input_schema, \n",
    "    output_schema=output_schema, \n",
    "    accel=Acceleration.QAIC.with_config(qaic_config)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3d0e6",
   "metadata": {},
   "source": [
    "The other upload option is the Wallaroo MLOps API endpoint `v1/api/models/upload_and_convert`.  For this option, the base64 converted input and output schemas are used, and the `framework_config` and `accel` options are specified in `dict` format.  Otherwise, the same parameters are set:\n",
    "\n",
    "* The model name and file path.\n",
    "* The `conversion` parameter which defines:\n",
    "  * The framework as native vLLM\n",
    "  * The optional framework configuration, which sets specific options for the LLM's performance.\n",
    "* The input and output schemas set as base64 strings.\n",
    "* the `accel` parameter which specifies the AI accelerator as `qaic` with the additional hardware configuration options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9acbe26",
   "metadata": {},
   "source": [
    "```bash\n",
    "curl --progress-bar -X POST \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -H \"Authorization: Bearer <your-token-here>\" \\\n",
    "  -F 'metadata={\"name\": \"vllm-llama-31-8b-qaic-new-v1\", \"visibility\": \"private\", \"workspace_id\": 6, \"conversion\": {\"framework\": \"vllm\", \"framework_config\": {\"framework\": \"vllm\", \"config\":{\"max_num_seqs\": 16, \"max_model_len\": 256, \"max_seq_len_to_capture\": 128, \"quantization\": \"mxfp6\", \"kv_cache_dtype\": \"mxint8\", \"gpu_memory_utilization\": 1}}, \"accel\": {\"qaic\":{\"num_devices\":4,\"full_batch_size\": 16, \"ctx_len\": 256, \"prefill_seq_len\": 128, \"mxfp6_matmul\":true,\"mxint8_kv_cache\":true}}, \"python_version\": \"3.8\", \"requirements\": []}, \"input_schema\": \"/////7AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAIAAABUAAAABAAAAMT///8AAAECEAAAACQAAAAEAAAAAAAAAAoAAABtYXhfdG9rZW5zAAAIAAwACAAHAAgAAAAAAAABQAAAABAAFAAIAAYABwAMAAAAEAAQAAAAAAABBRAAAAAcAAAABAAAAAAAAAAGAAAAcHJvbXB0AAAEAAQABAAAAA==\", \"output_schema\": \"/////8AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAIAAABcAAAABAAAALz///8AAAECEAAAACwAAAAEAAAAAAAAABEAAABudW1fb3V0cHV0X3Rva2VucwAAAAgADAAIAAcACAAAAAAAAAFAAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA=\"};type=application/json' \\\n",
    "  -F \"file=@llama-31-8b.zip;type=application/octet-stream\" \\\n",
    "  https://qaic-poc.pov.wallaroo.io/v1/api/models/upload_and_convert | cat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454cb57d",
   "metadata": {},
   "source": [
    "When the llm is uploaded, we retrieve it via the `wallaroo.client.Client.get_model` for use in later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639c1bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>llama-31-8b-qaic</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>0600dc44-c530-4425-a29d-9754406b0bb2</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy-qaic-vllm:v2025.1.0-6196</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>{'qaic': {'ctx_len': 256, 'num_cores': 16, 'num_devices': 4, 'mxfp6_matmul': True, 'full_batch_size': 16, 'mxint8_kv_cache': True, 'prefill_seq_len': 128, 'aic_enable_depth_first': False}}</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-12-Jun 17:46:32</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>9</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'llama-31-8b-qaic', 'version': '0600dc44-c530-4425-a29d-9754406b0bb2', 'file_name': 'llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy-qaic-vllm:v2025.1.0-6196', 'arch': 'x86', 'accel': {'qaic': {'ctx_len': 256, 'num_cores': 16, 'num_devices': 4, 'mxfp6_matmul': True, 'full_batch_size': 16, 'mxint8_kv_cache': True, 'prefill_seq_len': 128, 'aic_enable_depth_first': False}}, 'last_update_time': datetime.datetime(2025, 6, 12, 17, 46, 32, 901022, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = wl.get_model(\"llama-31-8b-qaic\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbedf645",
   "metadata": {},
   "source": [
    "### Configure Continuous Batching\n",
    "\n",
    "Continuous batching options are applied for the model configuration with the `model.Model.configure` parameter.  This method required both the input and output schemas, and the `wallaroo.continuous_batching_config.ContinuousBatchingConfig` settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94feb0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wallaroo.continuous_batching_config import ContinuousBatchingConfig\n",
    "cbc = ContinuousBatchingConfig(max_concurrent_batch_size = 100)\n",
    "\n",
    "llm = llm.configure(input_schema=input_schema,output_schema=output_schema,continuous_batching_config = cbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ed833",
   "metadata": {},
   "source": [
    "### Deploy the LLM\n",
    "\n",
    "Deploying the LLM takes the following steps:\n",
    "\n",
    "* Create or retrieve the Wallaroo pipeline.\n",
    "* Set the deployment configuration.\n",
    "* Deploy the LLM with the deployment configuration.\n",
    "\n",
    "#### Create or Retrieve the Pipeline\n",
    "\n",
    "The following helper method is provided to either **retrieve an existing pipeline** or **create a new one**.  In either case, the Wallaroo pipeline is used to deploy the LLM and set other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59139ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pipeline(name):\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(name)\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5f431a",
   "metadata": {},
   "source": [
    "#### Set the Deployment Configuration\n",
    "\n",
    "The deployment configuration determines what hardware resources allocated for the LLMs exclusive use.  The LLM options are set via the `sidekick` options.\n",
    "\n",
    "For this example, the deployment hardware includes a Qualcomm AI 100 and allocates the following resources:\n",
    "\n",
    "* Replicas:  1 minimum, maximum 2.  This provides scalability with additional replicas scaled up or down automatically based on resource usage. \n",
    "* Cpus: 4 \n",
    "* RAM:  12 Gi\n",
    "* gpus: 4\n",
    "  * For Wallaroo deployment configurations for QAIC, the `gpu` parameter specifies the number of System-on-Chips (SoCs) allocated.\n",
    "* Deployment label:  Specifies the node with the gpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab19c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sidekick_gpus is the number Qualcomm AI 100 SOCs \n",
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .replica_autoscale_min_max(minimum=1, maximum=2) \\\n",
    "    .cpus(1).memory('1Gi') \\\n",
    "    .sidekick_cpus(llm, 4) \\\n",
    "    .sidekick_memory(llm, '12Gi') \\\n",
    "    .sidekick_gpus(llm, 4) \\\n",
    "    .deployment_label(\"kubernetes.io/os:linux\") \\\n",
    "    .scale_up_queue_depth(5) \\\n",
    "    .autoscaling_window(600) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32f473a",
   "metadata": {},
   "source": [
    "The LLm is applied to a Wallaroo pipeline as a pipeline step.  Once set, the pipeline is deployed with the deployment configuration.  When the deployment is complete, the LLM is ready for inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa848a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = get_pipeline(\"llama-31-qaic-yns1\")\n",
    "pipeline.clear()\n",
    "pipeline.undeploy()\n",
    "pipeline.add_model_step(llm)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "37b056b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.244.69.157',\n",
       "   'name': 'engine-f4bf767cd-hgffn',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'llama-31-qaic-yns1',\n",
       "      'status': 'Running',\n",
       "      'version': 'bf637d55-0eca-4448-8417-8cf78570dc29'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 62,\n",
       "      'name': 'llama-31-8b-qaic',\n",
       "      'sha': '62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838',\n",
       "      'status': 'Running',\n",
       "      'version': '0600dc44-c530-4425-a29d-9754406b0bb2'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.244.69.177',\n",
       "   'name': 'engine-lb-664c6d8455-zfb4b',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.244.69.160',\n",
       "   'name': 'engine-sidekick-llama-31-8b-qaic-62-5df4569fd5-nlhpm',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e926f1",
   "metadata": {},
   "source": [
    "### Inference Examples\n",
    "\n",
    "LLMs deployed in Wallaroo accept pandas DataFrames as inference inputs.  This is submitted to the pipeline with the `infer` method, and the results are received as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "60c573f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>max_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Wallaroo.AI?</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 prompt  max_tokens\n",
       "0  What is Wallaroo.AI?         128"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"prompt\": [\"What is Wallaroo.AI?\"], \"max_tokens\": [128]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bf898bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.max_tokens</th>\n",
       "      <th>in.prompt</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>out.num_output_tokens</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-12 18:33:45.902</td>\n",
       "      <td>128</td>\n",
       "      <td>What is Wallaroo.AI?</td>\n",
       "      <td>\\nWallaroo.AI is a high-performance, scalable...</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  in.max_tokens             in.prompt  \\\n",
       "0 2025-06-12 18:33:45.902            128  What is Wallaroo.AI?   \n",
       "\n",
       "                                  out.generated_text  out.num_output_tokens  \\\n",
       "0   \\nWallaroo.AI is a high-performance, scalable...                    128   \n",
       "\n",
       "   anomaly.count  \n",
       "0              0  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.infer(df, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3652b532",
   "metadata": {},
   "source": [
    "The pipeline `logs` method returns a pandas DataFrame showing the inputs and outputs of the inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3bab4434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.max_tokens</th>\n",
       "      <th>in.prompt</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>out.num_output_tokens</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-12 18:33:45.902</td>\n",
       "      <td>128</td>\n",
       "      <td>What is Wallaroo.AI?</td>\n",
       "      <td>\\nWallaroo.AI is a high-performance, scalable...</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  in.max_tokens             in.prompt  \\\n",
       "0 2025-06-12 18:33:45.902            128  What is Wallaroo.AI?   \n",
       "\n",
       "                                  out.generated_text  out.num_output_tokens  \\\n",
       "0   \\nWallaroo.AI is a high-performance, scalable...                    128   \n",
       "\n",
       "   anomaly.count  \n",
       "0              0  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97be021",
   "metadata": {},
   "source": [
    "For access to these sample models and for a demonstration of how to use a LLM deployment with QAIC acceleration, continuous batching, and other features:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
