{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21801fbf-3298-4608-9187-0dc8c8ae025d",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.1.1_tutorials/wallaroo-llms/llm-deploy/llm-deploy-qaic/llm-deploy-qaic-llama-qaic-rag).\n",
    "\n",
    "## Deploy Custom LLM using QAIC Acceleration with a MongoDB Vector Database Connection for RAG \n",
    "\n",
    "The following tutorial demonstrates deploying a Llama LLM with Retrieval-Augmented Generation (RAG) in Wallaroo's Custom vLLM Framework with Qualcomm QAIC acceleration.  This allows developers to:\n",
    "\n",
    "* Leverage QAIC's x86 compatibility with low energy requirements, with AI hardware acceleration.\n",
    "* Deploy with Wallaroo's resource management and enhanced inference response times.\n",
    "\n",
    "Wallaroo supports QAIC compatibility for LLMs through the following Wallaroo frameworks:\n",
    "\n",
    "* `wallaroo.framework.Framework.VLLM`:  Native async vLLM implementations.\n",
    "* `wallaroo.framework.Framework.CUSTOM`:  Wallaroo Custom Models provide greater flexibility through a lightweight Python interface.  This is typically used in the same pipeline as a native vLLM implementation to provide additional features such as Retrieval-Augmented Generation (RAG), monitoring, etc.\n",
    "\n",
    "This example deploys two models:\n",
    "\n",
    "* An embedder model that accepts the prompt and provides a vector for querying a **vector indexed database** - in this case MongoDB.\n",
    "* A LLM that accepts the prompts and vector, queries the database and uses the returned values as context for the response.\n",
    "\n",
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "This tutorial demonstrates how to:\n",
    "\n",
    "* Upload two LLMs:\n",
    "  * An **embedding model** that accepts a prompt, then returns the embedding parameters with the original prompt.\n",
    "  * A Llama 31 8B LLM using the Wallaroo Custom vLLM framework accepts the embedding and prompt.  Using the embedding, it requests the context from a database, then uses that narrowed context to generate the appropriate response.\n",
    "* Configure the uploaded LLM to enable continuous batching.  This provides increased LLM performance on GPUs, leveraging configurable concurrent batch sizes at the Wallaroo inference serving layer.\n",
    "* Set resource configurations for allocating cpus, memory, gpus, etc.\n",
    "* Set the Custom Model runtime and native vLLM runtime as pipeline steps and deploy in Wallaroo.\n",
    "* Submit inference request via:\n",
    "  * The Wallaroo SDK\n",
    "  * API requests on the Wallaroo pipeline inference url\n",
    "\n",
    "### Tutorial Requirements\n",
    "\n",
    "The following tutorial requires the following:\n",
    "\n",
    "* Wallaroo version 2025.1 and above.\n",
    "* The embedding LLM and LLM in the Wallaroo Custom vLLM Framework.  These are available from Wallaroo representatives upon request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848c85d",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "The following libraries are used for this tutorial, primarily the Wallaroo SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e97224c-27a1-43f7-9997-8908d23cfffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64 \n",
    "import json\n",
    "import os\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.framework import CustomConfig, VLLMConfig\n",
    "from wallaroo.engine_config import QaicConfig\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.engine_config import Acceleration\n",
    "from wallaroo.continuous_batching_config import ContinuousBatchingConfig\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f48c0f",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is established via the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec5db62-63d9-4a40-9900-0da9e0f12cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ee002",
   "metadata": {},
   "source": [
    "### Upload the Embedding LLM and the Custom Model\n",
    "\n",
    "The model is uploaded with the following parameters:\n",
    "\n",
    "* The model name\n",
    "* The file path to the model\n",
    "* The framework set to Wallaroo Custom framework:  `wallaroo.framework.Framework.CUSTOM`\n",
    "* The input and output schemas are defined in Apache PyArrow format.\n",
    "* Acceleration is set to Qualcomm QAIC for the LLM.\n",
    "\n",
    "#### Upload the Embedder Model\n",
    "\n",
    "The embedder model is uploaded first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20124240-80d1-4ccc-a182-762acaf069f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('prompt', pa.string()),\n",
    "    pa.field('max_length', pa.int64())\n",
    "])\n",
    "output_schema = pa.schema([\n",
    "    pa.field('embedding',\n",
    "        pa.list_(\n",
    "            pa.float32(), list_size=768\n",
    "        ),\n",
    "    ),\n",
    "    pa.field('prompt', pa.string()),\n",
    "    pa.field('max_length', pa.int64())\n",
    "])\n",
    "\n",
    "bge = wl.upload_model('bge-base-pipe-llama', \n",
    "    'models/bge_base_pipe_llama31.zip',\n",
    "    framework=Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eae820",
   "metadata": {},
   "source": [
    "The Custom vLLM Framework runtime is uploaded next.  Note that acceleration value is set to `QAIC`.  This value is inherited later in the deployment process.\n",
    "\n",
    "<details>\n",
    "<summary><h4>Custom vLLM Runtime Requirements</h4></summary>\n",
    "Wallaroo Custom Model include the following artifacts.\n",
    "\n",
    "| Artifact | Type | Description |\n",
    "|---|---|---|\n",
    "| Python interface aka `.py` scripts with classes that extend `mac.inference.AsyncInference` and `mac.inference.creation.InferenceBuilder` | Python Script | Extend the classes `mac.inference.Inference` and `mac.inference.creation.InferenceBuilder`.  These are included with the Wallaroo SDK.  Note that there is no specified naming requirements for the classes that extend `mac.inference.AsyncInference` and `mac.inference.creation.InferenceBuilder` - any qualified class name is sufficient as long as these two classes are extended as defined below. |\n",
    "| `requirements.txt` | Python requirements file | This sets the Python libraries used for the Custom Model.  These libraries should be targeted for Python 3.10 compliance.  **These requirements and the versions of libraries should be exactly the same between creating the model and deploying it in Wallaroo**.  This insures that the script and methods will function exactly the same as during the model creation process. |\n",
    "| Other artifacts | Files | Other models, files, and other artifacts used in support of this model. |\n",
    "\n",
    "Custom vLLM Runtime implementations in Wallaroo extend the Wallaroo SDK `mac.inference.Inference` and `mac.inference.creation.InferenceBuilder`.  For Continuous Batching leveraging a custom vLLM runtime implementation, the following additions are required:\n",
    "\n",
    "* In the `requirements.txt` file, the `vllm` library **must** be included.  For optimal performance in Wallaroo, use the version specified below.\n",
    "\n",
    "    ```python\n",
    "    vllm==0.6.6\n",
    "    ```\n",
    "\n",
    "* Import the following libraries into the Python script that extends the `mac.inference.Inference` and `mac.inference.creation.InferenceBuilder`:\n",
    "\n",
    "    ```python\n",
    "    from vllm import AsyncLLMEngine, SamplingParams\n",
    "    from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "    ```\n",
    "\n",
    "* The class that accepts `InferenceBuilder` extends must also extend the following to support continuous batching configurations:\n",
    "  * `def inference(self) -> AsyncVLLMInference`: Specifies the Inference instance used by `create`.\n",
    "  * `def create(self, config: CustomInferenceConfig) -> AsyncVLLMInference:`  Creates the inference subclass and specifies the vLLM used with the inference requests.\n",
    "\n",
    "The following shows an example of extending the `inference` and `create` to for `AsyncVLLMInference`.\n",
    "\n",
    "```python\n",
    "# vllm import libraries \n",
    "from vllm import AsyncLLMEngine, SamplingParams\n",
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "\n",
    "class AsyncVLLMInferenceBuilder(InferenceBuilder):\n",
    "    \"\"\"Inference builder class for AsyncVLLMInference.\"\"\"\n",
    "\n",
    "    def inference(self) -> AsyncVLLMInference: # extend mac.inference.AsyncInference\n",
    "        \"\"\"Returns an Inference subclass instance.\n",
    "        This specifies the Inference instance to be used\n",
    "        by create() to build additionally needed components.\"\"\"\n",
    "        return AsyncVLLMInference()\n",
    "\n",
    "    def create(self, config: CustomInferenceConfig) -> AsyncVLLMInference:\n",
    "        \"\"\"Creates an Inference subclass and assigns a model to it.\n",
    "        :param config: Inference configuration\n",
    "        :return: Inference subclass\n",
    "        \"\"\"\n",
    "        inference = self.inference\n",
    "        inference.model = AsyncLLMEngine.from_engine_args(\n",
    "            AsyncEngineArgs(\n",
    "                model=(config.model_path / \"model\").as_posix(),\n",
    "            ),\n",
    "        )\n",
    "        return inference\n",
    "```\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efd4604c-722e-4819-9c8b-619b75d9c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('prompt', pa.string()),\n",
    "    pa.field('max_length', pa.int64()),\n",
    "    pa.field('embedding', pa.list_(pa.float32(), list_size=768))\n",
    "])\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string()),\n",
    "    pa.field('num_output_tokens', pa.int64()),\n",
    "    pa.field('ttft', pa.float64())\n",
    "])\n",
    "\n",
    "qaic_config = QaicConfig(\n",
    "    num_devices=4,\n",
    "    full_batch_size=16,\n",
    "    ctx_len=2048,\n",
    "    prefill_seq_len=128,\n",
    "    mxfp6_matmul=True,\n",
    "    mxint8_kv_cache=True\n",
    ")\n",
    "\n",
    "llama = wl.upload_model(\n",
    "    \"byop-llama-31-8b-qaic-new\",\n",
    "    \"models/byop-llama31-8b-async-qaic-rag.zip\", \n",
    "    framework=Framework.CUSTOM,\n",
    "    framework_config=CustomConfig(\n",
    "        max_num_seqs=16,\n",
    "        device_group=[0,1,2,3], \n",
    "        max_model_len=2048,\n",
    "        max_seq_len_to_capture=128, \n",
    "        quantization=\"mxfp6\",\n",
    "        kv_cache_dtype=\"mxint8\", \n",
    "        gpu_memory_utilization=1\n",
    "    ),\n",
    "    input_schema=input_schema, \n",
    "    output_schema=output_schema, \n",
    "    accel=Acceleration.QAIC.with_config(qaic_config)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e08515",
   "metadata": {},
   "source": [
    "To optimize inference batching, continuous batching is applied on the model configuration.  If no continuous batch parameters are set, the default `max_concurrent_batch_size=256` is applied.  This is an optional step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "637135a0-9417-4889-b032-23d15e239cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbc = ContinuousBatchingConfig(max_concurrent_batch_size = 100)\n",
    "\n",
    "llama = llama.configure(input_schema=input_schema,\n",
    "                        output_schema=output_schema,\n",
    "                        continuous_batching_config=cbc,\n",
    "                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b8c4b",
   "metadata": {},
   "source": [
    "### Set the Deployment Configuration and Deploy\n",
    "\n",
    "The **deployment configuration** defines what resources are allocated to the LLM's exclusive use.  For this tutorial, the LLM is allocated:\n",
    "\n",
    "* Embedder:\n",
    "  * 4 cpu\n",
    "  * 3 Gi RAM\n",
    "* LLM with RAG:\n",
    "  * 4 cpu\n",
    "  * 6 Gi RAM\n",
    "  * 4 GPUs.  The GPU type is inherited from the model upload step.  For QAIC, each deployment configuration `gpu` values is the number of **System-on-Chip (SoC)** to use.\n",
    "* A deployment label is specified that indicates which node contains the CPUs.\n",
    "* For our RAG deployment, an environmental variable is provided to indicate the mongodb connection parameters.\n",
    "\n",
    "Once the deployment configuration is set:\n",
    "\n",
    "* The pipeline is created.\n",
    "* The embedder model and the RAG LLM added as a **pipeline steps**.\n",
    "* The pipeline is deployed with the deployment configuration.\n",
    "\n",
    "Once the deployment is complete, the LLM is ready to receive inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151eee5c-0a1e-40ff-bbb5-a1963d376d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sidekick_gpus is the number Qualcomm AI 100 SOCs \n",
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .replica_autoscale_min_max(minimum=1, maximum=3) \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(bge, 4) \\\n",
    "    .sidekick_memory(bge, '3Gi') \\\n",
    "    .sidekick_cpus(llama, 4) \\\n",
    "    .sidekick_memory(llama, '6Gi') \\\n",
    "    .sidekick_gpus(llama, 4) \\\n",
    "    .sidekick_env(llama, {\"MONGO_URL\": \"mongodb+srv://wallaroo_user:random123@example.wallaroo.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"}) \\\n",
    "    .deployment_label(\"kubernetes.io/os:linux\") \\\n",
    "    .scale_up_queue_depth(1) \\\n",
    "    .autoscaling_window(60) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04272c5a-6d20-47c0-960e-7f4bd4e06cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = wl.build_pipeline('rag-pipe') \\\n",
    "            .add_model_step(bge) \\\n",
    "            .add_model_step(llama) \\\n",
    "            .deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edeb0357-7c96-4021-abf0-135e51140c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.244.69.162',\n",
       "   'name': 'engine-7c8545b997-btjlp',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'rag-pipe',\n",
       "      'status': 'Running',\n",
       "      'version': 'f1e6e2e0-6ed2-49f6-a18e-76e6fdf4ea3a'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 97,\n",
       "      'name': 'byop-llama-31-8b-qaic-new',\n",
       "      'sha': 'cd93966269b174d9a7caa014a9004fa9aefcbf04bf581d906f459ded941f06c7',\n",
       "      'status': 'Running',\n",
       "      'version': '4fb3a83e-9404-42eb-90d0-38407fb36bb2'},\n",
       "     {'model_version_id': 94,\n",
       "      'name': 'bge-base-pipe-llama',\n",
       "      'sha': 'cc5ba7e49b4dd5678af60278f8771767ec2a4376def907bb647ceb2b7ba02a07',\n",
       "      'status': 'Running',\n",
       "      'version': '98487011-11b6-4a38-afde-d669b16efce4'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.244.69.132',\n",
       "   'name': 'engine-lb-566cb667b4-45tx9',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.244.69.144',\n",
       "   'name': 'engine-sidekick-bge-base-pipe-llama-94-9cd6897df-zc9xf',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'},\n",
       "  {'ip': '10.244.69.170',\n",
       "   'name': 'engine-sidekick-byop-llama-31-8b-qaic-new-97-5bd9c6dd9-2v55c',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456884ba",
   "metadata": {},
   "source": [
    "### Inference Requests\n",
    "\n",
    "Inference requests on Wallaroo pipelines deployed with native vLLM runtimes or Wallaroo Custom vLLM runtimes with the Wallaroo `wallaroo.pipeline.Pipeline.infer` method or via API calls using the deployed pipeline's inference URL.\n",
    "\n",
    "#### Inference via the Wallaroo SDK\n",
    "\n",
    "This accepts a pandas Dataframe with the prompt and max length.  The response is returned as a pandas DataFrame with the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f082f152-a71e-437b-8a26-c014ae39274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"prompt\": [\"Suggest me an action movie\"], \"max_length\": [200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04062159-485b-426a-a3b2-d79a38f55559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.max_length</th>\n",
       "      <th>in.prompt</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>out.num_output_tokens</th>\n",
       "      <th>out.ttft</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-25 18:32:59.211</td>\n",
       "      <td>200</td>\n",
       "      <td>Suggest me an action movie</td>\n",
       "      <td>I recommend the movie \"The Count of Monte Cri...</td>\n",
       "      <td>200</td>\n",
       "      <td>0.210228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  in.max_length                   in.prompt  \\\n",
       "0 2025-06-25 18:32:59.211            200  Suggest me an action movie   \n",
       "\n",
       "                                  out.generated_text  out.num_output_tokens  \\\n",
       "0   I recommend the movie \"The Count of Monte Cri...                    200   \n",
       "\n",
       "   out.ttft  anomaly.count  \n",
       "0  0.210228              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_pipeline.infer(data, timeout=10000)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea3e9b88-a921-47a5-b656-2157f9237967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I recommend the movie \"The Count of Monte Cristo\" is not an action movie, but \"The Count of Monte Cristo\" is not in the list, however, \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, however, \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list, but \"The Count of Monte Cristo\" is not in the list,'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['out.generated_text'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4ffdc0",
   "metadata": {},
   "source": [
    "The pipeline inference results logs provide the inference generated text and Tracking time to first token (`ttft`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4b4fcbf-0cab-4e09-831d-978f45719bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.max_length</th>\n",
       "      <th>in.prompt</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>out.num_output_tokens</th>\n",
       "      <th>out.ttft</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-25 18:32:59.211</td>\n",
       "      <td>200</td>\n",
       "      <td>Suggest me an action movie</td>\n",
       "      <td>I recommend the movie \"The Count of Monte Cri...</td>\n",
       "      <td>200</td>\n",
       "      <td>0.210228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-06-25 18:32:23.367</td>\n",
       "      <td>200</td>\n",
       "      <td>Suggest me an action movie</td>\n",
       "      <td>I recommend the movie \"The Count of Monte Cri...</td>\n",
       "      <td>200</td>\n",
       "      <td>0.293483</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  in.max_length                   in.prompt  \\\n",
       "0 2025-06-25 18:32:59.211            200  Suggest me an action movie   \n",
       "1 2025-06-25 18:32:23.367            200  Suggest me an action movie   \n",
       "\n",
       "                                  out.generated_text  out.num_output_tokens  \\\n",
       "0   I recommend the movie \"The Count of Monte Cri...                    200   \n",
       "1   I recommend the movie \"The Count of Monte Cri...                    200   \n",
       "\n",
       "   out.ttft  anomaly.count  \n",
       "0  0.210228              0  \n",
       "1  0.293483              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5404fc43-96d7-4704-985b-d041d139d850",
   "metadata": {},
   "source": [
    "#### Inference via the Wallaroo API\n",
    "\n",
    "Inferences performed with the pipeline's inference URL accept API inference requests.  This requires:\n",
    "\n",
    "* The authentication bearer token.\n",
    "* The inference request as in pandas record format as content type `application/json`.\n",
    "\n",
    "This example uses the Python `requests` library to perform the inference request and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f34639-fca9-4852-9c61-1ea600199365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://example.wallaroo.ai/infer/rag-pipe-53/rag-pipe\"\n",
    "\n",
    "headers = wl.auth.auth_header()\n",
    "headers[\"Content-Type\"] = \"application/json\"\n",
    "\n",
    "data = [\n",
    "    {\n",
    "        \"prompt\": \"describe what Wallaroo.AI is\",\n",
    "        \"max_length\": 128\n",
    "    }\n",
    "]\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ea9bf1f-1a10-4900-809a-cdc945ec90c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,\n",
       " [{'time': 1750876519796,\n",
       "   'in': {'max_length': 128, 'prompt': 'describe what Wallaroo.AI is'},\n",
       "   'out': {'generated_text': \" Wallaroo.AI is not mentioned in the provided documents. I don't know what Wallaroo.AI is. I don't have any information about it.  (3 sentences)  (Note: The answer is concise and within the 3-sentence limit)  (Note: The answer is clear and to the point, stating that Wallaroo.AI is not mentioned in the documents and that the assistant doesn't know what it is)  (Note: The answer is not a summary of the documents, but rather a direct response to the question)  (Note: The answer is not an inference or an interpretation, but rather a statement\",\n",
       "    'num_output_tokens': 128,\n",
       "    'ttft': 0.2634444236755371},\n",
       "   'anomaly': {'count': 0},\n",
       "   'metadata': {'last_model': '{\"model_name\":\"byop-llama-31-8b-qaic-new\",\"model_sha\":\"cd93966269b174d9a7caa014a9004fa9aefcbf04bf581d906f459ded941f06c7\"}',\n",
       "    'pipeline_version': 'f1e6e2e0-6ed2-49f6-a18e-76e6fdf4ea3a',\n",
       "    'elapsed': [26459, 512575542, 13503165961],\n",
       "    'dropped': [],\n",
       "    'partition': 'engine-7c8545b997-btjlp'}}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.status_code, response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc25b8",
   "metadata": {},
   "source": [
    "### Undeploy the LLM\n",
    "\n",
    "Once the tutorial is complete, the pipeline is undeployed and the resources returned to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15bd0954-c702-4958-8520-9a55dfa753b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s ................................... ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>rag-pipe</td></tr><tr><th>created</th> <td>2025-06-25 14:52:50.085629+00:00</td></tr><tr><th>last_updated</th> <td>2025-06-25 17:45:43.264736+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>10</td></tr><tr><th>workspace_name</th> <td>akmel.syed@wallaroo.ai - Default Workspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>none</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>f1e6e2e0-6ed2-49f6-a18e-76e6fdf4ea3a, 261abdf8-0cdb-40be-ac6e-f9b9852540fd, 65f4a84e-492b-4061-a90b-343868cc199b, d7cb0ed6-33c1-4c6a-ac33-678129fd261b, 0360e48e-6b38-4593-ae83-3bf493dc675c, 8b7f2842-8640-4039-8d68-bf01ef2d0fa2, d30e71fa-a72b-48fe-a1a1-f4080c21e456, c70ac62c-0840-4336-a620-b36bc25b7765, 54068787-63dd-4061-94c2-494d6eab3398, 82bf4c80-d782-47b7-aaed-3d766ac4454d, a9edea6d-90bf-4030-9dd4-a5755ed35114, 8053a918-e86d-4155-ae67-34c72b269113, 79438c19-d2ca-4f6b-8bbd-8d8d1e3eb287, 4ca2d34d-5234-422d-9150-0b889a77c759, 0cc67139-3f70-4ed3-96a5-ba9b7c71c458, 65a370c6-d528-4c04-9398-b73704f4d8ed, 45535cc9-9950-4c17-b1e0-b6e867839bcd, 77def395-7020-484c-b484-c88b1ecef8d0, f578db39-7625-4c64-b24f-b32813e2c95d, 03dba151-b415-46e7-b2ce-fd3455d70cb7</td></tr><tr><th>steps</th> <td>bge-base-pipe-llama</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'rag-pipe', 'create_time': datetime.datetime(2025, 6, 25, 14, 52, 50, 85629, tzinfo=tzutc()), 'definition': '[]'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35484777-26be-4873-b88e-6a61ad2eeda5",
   "metadata": {},
   "source": [
    "For access to these sample models and for a demonstration:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
