{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9967cde1-bbd3-445b-8267-ab3575b4df2d",
   "metadata": {},
   "source": [
    "# vLLM Handoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d4e0c-2ea2-484b-b9bb-4ea47dcac996",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e04a573-b12a-41b5-83dc-fb0548aab169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import wallaroo\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Acceleration\n",
    "from wallaroo.continuous_batching_config import ContinuousBatchingConfig\n",
    "from wallaroo.object import EntityNotFoundError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7683298b-7e1b-4498-b184-7050a9034414",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ade81b-f210-4d88-9fea-fd97a33f75c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Download & zip model\n",
    "\n",
    "```bash\n",
    "git lfs install\n",
    "```\n",
    "\n",
    "```bash\n",
    "git clone https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "```\n",
    "\n",
    "```bash\n",
    "cd ./Llama-3.1-8B-Instruct && rm -rf .git\n",
    "```\n",
    "\n",
    "```bash\n",
    "cd .. & zip -r llama-31-8b-instruct.zip ./Llama-3.1-8B-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2206be-95e6-4ebc-9a4a-113c6dca0dfd",
   "metadata": {},
   "source": [
    "## Define Schemas & Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c006df23-acb4-4baa-b0db-129a1e7b9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('prompt', pa.string()),\n",
    "    pa.field('max_tokens', pa.int64()),\n",
    "    # pa.field('top_p', pa.int64()),\n",
    "    # pa.field('ignore_eos', pa.bool()),\n",
    "    # pa.field('temperature', pa.int64()),\n",
    "])\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string()),\n",
    "    pa.field('num_output_tokens', pa.int64())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105013f0-4688-424a-8aa7-5290fcca0323",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Upload model via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcba6154-0eca-47a2-89ce-e70a7314d79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64.b64encode(\n",
    "    bytes(input_schema.serialize())\n",
    ").decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695e40c-63dd-4d41-a610-42cef02951ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "base64.b64encode(\n",
    "    bytes(output_schema.serialize())\n",
    ").decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec000884-a93a-424b-9d17-1e6ce6853af4",
   "metadata": {},
   "source": [
    "Run the following command in order to upload the model via **API**:\n",
    "\n",
    "```bash\n",
    "curl --progress-bar -X POST   -H \"Content-Type: multipart/form-data\"   -H \"Authorization: Bearer <your-auth-token-here>\"   -F 'metadata={\"name\": \"vllm-llama31-8b-async-fc-v3\", \"visibility\": \"private\", \"workspace_id\": <your-workspace-id-here>, \"conversion\": {\"framework\": \"vllm\", \"python_version\": \"3.8\", \"requirements\": [], \"framework_config\": {\"config\": {\"gpu_memory_utilization\": 0.9, \"max_model_len\": 128}, \"framework\": \"vllm\"}}, \"input_schema\": \"/////7AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAIAAABUAAAABAAAAMT///8AAAECEAAAACQAAAAEAAAAAAAAAAoAAABtYXhfdG9rZW5zAAAIAAwACAAHAAgAAAAAAAABQAAAABAAFAAIAAYABwAMAAAAEAAQAAAAAAABBRAAAAAcAAAABAAAAAAAAAAGAAAAcHJvbXB0AAAEAAQABAAAAA==\", \"output_schema\": \"/////8AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAIAAABcAAAABAAAALz///8AAAECEAAAACwAAAAEAAAAAAAAABEAAABudW1fb3V0cHV0X3Rva2VucwAAAAgADAAIAAcACAAAAAAAAAFAAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA=\"};type=application/json'   -F \"file=@llama-31-8b-instruct.zip;type=application/octet-stream\"   https://benchmarkscluster.wallaroocommunity.ninja/v1/api/models/upload_and_convert | cat\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216dfce1-e1f9-4574-bd0d-d748b04ec685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the model\n",
    "model = wl.get_model(\"vllm-llama31-8b-async-fc-v3\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39d8cc-3d86-4e94-ba07-32004aaae54f",
   "metadata": {},
   "source": [
    "### Upload model via SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2734d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wallaroo.framework import VLLMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e572f22b-0e0e-4fdc-82af-1148996e903c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mVLLMConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice_group\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7b569d49cca0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgpu_memory_utilization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkv_cache_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache_dtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKvCacheDtype\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mKvCacheDtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_model_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7b569d49cd60\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_num_seqs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_seq_len_to_capture\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquantization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQuantization\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mQuantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mwallaroo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwallaroo_ml_ops_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnset\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7b569d49d120\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mVLLMConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameworkConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m    A config for the VLLM framework.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice_group\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_group\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"List of device ids to compile the model for that is only supported for `Acceleration.QAIC`. Defaults to `None`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgpu_memory_utilization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_memory_utilization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The percentage of GPU memory to use. Defaults to `0.9`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkv_cache_dtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKvCacheDtype\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkv_cache_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The data type of the KV cache. `mxint8` is only supported for `Acceleration.QAIC`. Defaults to 'auto'.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_model_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_model_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The maximum length of the model. Defaults to `None`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_num_seqs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_num_seqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The maximum number of sequences to run in parallel. Defaults to `None`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_seq_len_to_capture\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_len_to_capture\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The maximum length of the sequences to capture. Defaults to `8192`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquantization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantization\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The quantization to use. `mxfp6` is only supported for `Acceleration.QAIC`. Defaults to 'none'.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mblock_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_open_api_vllm_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mvalidate_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"The block size to use for the model. Recommended to set to `32` for better performance with `Acceleration.QAIC`. Defaults to `None`.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFramework\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"The framework that this config is for.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mFramework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVLLM\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_to_openapi_framework_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOpenapiFrameworkConfigVLLM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Convert the config to the autogenerated OpenAPI version\u001b[0m\n",
       "\u001b[0;34m        in order to validate the config.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mOpenapiFrameworkConfigVLLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_to_openapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/wallaroo/framework.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "??VLLMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "730a5fd6-2a75-4fb4-a1df-dd0cbf45a032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log into the following URL in a web browser:\n",
      "\n",
      "\thttps://autoscale-uat-gcp.wallaroo.dev/auth/realms/master/device?user_code=VPKN-MCIQ\n",
      "\n",
      "Login successful!\n",
      "Waiting for model loading - this will take up to 10min.\n",
      ".odel is pending loading to a container runtime.\n",
      ".............................................successful\n",
      "\n",
      "Ready\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>vllm-llama31-8b-async-demo</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>422d3ad9-1bc7-40c1-99af-0ba109964bfd</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>vLLM_llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>cuda</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-08-May 19:24:36</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'vllm-llama31-8b-async-demo', 'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd', 'file_name': 'vLLM_llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132', 'arch': 'x86', 'accel': 'cuda', 'last_update_time': datetime.datetime(2025, 5, 8, 19, 24, 36, 389585, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = wl.upload_model(\n",
    "    \"vllm-llama31-8b-async-demo\", \n",
    "    \"./vLLM_llama-31-8b.zip\",\n",
    "    framework=Framework.VLLM,\n",
    "    framework_config=VLLMConfig(\n",
    "        gpu_memory_utilization=0.9, \n",
    "        max_model_len=128\n",
    "    ),\n",
    "    input_schema=input_schema, \n",
    "    output_schema=output_schema,\n",
    "    accel=Acceleration.CUDA\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce824ba-a8c1-47cd-9b56-40be3878d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>vllm-llama31-8b-async-demo</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>422d3ad9-1bc7-40c1-99af-0ba109964bfd</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>vLLM_llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>cuda</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-08-May 19:24:36</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'vllm-llama31-8b-async-demo', 'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd', 'file_name': 'vLLM_llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132', 'arch': 'x86', 'accel': 'cuda', 'last_update_time': datetime.datetime(2025, 5, 8, 19, 24, 36, 389585, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd4f16b-b40a-4d2f-bb83-806ff285aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continous batching for Async vLLM (you can choose the number of connections you want)\n",
    "cbc = ContinuousBatchingConfig(max_concurrent_batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c427430d-58b5-4d26-b930-52a175fb2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = model.configure(\n",
    "    input_schema = input_schema,\n",
    "    output_schema = output_schema,\n",
    "    continuous_batching_config = cbc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bff9f1a-6db6-412c-bd0c-56cc3fcf8d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>vllm-llama31-8b-async-demo</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>422d3ad9-1bc7-40c1-99af-0ba109964bfd</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>vLLM_llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>cuda</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-08-May 19:24:36</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'vllm-llama31-8b-async-demo', 'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd', 'file_name': 'vLLM_llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132', 'arch': 'x86', 'accel': 'cuda', 'last_update_time': datetime.datetime(2025, 5, 8, 19, 24, 36, 389585, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3496d0",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20972f6c-594c-4b3b-b136-bdf25f6e1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1.).memory('1Gi') \\\n",
    "    .sidekick_cpus(batch, 1.) \\\n",
    "    .sidekick_memory(batch, '10Gi') \\\n",
    "    .sidekick_gpus(batch, 1) \\\n",
    "    .deployment_label(\"wallaroo.ai/accelerator:a100\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be23d51-b4c7-45c1-8f03-f6034f4fb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"llama-31-8b-vllm-demo\")\n",
    "pipeline.clear()\n",
    "pipeline.undeploy()\n",
    "\n",
    "pipeline.add_model_step(batch)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7caa327-22e9-4838-b582-0589430b6476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.4.8.2',\n",
       "   'name': 'engine-8558f6576d-8h7pc',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'llama-31-8b-vllm-demo',\n",
       "      'status': 'Running',\n",
       "      'version': '62806288-5f42-44b8-9345-bb4dfb613801'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 443,\n",
       "      'name': 'vllm-llama31-8b-async-demo',\n",
       "      'sha': '62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838',\n",
       "      'status': 'Running',\n",
       "      'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.4.1.17',\n",
       "   'name': 'engine-lb-5cf49f9d5f-sqr4f',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.4.8.7',\n",
       "   'name': 'engine-sidekick-vllm-llama31-8b-async-demo-443-75d58845c-svvll',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3336d28",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8691aaee-2a24-4ec5-90b0-7f87b13163f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"prompt\": [\"What is Wallaroo.AI?\"], \"max_tokens\": [200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b89b57d-d8a3-47b0-bc7f-e28a551b932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.max_tokens</th>\n",
       "      <th>in.prompt</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>out.num_output_tokens</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-08 19:42:06.259</td>\n",
       "      <td>200</td>\n",
       "      <td>What is Wallaroo.AI?</td>\n",
       "      <td>Cloud and AutoML with Python\\nWallaroo.AI is a...</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  in.max_tokens             in.prompt  \\\n",
       "0 2025-05-08 19:42:06.259            200  What is Wallaroo.AI?   \n",
       "\n",
       "                                  out.generated_text  out.num_output_tokens  \\\n",
       "0  Cloud and AutoML with Python\\nWallaroo.AI is a...                    122   \n",
       "\n",
       "   anomaly.count  \n",
       "0              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.infer(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1c1779-6662-4825-942f-3bdaa86e81be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for pipeline publish... It may take up to 600 sec.\n",
      "............................................... Published.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "          <table>\n",
       "              <tr><td>ID</td><td>36</td></tr>\n",
       "              <tr><td>Pipeline Name</td><td>llama-31-8b-vllm-demo</td></tr>\n",
       "              <tr><td>Pipeline Version</td><td>a5b7a202-9923-4d8d-ba4c-31e22a83cddc</td></tr>\n",
       "              <tr><td>Status</td><td>Published</td></tr>\n",
       "              <tr><td>Workspace Id</td><td>60</td></tr>\n",
       "              <tr><td>Workspace Name</td><td>younes.amar@wallaroo.ai - Default Workspace</td></tr>\n",
       "              <tr><td>Edges</td><td></td></tr>\n",
       "              <tr><td>Engine URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132</a></td></tr>\n",
       "              <tr><td>Pipeline URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc</a></td></tr>\n",
       "              <tr><td>Helm Chart URL</td><td>oci://<a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo</a></td></tr>\n",
       "              <tr><td>Helm Chart Reference</td><td>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:af38b73f10fbf6d9da318568d86383b762dee766547a35c30dccf5f7907695e1</td></tr>\n",
       "              <tr><td>Helm Chart Version</td><td>0.0.1-a5b7a202-9923-4d8d-ba4c-31e22a83cddc</td></tr>\n",
       "              <tr><td>Engine Config</td><td>{'engine': {'resources': {'limits': {'cpu': 1.0, 'memory': '1Gi'}, 'requests': {'cpu': 1.0, 'memory': '1Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none', 'cpu_utilization': 50.0}, 'images': {'vllm-llama31-8b-async-demo-443': {'resources': {'limits': {'cpu': 1.0, 'memory': '10Gi'}, 'requests': {'cpu': 1.0, 'memory': '10Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': True}}}}}</td></tr>\n",
       "              <tr><td>User Images</td><td>[]</td></tr>\n",
       "              <tr><td>Created By</td><td>younes.amar@wallaroo.ai</td></tr>\n",
       "              <tr><td>Created At</td><td>2025-05-08 19:42:16.092419+00:00</td></tr>\n",
       "              <tr><td>Updated At</td><td>2025-05-08 19:42:16.092419+00:00</td></tr>\n",
       "              <tr><td>Replaces</td><td></td></tr>\n",
       "              <tr>\n",
       "                  <td>Docker Run Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">docker run \\\n",
       "    -p $EDGE_PORT:8080 \\\n",
       "    -e OCI_USERNAME=$OCI_USERNAME \\\n",
       "    -e OCI_PASSWORD=$OCI_PASSWORD \\\n",
       "    -e PIPELINE_URL=us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc \\\n",
       "    -e CONFIG_CPUS=1.0 --gpus all --cpus=2.0 --memory=11g \\\n",
       "    us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>EDGE_PORT</code>, <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              <tr>\n",
       "                  <td>Helm Install Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">helm install --atomic $HELM_INSTALL_NAME \\\n",
       "    oci://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo \\\n",
       "    --namespace $HELM_INSTALL_NAMESPACE \\\n",
       "    --version 0.0.1-a5b7a202-9923-4d8d-ba4c-31e22a83cddc \\\n",
       "    --set ociRegistry.username=$OCI_USERNAME \\\n",
       "    --set ociRegistry.password=$OCI_PASSWORD</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>HELM_INSTALL_NAME</code>, <code>HELM_INSTALL_NAMESPACE</code>,\n",
       "                          <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              \n",
       "          </table>\n",
       "        "
      ],
      "text/plain": [
       "PipelinePublish(created_at=datetime.datetime(2025, 5, 8, 19, 42, 16, 92419, tzinfo=tzutc()), docker_run_variables={'PIPELINE_URL': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc'}, engine_config={'engine': {'resources': {'limits': {'cpu': 1.0, 'memory': '1Gi'}, 'requests': {'cpu': 1.0, 'memory': '1Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none', 'cpu_utilization': 50.0}, 'images': {'vllm-llama31-8b-async-demo-443': {'resources': {'limits': {'cpu': 1.0, 'memory': '10Gi'}, 'requests': {'cpu': 1.0, 'memory': '10Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': True}}}}}, id=36, pipeline_name='llama-31-8b-vllm-demo', pipeline_version_id=1000, replaces=[], status='Published', updated_at=datetime.datetime(2025, 5, 8, 19, 42, 16, 92419, tzinfo=tzutc()), user_images=[], created_by='fbddff9d-4916-4f46-a906-0be1b372a9f2', created_on_version='2025.1.0', edge_bundles=<wallaroo.wallaroo_ml_ops_api_client.types.Unset object at 0x7b56a36de6e0>, engine_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132', error=None, helm={'reference': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:af38b73f10fbf6d9da318568d86383b762dee766547a35c30dccf5f7907695e1', 'values': {}, 'chart': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo', 'version': '0.0.1-a5b7a202-9923-4d8d-ba4c-31e22a83cddc'}, pipeline_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc', pipeline_version_name='a5b7a202-9923-4d8d-ba4c-31e22a83cddc', workspace_id=60, workspace_name='younes.amar@wallaroo.ai - Default Workspace', additional_properties={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.publish(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c94a2a",
   "metadata": {},
   "source": [
    "## Undeploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca5100af-0149-48a9-9501-6359f76988b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log into the following URL in a web browser:\n",
      "\n",
      "\thttps://autoscale-uat-gcp.wallaroo.dev/auth/realms/master/device?user_code=XXND-IZZK\n",
      "\n",
      "Login successful!\n",
      " ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>llama-31-8b-vllm-ynsv5</td></tr><tr><th>created</th> <td>2025-05-06 12:31:40.360907+00:00</td></tr><tr><th>last_updated</th> <td>2025-05-06 19:51:47.490400+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>60</td></tr><tr><th>workspace_name</th> <td>younes.amar@wallaroo.ai - Default Workspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>cuda</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>b82ed30f-e937-4b49-94d5-63e6e798cc4b, b0a4ab4d-28ee-4470-9391-888a486375d2, 47760536-b263-428d-a9eb-f763c84f8920, 632917ff-0ffd-49be-abca-5a69a6432f93, 18cc0cad-cf6c-4abf-9083-ee90c2e704e2</td></tr><tr><th>steps</th> <td>vllm-llama31-8b-async-ynsv5</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'llama-31-8b-vllm-ynsv5', 'create_time': datetime.datetime(2025, 5, 6, 12, 31, 40, 360907, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'vllm-llama31-8b-async-ynsv5', 'version': 'c96ec281-6c97-4529-b722-76e44d41ea9a', 'sha': '62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838'}]}}]\"}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147baad-0092-4c6a-8403-26147800f09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
