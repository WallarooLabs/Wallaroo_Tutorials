{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9967cde1-bbd3-445b-8267-ab3575b4df2d",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.1_tutorials/wallaroo-llms/llm-performance-optimizations/continuous-batching-standard-llama).\n",
    "\n",
    "## Continuous Batching with Llama 3.1 8B Instruct Tutorial\n",
    "\n",
    "Wallaroo **continuous batching** for vLLMs provides:\n",
    "\n",
    "* Standards based vLLM deployment options.\n",
    "* Increased performance for vLLM deployments that leverage GPUs.\n",
    "\n",
    "**Continuous Batching** improves throughput by dynamically grouping incoming inference requests in real time to optimize processing. Itâ€™s useful for real concurrent inference requests when LLM-based or agentic AI applications run at scale, balancing latency, throughput, and resource use.\n",
    "\n",
    "Performance is fine tuned through **Framework Configurations** which tailor how the vLLM is deployed and processes requests.\n",
    "\n",
    "Wallaroo continuous batching is available for the following frameworks:\n",
    "\n",
    "* `wallaroo.framework.Framework.VLLM` aka \"Standard Framework\":  Hugging Face vLLM models compatible with NVIDIA CUDA.\n",
    "* `wallaroo.framework.Framework.CUSTOM` aka \"Custom Framework\":  Wallaroo Custom Models aka BYOP (Bring Your Own Predict) provide greater flexibility through Python scripts included with the model artifacts.  For more details, see [Custom Model Upload](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "\n",
    "This tutorial demonstrates deploying the Llama V3 Instruct LLM with continuous batching in Wallaroo with CUDA AI Acceleration with the Standard Framework.  For access to these sample models and for a demonstration of how to use Continuous Batching to improve LLM performance:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "## Tutorial Overview\n",
    "\n",
    "This tutorial demonstrates using Wallaroo to:\n",
    "\n",
    "* Upload a LLM with the following options:\n",
    "  * Framework:  `vLLM`\n",
    "  * Framework Configuration to specify LLM options.\n",
    "* Define a Continuous Batching Configuration and apply it to the LLM model configuration.\n",
    "* Deploy a the LLM with a Deployment Configuration that allocates resources to the LLM; the Framework Configuration is applied at the LLM level, so it inherited during deployment.\n",
    "* Demonstrate how to perform a sample inference.\n",
    "* Demonstrate publishing an Wallaroo pipeline to an Open Container Initiative (OCI) registry for deployment in multi-cloud or edge environments.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "The following tutorial requires the following:\n",
    "\n",
    "* Llama V3 Instruct vLLM.  This is available through a Wallaroo representative.\n",
    "* Wallaroo version 2025.1 and above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d4e0c-2ea2-484b-b9bb-4ea47dcac996",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Library Imports\n",
    "\n",
    "We start by importing the libraries used for this tutorial, including the [Wallaroo SDK](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/).  This is provided by default when executing this Jupyter Notebook in the Wallaroo JupyterHub service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e04a573-b12a-41b5-83dc-fb0548aab169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Acceleration\n",
    "from wallaroo.continuous_batching_config import ContinuousBatchingConfig\n",
    "from wallaroo.object import EntityNotFoundError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d86b65c",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The next step to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7683298b-7e1b-4498-b184-7050a9034414",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2206be-95e6-4ebc-9a4a-113c6dca0dfd",
   "metadata": {},
   "source": [
    "### Define Schemas and Upload Model\n",
    "\n",
    "The model is uploaded via the Wallaroo SDK method `wallaroo.client.Client.upload_model` which takes the following parameters.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| `name` | `string` (*Required*) | The name of the model.  Model names are unique **per workspace**.  Models that are uploaded with the same name are assigned as a new **version** of the model. |\n",
    "| `path` | `string` (*Required*) | The path to the model file being uploaded. |\n",
    "| `framework` |`string` (*Required*) | The framework of the model from `wallaroo.framework.Framework`.  For vLLMs, this framework is `wallaroo.framework.Framework.VLLM`.|\n",
    "| `input_schema` | `pyarrow.lib.Schema` <ul><li>Native Wallaroo Runtimes: (*Optional*)</li><li>Non-Native Wallaroo Runtimes: (*Required*)</li></ul> | The input schema in Apache Arrow schema format. |\n",
    "| `output_schema` | `pyarrow.lib.Schema` <ul><li>Native Wallaroo Runtimes: (*Optional*)</li><li>Non-Native Wallaroo Runtimes: (*Required*)</li></ul> | The output schema in Apache Arrow schema format. |\n",
    "| `framework_config` | `wallaroo.framework.VLLMConfig` (*Optional*) | Sets the vLLM configuration options based on the [Framework Configuration Parameters](#framework-configuration-parameters).  If no options are specified, the default values are applied. |\n",
    "| `convert_wait` | `bool` (*Optional*) | <ul><li>**True**: Waits in the script for the model conversion completion.</li><li>**False**:  Proceeds with the script without waiting for the model conversion process to display complete.</li></ul> |\n",
    "\n",
    "`wallaroo.framework.VLLMConfig` contains the following parameters.\n",
    "\n",
    "| Parameters | Type |\n",
    "|---|---|\n",
    "| **max_num_seqs** | *Integer* (*Default: 256*) |\n",
    "| **max_model_len** | *Integer* (*Default: None*) |\n",
    "| **max_seq_len_to_capture** | *Integer* (*Default: 8192*) |\n",
    "| **quantization** | (*Default: None*)  |\n",
    "| **kv_cache_dtype** (*Default: 'auto'*) |\n",
    "| **gpu_memory_utilization** | **Float** (*Default: 0.9*) |\n",
    "| **block_size** | (*Default: None*)  |\n",
    "| **device_group** |  (*Default: None*) This setting is ignored for CUDA acceleration. |\n",
    "\n",
    "#### Define Input and Output Schemas\n",
    "\n",
    "The input and output schemas are defined in Apache pyarrow format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006df23-acb4-4baa-b0db-129a1e7b9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('prompt', pa.string()),\n",
    "    pa.field('max_tokens', pa.int64())\n",
    "])\n",
    "output_schema = pa.schema([\n",
    "    pa.field('generated_text', pa.string()),\n",
    "    pa.field('num_output_tokens', pa.int64())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d39d8cc-3d86-4e94-ba07-32004aaae54f",
   "metadata": {},
   "source": [
    "#### Define VLLMConfig\n",
    "\n",
    "We define the `wallaroo.framework.VLLMConfig` object and \n",
    "\n",
    "`wallaroo.framework.VLLMConfig` contains the following parameters.\n",
    "\n",
    "| Parameters | Type |\n",
    "|---|---|\n",
    "| **max_num_seqs** | *Integer* (*Default: 256*) |\n",
    "| **max_model_len** | *Integer* (*Default: None*) |\n",
    "| **max_seq_len_to_capture** | *Integer* (*Default: 8192*) |\n",
    "| **quantization** | (*Default: None*)  |\n",
    "| **kv_cache_dtype** (*Default: 'auto'*) |\n",
    "| **gpu_memory_utilization** | **Float** (*Default: 0.9*) |\n",
    "| **block_size** | (*Default: None*) |\n",
    "| **device_group** |  (*Default: None*) This setting is ignored for for CUDA acceleration. |\n",
    "\n",
    "For this example, the `VLLMConfig` parameters are set with the following:\n",
    "\n",
    "* `gpu_memory_utilization=0.9` \n",
    "* `max_model_len=128`\n",
    "\n",
    "Other parameters will use the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_config = VLLMConfig(\n",
    "        gpu_memory_utilization=0.9, \n",
    "        max_model_len=128\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc0c91f",
   "metadata": {},
   "source": [
    "#### Upload model via the Wallaroo SDK\n",
    "\n",
    "With our values set, we upload the model with the `wallaroo.client.Client.upload_model` method with the following parameters:\n",
    "\n",
    "* Model name and path to the Llama V3 Instruct LLM.\n",
    "* `framework_config` set to our defined `VLLMConfig`.\n",
    "* Input and output schemas.\n",
    "* `accel` set to `from wallaroo.engine_config.Acceleration.CUDA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730a5fd6-2a75-4fb4-a1df-dd0cbf45a032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for model loading - this will take up to 10min.\n",
      ".odel is pending loading to a container runtime.\n",
      ".............................................successful\n",
      "\n",
      "Ready\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>vllm-llama31-8b-async-demo</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>422d3ad9-1bc7-40c1-99af-0ba109964bfd</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>vLLM_llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>cuda</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-08-May 19:24:36</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'vllm-llama31-8b-async-demo', 'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd', 'file_name': 'vLLM_llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132', 'arch': 'x86', 'accel': 'cuda', 'last_update_time': datetime.datetime(2025, 5, 8, 19, 24, 36, 389585, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = wl.upload_model(\n",
    "    \"vllm-llama31-8b-async-demo\", \n",
    "    \"./models/vLLM_llama-31-8b.zip\",\n",
    "    framework=Framework.VLLM,\n",
    "    framework_config=vllm_config,\n",
    "    input_schema=input_schema, \n",
    "    output_schema=output_schema,\n",
    "    accel=Acceleration.CUDA\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce824ba-a8c1-47cd-9b56-40be3878d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>vllm-llama31-8b-async-demo</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>422d3ad9-1bc7-40c1-99af-0ba109964bfd</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>vLLM_llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>cuda</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-08-May 19:24:36</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'vllm-llama31-8b-async-demo', 'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd', 'file_name': 'vLLM_llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132', 'arch': 'x86', 'accel': 'cuda', 'last_update_time': datetime.datetime(2025, 5, 8, 19, 24, 36, 389585, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4923b78",
   "metadata": {},
   "source": [
    "### Set Continuous Batching Configuration\n",
    "\n",
    "The model configuration is set either during model upload or post model upload.  We define the continuous batching configuration with the max current batch size set to `100`, then apply it to the model configuration.\n",
    "\n",
    "If the `max_concurrent_batch_size` is **not** specified it is set to the default to the value of `256`.\n",
    "\n",
    "When applying a continuous batch configuration to a model configuration, the input and output schemas **must** be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd4f16b-b40a-4d2f-bb83-806ff285aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continuous batching for Async vLLM (you can choose the number of connections you want)\n",
    "cbc = ContinuousBatchingConfig(max_concurrent_batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c427430d-58b5-4d26-b930-52a175fb2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vllm_with_continuous_batching = model.configure(\n",
    "    input_schema = input_schema,\n",
    "    output_schema = output_schema,\n",
    "    continuous_batching_config = cbc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bff9f1a-6db6-412c-bd0c-56cc3fcf8d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>vllm-llama31-8b-async-demo</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>422d3ad9-1bc7-40c1-99af-0ba109964bfd</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>vLLM_llama-31-8b.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>x86</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>cuda</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2025-08-May 19:24:36</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace id</td>\n",
       "          <td>60</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Workspace name</td>\n",
       "          <td>younes.amar@wallaroo.ai - Default Workspace</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'vllm-llama31-8b-async-demo', 'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd', 'file_name': 'vLLM_llama-31-8b.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2025.1.0-main-6132', 'arch': 'x86', 'accel': 'cuda', 'last_update_time': datetime.datetime(2025, 5, 8, 19, 24, 36, 389585, tzinfo=tzutc())}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vllm_with_continuous_batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3496d0",
   "metadata": {},
   "source": [
    "### Deploy Model\n",
    "\n",
    "Models are deployed in Wallaroo via **Wallaroo Pipelines** through the following process.\n",
    "\n",
    "* (Optional): Create a **deployment configuration**.  If no deployment configuration is specified, then the default values are used.  For our deployment, we specify the LLM is assigned the following resources:\n",
    "  * 1 cpu\n",
    "  * 10 Gi RAM\n",
    "  * 1 gpu from the nodepool `\"wallaroo.ai/accelerator:a100\"`.  Wallaroo deployments and pipelines inherit the acceleration settings from the model, so this will be `CUDA`.\n",
    "* Create the Wallaroo pipeline.\n",
    "* Assign the model as a **pipeline step** to processing incoming data and return the inference results.\n",
    "* Deploy the pipeline with the pipeline configuration.\n",
    "\n",
    "#### Define the Deployment Configuration\n",
    "\n",
    "The deployment configuration allocates resources for the LLM's exclusive use.  These resources are used by the LLM until the pipeline is **undeployed** and the resources returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20972f6c-594c-4b3b-b136-bdf25f6e1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = DeploymentConfigBuilder() \\\n",
    "    .cpus(1.).memory('1Gi') \\\n",
    "    .sidekick_cpus(vllm_with_continuous_batching, 1.) \\\n",
    "    .sidekick_memory(vllm_with_continuous_batching, '10Gi') \\\n",
    "    .sidekick_gpus(vllm_with_continuous_batching, 1) \\\n",
    "    .deployment_label(\"wallaroo.ai/accelerator:a100\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c12cd",
   "metadata": {},
   "source": [
    "#### Deploy vLLM\n",
    "\n",
    "The next steps we deploy the model by creating the pipeline, adding the vLLM as the pipeline step, and deploying the pipeline with the deployment configuration.\n",
    "\n",
    "Once complete, the model is ready to accept inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be23d51-b4c7-45c1-8f03-f6034f4fb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"llama-31-8b-vllm-demo\")\n",
    "pipeline.clear()\n",
    "\n",
    "pipeline.add_model_step(vllm_with_continuous_batching)\n",
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7caa327-22e9-4838-b582-0589430b6476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.4.8.2',\n",
       "   'name': 'engine-8558f6576d-8h7pc',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'llama-31-8b-vllm-demo',\n",
       "      'status': 'Running',\n",
       "      'version': '62806288-5f42-44b8-9345-bb4dfb613801'}]},\n",
       "   'model_statuses': {'models': [{'model_version_id': 443,\n",
       "      'name': 'vllm-llama31-8b-async-demo',\n",
       "      'sha': '62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838',\n",
       "      'status': 'Running',\n",
       "      'version': '422d3ad9-1bc7-40c1-99af-0ba109964bfd'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.4.1.17',\n",
       "   'name': 'engine-lb-5cf49f9d5f-sqr4f',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': [{'ip': '10.4.8.7',\n",
       "   'name': 'engine-sidekick-vllm-llama31-8b-async-demo-443-75d58845c-svvll',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'statuses': '\\n'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3336d28",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Inference requests are submitted to deployed models as either pandas DataFrames or Apache Arrow tables.  The inference data must match the input schemas defined earlier.\n",
    "\n",
    "Our sample inference request submits a pandas DataFrame with a simple prompt and the `max_tokens` field set to 200.  We receive a pandas DataFrame in return with the outputs labeled as `out.{variable_name}`, with `variable_name` matching the output schemas defined at model upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8691aaee-2a24-4ec5-90b0-7f87b13163f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"prompt\": [\"What is Wallaroo.AI?\"], \"max_tokens\": [200]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b89b57d-d8a3-47b0-bc7f-e28a551b932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.max_tokens</th>\n",
       "      <th>in.prompt</th>\n",
       "      <th>out.generated_text</th>\n",
       "      <th>out.num_output_tokens</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-08 19:42:06.259</td>\n",
       "      <td>200</td>\n",
       "      <td>What is Wallaroo.AI?</td>\n",
       "      <td>Cloud and AutoML with Python\\nWallaroo.AI is a...</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time  in.max_tokens             in.prompt  \\\n",
       "0 2025-05-08 19:42:06.259            200  What is Wallaroo.AI?   \n",
       "\n",
       "                                  out.generated_text  out.num_output_tokens  \\\n",
       "0  Cloud and AutoML with Python\\nWallaroo.AI is a...                    122   \n",
       "\n",
       "   anomaly.count  \n",
       "0              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.infer(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9fd72e",
   "metadata": {},
   "source": [
    "### Publish Pipeline\n",
    "\n",
    "Wallaroo pipelines are published to OCI Registries via the `wallaroo.pipeline.Pipeline.publish` method.  This stores the following in the OCI registry:\n",
    "\n",
    "* The LLM set as the pipeline step.\n",
    "* The Wallaroo engine used to deploy the LLM.  The engine used is targeted based on settings inherited from the LLM set during the **model upload** stage.  These settings include:\n",
    "  * Architecture\n",
    "  * AI accelerations\n",
    "  * Framework Configuration\n",
    "* The deployment configuration included with as a parameter to the publish command.\n",
    "\n",
    "For more details on publishing, deploying, and inferencing in multi-cloud and edge with Wallaroo, see [Edge and Multi-cloud Model Publish and Deploy](https://docs.wallaroo.ai/wallaroo-model-operations-run-anywhere/wallaroo-model-operations-run-anywhere-inference/wallaroo-model-operations-run-anywhere-publish/).\n",
    "\n",
    "Note that when published to an OCI registry, the `publish` command returns the `docker run` and `helm install` commands used to deploy the specified LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe1c1779-6662-4825-942f-3bdaa86e81be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for pipeline publish... It may take up to 600 sec.\n",
      "............................................... Published.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "          <table>\n",
       "              <tr><td>ID</td><td>36</td></tr>\n",
       "              <tr><td>Pipeline Name</td><td>llama-31-8b-vllm-demo</td></tr>\n",
       "              <tr><td>Pipeline Version</td><td>a5b7a202-9923-4d8d-ba4c-31e22a83cddc</td></tr>\n",
       "              <tr><td>Status</td><td>Published</td></tr>\n",
       "              <tr><td>Workspace Id</td><td>60</td></tr>\n",
       "              <tr><td>Workspace Name</td><td>younes.amar@wallaroo.ai - Default Workspace</td></tr>\n",
       "              <tr><td>Edges</td><td></td></tr>\n",
       "              <tr><td>Engine URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132</a></td></tr>\n",
       "              <tr><td>Pipeline URL</td><td><a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc</a></td></tr>\n",
       "              <tr><td>Helm Chart URL</td><td>oci://<a href='https://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo'>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo</a></td></tr>\n",
       "              <tr><td>Helm Chart Reference</td><td>us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:af38b73f10fbf6d9da318568d86383b762dee766547a35c30dccf5f7907695e1</td></tr>\n",
       "              <tr><td>Helm Chart Version</td><td>0.0.1-a5b7a202-9923-4d8d-ba4c-31e22a83cddc</td></tr>\n",
       "              <tr><td>Engine Config</td><td>{'engine': {'resources': {'limits': {'cpu': 1.0, 'memory': '1Gi'}, 'requests': {'cpu': 1.0, 'memory': '1Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none', 'cpu_utilization': 50.0}, 'images': {'vllm-llama31-8b-async-demo-443': {'resources': {'limits': {'cpu': 1.0, 'memory': '10Gi'}, 'requests': {'cpu': 1.0, 'memory': '10Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': True}}}}}</td></tr>\n",
       "              <tr><td>User Images</td><td>[]</td></tr>\n",
       "              <tr><td>Created By</td><td>younes.amar@wallaroo.ai</td></tr>\n",
       "              <tr><td>Created At</td><td>2025-05-08 19:42:16.092419+00:00</td></tr>\n",
       "              <tr><td>Updated At</td><td>2025-05-08 19:42:16.092419+00:00</td></tr>\n",
       "              <tr><td>Replaces</td><td></td></tr>\n",
       "              <tr>\n",
       "                  <td>Docker Run Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">docker run \\\n",
       "    -p $EDGE_PORT:8080 \\\n",
       "    -e OCI_USERNAME=$OCI_USERNAME \\\n",
       "    -e OCI_PASSWORD=$OCI_PASSWORD \\\n",
       "    -e PIPELINE_URL=us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc \\\n",
       "    -e CONFIG_CPUS=1.0 --gpus all --cpus=2.0 --memory=11g \\\n",
       "    us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>EDGE_PORT</code>, <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              <tr>\n",
       "                  <td>Helm Install Command</td>\n",
       "                  <td>\n",
       "                      <table><tr><td>\n",
       "<pre style=\"text-align: left\">helm install --atomic $HELM_INSTALL_NAME \\\n",
       "    oci://us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo \\\n",
       "    --namespace $HELM_INSTALL_NAMESPACE \\\n",
       "    --version 0.0.1-a5b7a202-9923-4d8d-ba4c-31e22a83cddc \\\n",
       "    --set ociRegistry.username=$OCI_USERNAME \\\n",
       "    --set ociRegistry.password=$OCI_PASSWORD</pre></td></tr></table>\n",
       "                      <br />\n",
       "                      <i>\n",
       "                          Note: Please set the <code>HELM_INSTALL_NAME</code>, <code>HELM_INSTALL_NAMESPACE</code>,\n",
       "                          <code>OCI_USERNAME</code>, and <code>OCI_PASSWORD</code> environment variables.\n",
       "                      </i>\n",
       "                  </td>\n",
       "              </tr>\n",
       "              \n",
       "          </table>\n",
       "        "
      ],
      "text/plain": [
       "PipelinePublish(created_at=datetime.datetime(2025, 5, 8, 19, 42, 16, 92419, tzinfo=tzutc()), docker_run_variables={'PIPELINE_URL': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc'}, engine_config={'engine': {'resources': {'limits': {'cpu': 1.0, 'memory': '1Gi'}, 'requests': {'cpu': 1.0, 'memory': '1Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': False}}, 'engineAux': {'autoscale': {'type': 'none', 'cpu_utilization': 50.0}, 'images': {'vllm-llama31-8b-async-demo-443': {'resources': {'limits': {'cpu': 1.0, 'memory': '10Gi'}, 'requests': {'cpu': 1.0, 'memory': '10Gi'}, 'accel': 'cuda', 'arch': 'x86', 'gpu': True}}}}}, id=36, pipeline_name='llama-31-8b-vllm-demo', pipeline_version_id=1000, replaces=[], status='Published', updated_at=datetime.datetime(2025, 5, 8, 19, 42, 16, 92419, tzinfo=tzutc()), user_images=[], created_by='fbddff9d-4916-4f46-a906-0be1b372a9f2', created_on_version='2025.1.0', edge_bundles=<wallaroo.wallaroo_ml_ops_api_client.types.Unset object at 0x7b56a36de6e0>, engine_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/engines/proxy/wallaroo/ghcr.io/wallaroolabs/fitzroy-mini-cuda:v2025.1.0-main-6132', error=None, helm={'reference': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts@sha256:af38b73f10fbf6d9da318568d86383b762dee766547a35c30dccf5f7907695e1', 'values': {}, 'chart': 'us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/charts/llama-31-8b-vllm-demo', 'version': '0.0.1-a5b7a202-9923-4d8d-ba4c-31e22a83cddc'}, pipeline_url='us-central1-docker.pkg.dev/wallaroo-dev-253816/uat/pipelines/llama-31-8b-vllm-demo:a5b7a202-9923-4d8d-ba4c-31e22a83cddc', pipeline_version_name='a5b7a202-9923-4d8d-ba4c-31e22a83cddc', workspace_id=60, workspace_name='younes.amar@wallaroo.ai - Default Workspace', additional_properties={})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.publish(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c94a2a",
   "metadata": {},
   "source": [
    "### Undeploy\n",
    "\n",
    "With the tutorial complete, the pipeline is undeployed to return the resources back to the Wallaroo environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca5100af-0149-48a9-9501-6359f76988b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>llama-31-8b-vllm-ynsv5</td></tr><tr><th>created</th> <td>2025-05-06 12:31:40.360907+00:00</td></tr><tr><th>last_updated</th> <td>2025-05-06 19:51:47.490400+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>workspace_id</th> <td>60</td></tr><tr><th>workspace_name</th> <td>younes.amar@wallaroo.ai - Default Workspace</td></tr><tr><th>arch</th> <td>x86</td></tr><tr><th>accel</th> <td>cuda</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>b82ed30f-e937-4b49-94d5-63e6e798cc4b, b0a4ab4d-28ee-4470-9391-888a486375d2, 47760536-b263-428d-a9eb-f763c84f8920, 632917ff-0ffd-49be-abca-5a69a6432f93, 18cc0cad-cf6c-4abf-9083-ee90c2e704e2</td></tr><tr><th>steps</th> <td>vllm-llama31-8b-async-ynsv5</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'llama-31-8b-vllm-ynsv5', 'create_time': datetime.datetime(2025, 5, 6, 12, 31, 40, 360907, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'vllm-llama31-8b-async-ynsv5', 'version': 'c96ec281-6c97-4529-b722-76e44d41ea9a', 'sha': '62c338e77c031d7c071fe25e1d202fcd1ded052377a007ebd18cb63eadddf838'}]}}]\"}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7147baad-0092-4c6a-8403-26147800f09d",
   "metadata": {},
   "source": [
    "This tutorial demonstrates deploying the Llama V3 Instruct LLM with continuous batching in Wallaroo with CUDA AI Acceleration.  For access to these sample models and for a demonstration of how to use Continuous Batching to improve LLM performance:\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
