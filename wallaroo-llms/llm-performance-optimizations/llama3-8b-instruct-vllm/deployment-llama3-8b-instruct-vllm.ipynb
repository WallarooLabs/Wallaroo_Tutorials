{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0cddd10-25e6-4891-bd6a-a14f81ac351e",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.1_tutorials/wallaroo-llms/llm-performance-optimizations/llama3-8b-instruct-vllm).\n",
    "\n",
    "## Llama 3 8B Instruct Inference with vLLM\n",
    "\n",
    "The following tutorial demonstrates deploying the Llama 3 8B Instruct Inference with vLLM LLM with Wallaroo.  This tutorial focuses on:\n",
    "\n",
    "* Uploading the model\n",
    "* Preparing the model for deployment.\n",
    "* Deploying the model and performing inferences.\n",
    "\n",
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)\n",
    "\n",
    "## Context\n",
    "\n",
    "This BYOP model uses the [`vLLM` library](https://github.com/vllm-project/vllm) and the **Llama 3 8B Instruct** LLM.  Once deployed, it accepts a a text prompt from the user and generates a text response appropriate to the prompt.\n",
    "\n",
    "### What is vLLM?\n",
    "\n",
    "vLLM, or Very Large Language Model serving engine, is designed to enhance the efficiency and performance of deploying large language models (LLMs). It stands out for its innovative approach utilizing a novel attention algorithm known as PagedAttention. This technology effectively organizes attention keys and values into smaller, manageable segments, significantly reducing memory usage and boosting throughput compared to traditional methods.\n",
    "\n",
    "One of the key advantages of vLLM is its ability to achieve much higher throughput: up to 24 times greater than HuggingFace Transformers, a widely-used LLM library. This capability allows for serving a larger number of users with fewer computational resources, making vLLM an attractive option for organizations looking to optimize their LLM deployments.\n",
    "\n",
    "## Model Overview\n",
    "\n",
    "The LLM used in this demonstrates has the following attributes.\n",
    "\n",
    "* Framework: `vllm` for more optimized model deployment, uploaded to Wallaroo in the [Wallaroo Custom Model aka Bring Your Own Predict (BYOP) Framework](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/wallaroo-sdk-model-arbitrary-python/).\n",
    "* Artifacts:  The original model is here the Llama 3 8B Instruct Hugging Face model:[Llama 3 8B Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "* Input/Output Types:  Both the input and outputs are text.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "For our sample vLLM, the original model is encapsulated in the Wallaroo BYOP framework with the following adjustments.\n",
    "\n",
    "#### vLLM Library Installation\n",
    "\n",
    "To run [vLLM](https://github.com/vllm-project/vllm) on CUDA, a specific `vLLM` Python wheel is used with an extra index to install the proper library.  To accommodate this, the following `pip install` code is executed directly in the BYOP Python script to install the `vLLM` via the `subprocess` library:\n",
    "\n",
    "``` python\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "pip_command = (\n",
    "    f'{sys.executable} -m pip install https://github.com/vllm-project/vllm/releases/download/v0.5.2/vllm-0.5.2+cu118-cp310-cp310-manylinux1_x86_64.whl --extra-index-url https://download.pytorch.org/whl/cu118'\n",
    ")\n",
    "\n",
    "subprocess.check_call(pip_command, shell=True)\n",
    "```\n",
    "\n",
    "#### Model Loading\n",
    "\n",
    "Loading the model uses `vLLM` with the original model weights. The model weights that are found [on the Llama 3 8B Instruct model page](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n",
    "\n",
    "```python\n",
    "def _load_model(self, model_path):\n",
    "    llm = LLM(\n",
    "        model=f\"{model_path}/artifacts/Meta-Llama-3-8B-Instruct/\"\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4ceab-98b1-4f2a-a0b5-680770727c32",
   "metadata": {},
   "source": [
    "## Tutorial Steps\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "We start by importing the required libraries.  This includes the following:\n",
    "\n",
    "* [Wallaroo SDK](https://pypi.org/project/wallaroo/):  Used to upload and deploy the model in Wallaroo.\n",
    "* [pyarrow](https://pypi.org/project/pyarrow/):  Models uploaded to Wallaroo are defined in the input/output format.\n",
    "* [pandas](https://pypi.org/project/pandas/):  Data is submitted to models deployed in Wallaroo as either Apache Arrow Table format or pandas Record Format as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09da43e6-9e0d-483b-8855-61f22ede7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.pipeline   import Pipeline\n",
    "from wallaroo.deployment_config import DeploymentConfigBuilder\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcbef68",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is set through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32932fea-5190-4e36-aca5-69ca0fc629a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ae08f-47cf-4372-9e44-5c343b0e1761",
   "metadata": {},
   "source": [
    "### Upload the Model\n",
    "\n",
    "For this example, the model is uploaded via the [Wallaroo MLOps API](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-api-guide/).  To save time, we use the [Wallaroo MLOps Upload Generate Command](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/#wallaroo-mlops-upload-generate-command) from the Wallaroo SDK method `wallaroo.client.Client.generate_upload_model_api_command`.  This generates a `curl` script for uploading models to Wallaroo via the Wallaroo MLOps API, and takes the following parameters:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| **base_url** | *String* (*Required*) | The Wallaroo domain name.  For example:  `wallaroo.example.com`. |\n",
    "| **name** | *String* (*Required*) | The name to assign the model at upload.  This must match DNS naming conventions. |\n",
    "| **path** | *String* (*Required*) | Path to the ML or LLM model file. |\n",
    "| **framework** | *String* (*Required*) | The framework from `wallaroo.framework.Framework`  For a complete list, see [Wallaroo Supported Models]({{<ref \"wallaroo-model-operations-upload-register#wallaroo-supported-models\">}}). |\n",
    "| **input_schema** |  *String* (*Required*) | The model’s input schema in PyArrow.Schema format. |\n",
    "| **output_schema** |  *String* (*Required*) | The model’s output schema in PyArrow.Schema format. |\n",
    "\n",
    "This generates an output similar to the following, used to upload the model via the Wallaroo MLops API.\n",
    "\n",
    "```bash\n",
    "curl --progress-bar -X POST \\\n",
    "    -H \"Content-Type: multipart/form-data\" \\\n",
    "    -H \"Authorization: Bearer abcdefg\" \\\n",
    "    -F \"metadata={\"name\": \"byop-llama-8b-v2\", \"visibility\": \"private\", \"workspace_id\": 8, \"conversion\": {\"arch\": \"x86\", \"accel\": \"none\", \"framework\": \"custom\", \"python_version\": \"3.8\", \"requirements\": []}, \"input_schema\": \"/////3AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAQAAAB0ZXh0AAAAAAQABAAEAAAA\", \"output_schema\": \"/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA=\"};type=application/json\" \\\n",
    "    -F \"file=@byop-llama3-8b-instruct-vllm.zip;type=application/octet-stream\"\n",
    "    https://doc-test.wallaroocommunity.ninja/v1/api/models/upload_and_convert\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa35c88e-395d-4c89-bf2d-233f3183639e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'curl --progress-bar -X POST            -H \"Content-Type: multipart/form-data\"            -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICI1aUdMclZ1NVluOE1nOU5xSDQtZGdJRXBQQTJqbVRYMHFaWlJQYXZpS2tJIn0.eyJleHAiOjE3Mjk2MTUyOTksImlhdCI6MTcyOTYxNTIzOSwianRpIjoiYTk1MjYzMTAtYzY0Mi00ZTA2LWJkZGMtNDgyM2YwYWI1YWNhIiwiaXNzIjoiaHR0cHM6Ly9kb2MtdGVzdC53YWxsYXJvb2NvbW11bml0eS5uaW5qYS9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJhdWQiOlsibWFzdGVyLXJlYWxtIiwiYWNjb3VudCJdLCJzdWIiOiIzZWVhYjU1NC1mYzJlLTQxYWMtOGI0ZS0wZDc3OGU4YTQ3MWIiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJzZGstY2xpZW50Iiwic2Vzc2lvbl9zdGF0ZSI6ImI0NWI4ZmRjLWNmY2YtNGQ3ZC04NmVhLTU2MTJjNjY2NDBmNSIsImFjciI6IjEiLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsiY3JlYXRlLXJlYWxtIiwiZGVmYXVsdC1yb2xlcy1tYXN0ZXIiLCJvZmZsaW5lX2FjY2VzcyIsImFkbWluIiwidW1hX2F1dGhvcml6YXRpb24iXX0sInJlc291cmNlX2FjY2VzcyI6eyJtYXN0ZXItcmVhbG0iOnsicm9sZXMiOlsidmlldy1yZWFsbSIsInZpZXctaWRlbnRpdHktcHJvdmlkZXJzIiwibWFuYWdlLWlkZW50aXR5LXByb3ZpZGVycyIsImltcGVyc29uYXRpb24iLCJjcmVhdGUtY2xpZW50IiwibWFuYWdlLXVzZXJzIiwicXVlcnktcmVhbG1zIiwidmlldy1hdXRob3JpemF0aW9uIiwicXVlcnktY2xpZW50cyIsInF1ZXJ5LXVzZXJzIiwibWFuYWdlLWV2ZW50cyIsIm1hbmFnZS1yZWFsbSIsInZpZXctZXZlbnRzIiwidmlldy11c2VycyIsInZpZXctY2xpZW50cyIsIm1hbmFnZS1hdXRob3JpemF0aW9uIiwibWFuYWdlLWNsaWVudHMiLCJxdWVyeS1ncm91cHMiXX0sImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoicHJvZmlsZSBlbWFpbCBvcGVuaWQiLCJzaWQiOiJiNDViOGZkYy1jZmNmLTRkN2QtODZlYS01NjEyYzY2NjQwZjUiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiaHR0cHM6Ly9oYXN1cmEuaW8vand0L2NsYWltcyI6eyJ4LWhhc3VyYS11c2VyLWlkIjoiM2VlYWI1NTQtZmMyZS00MWFjLThiNGUtMGQ3NzhlOGE0NzFiIiwieC1oYXN1cmEtdXNlci1lbWFpbCI6ImpvaG4uaGFuc2FyaWNrQHdhbGxhcm9vLmFpIiwieC1oYXN1cmEtZGVmYXVsdC1yb2xlIjoiYWRtaW5fdXNlciIsIngtaGFzdXJhLWFsbG93ZWQtcm9sZXMiOlsidXNlciIsImFkbWluX3VzZXIiXSwieC1oYXN1cmEtdXNlci1ncm91cHMiOiJ7fSJ9LCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJqb2huLmhhbnNhcmlja0B3YWxsYXJvby5haSIsImVtYWlsIjoiam9obi5oYW5zYXJpY2tAd2FsbGFyb28uYWkifQ.L4i8bVauByo8eb0j7-KDUyPrvZSUKUm4_smh1SIb3WtyERVwHY0-qcKPewJtFK16KnRheUDfhZ60Z-mPQUasTVQkZMajElEq0cEOAbTsXHAvi9kgQbUYkpoKOaDcBuylrqMwsYe4aACZPav1nGTsq-vWn4mR2YLofm7fd81emCDbm6ufIjjZV38pfFNIVQIH1O_ownjATRoYx2Lt7j1kGpOz3AF4EZsD6gPNBqlnxVubTpq144ymX9J5Etq5zdiIfhaOsYMre_FzhZYllIYItDc9hJ0B6ROpd9vawHmqCXxBj7Mn5O62Q9Qesh4C9t8KN4egIhTTkWeSuJHgi7yLoQ\"            -F \"metadata={\"name\": \"byop-llama-8b-v2\", \"visibility\": \"private\", \"workspace_id\": 8, \"conversion\": {\"arch\": \"x86\", \"accel\": \"none\", \"framework\": \"custom\", \"python_version\": \"3.8\", \"requirements\": []}, \"input_schema\": \"/////3AAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAQAAAB0ZXh0AAAAAAQABAAEAAAA\", \"output_schema\": \"/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAA4AAABnZW5lcmF0ZWRfdGV4dAAABAAEAAQAAAA=\"};type=application/json\"            -F \"file=@byop-llama3-8b-instruct-vllm.zip;type=application/octet-stream\"        https://doc-test.wallaroocommunity.ninja/v1/api/models/upload_and_convert'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the input and output schemas\n",
    "\n",
    "import wallaroo.framework\n",
    "\n",
    "\n",
    "input_schema = pa.schema([\n",
    "    pa.field(\"text\", pa.string()),\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field(\"generated_text\", pa.string()),\n",
    "])\n",
    "\n",
    "# generate the curl command and execute it\n",
    "\n",
    "wl.generate_upload_model_api_command(\n",
    "    base_url=wl.api_endpoint,\n",
    "    name=\"byop-llama-8b-v2\",\n",
    "    path=\"byop-llama3-8b-instruct-vllm.zip\",\n",
    "    framework=wallaroo.framework.Framework.CUSTOM,\n",
    "    input_schema=input_schema,\n",
    "    output_schema=output_schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5033e290-d466-487f-93df-93af9d49bdfe",
   "metadata": {},
   "source": [
    "### Retrieve the Model\n",
    "\n",
    "Once the model is uploaded, we retrieve it through the `wallaroo.client.Client.get_model` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d3c67-3c3b-4630-8d02-933d8507a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = wl.get_model(\"byop-llama-8b-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705dc16-f70b-4afc-a6c8-9f693c887385",
   "metadata": {},
   "source": [
    "### Deploy the LLM\n",
    "\n",
    "The LLM is deployed through the following process:\n",
    "\n",
    "* Define the Deployment Configuration:  This sets what resources are allocated for the LLM's use from the clusters.\n",
    "* Create a Wallaroo Pipeline and Set the LLM as a Pipeline Step:  This sets the process for how inference inputs is passed through deployed LLMs and supporting ML models.\n",
    "* Deploy the LLM:  This deploys the LLM with the defined deployment configuration and pipeline steps.\n",
    "\n",
    "#### Define the Deployment Configuration\n",
    "\n",
    "For this step, the following resources are defined for allocation to the LLM when deployed through the class `wallaroo.deployment_config.DeploymentConfigBuilder`:\n",
    "\n",
    "* Cpus:  4\n",
    "* Memory:  10 Gi\n",
    "* Gpus: 1.  When setting `gpus` for deployment, the `deployment_label` must be defined to select the appropriate nodepool with the requested gpu resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499212ba-7a6a-4f06-a59d-908c7fefe657",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.deployment_config.DeploymentConfigBuilder() \\\n",
    "    .cpus(1).memory('2Gi') \\\n",
    "    .sidekick_cpus(llm, 4) \\\n",
    "    .sidekick_memory(llm, '10Gi') \\\n",
    "    .sidekick_gpus(llm, 1) \\\n",
    "    .deployment_label(\"wallaroo.ai/accelerator:a1002\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abe8a9",
   "metadata": {},
   "source": [
    "#### Create Pipeline and Steps\n",
    "\n",
    "In this step, the Wallaroo pipeline is established with the LLM set as the pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c0aa40-d037-4874-b662-7950cc120270",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"vllm-pipe-v9\")\n",
    "pipeline.add_model_step(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702eedc",
   "metadata": {},
   "source": [
    "#### Deploy the LLM\n",
    "\n",
    "With the Wallaroo pipeline created and the deployment configuration set, we deploy the LLM and set the deployment configuration to allocate the appropriate resources for the LLM's exclusive use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fca07c-1950-41cb-a81a-1f91281fd3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967709f8-b1e6-4fef-9b09-e9fe1678787f",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "Inference requests are submitted to deployed LLM's either as Apache Arrow Tables or pandas DataFrames.  For this example, a pandas DataFrame is submitted through the `wallaroo.pipeline.Pipeline.infer` method.\n",
    "\n",
    "For this example, the `start` and `end` time is collected to determine how long the inference request took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef741c36-a98a-411b-a1a7-0d6a5bc28700",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"text\": [\"Tell me about XAI.\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55717d7d-8061-47d4-bf7c-7b5b1cd09940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c59f9e-30a8-42b0-b1d2-8e7f1a81348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "result = pipeline.infer(data, timeout=10000)\n",
    "end = time.time()\n",
    "\n",
    "end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d4c23-dd73-4d5c-8842-59a9eb9bc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"out.generated_text\"].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0684f59-f9b0-4396-9850-6e4984d7b4df",
   "metadata": {},
   "source": [
    "### Undeploy the LLM\n",
    "\n",
    "With the example completed, the LLM is undeployed and the resources returned to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906949e3-b9f2-430f-920f-04caf8613011",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb9b4d-3cec-4292-a3b6-b8bb09472488",
   "metadata": {},
   "source": [
    "For access to these sample models and for a demonstration of how to use a LLM Validation Listener.\n",
    "\n",
    "* Contact your Wallaroo Support Representative **OR**\n",
    "* [Schedule Your Wallaroo.AI Demo Today](https://wallaroo.ai/request-a-demo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2024.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
