{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a6b7ac3",
   "metadata": {},
   "source": [
    "The following tutorial is available on the [Wallaroo Github Repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/20231004-wallaroo-inference-server/wallaroo-inference-server-tutorials/wallaroo-inference-server-hf-summarizer).\n",
    "\n",
    "## Wallaroo Inference Server:  Hugging Face Summarizer\n",
    "\n",
    "This notebook is used in conjunction with the [Wallaroo Inference Server Free Edition](https://docs.wallaroo.ai/wallaroo-inferencing-server/) for LLama 2.  This provides a free license for performing inferences through the Hugging Face Summarizer model.  For more information, see [the Llama 2 reference page](https://ai.meta.com/llama/).\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* A deployed Wallaroo Inference Server Free Edition with one of the following options:\n",
    "  * **Wallaroo.AI Llama Inference Server - GPU**\n",
    "* Access via port 8080 to the Wallaroo Inference Server Free Edition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b117715d",
   "metadata": {},
   "source": [
    "## Llama 2 Model Schemas\n",
    "\n",
    "### Inputs\n",
    "\n",
    "The Llama 2 Model takes the following inputs.\n",
    "\n",
    "| Field | Type | Description |\n",
    "|---|---|---|\n",
    "| `text` | String (*Required*) | The prompt for the llama model. |\n",
    "\n",
    "### Outputs\n",
    "\n",
    "| Field | Type | Description |\n",
    "|---|---|---|\n",
    "| `generated_text` | String | The generated text output. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e2edc",
   "metadata": {},
   "source": [
    "## Wallaroo Inference Server API Endpoints\n",
    "\n",
    "The following HTTPS API endpoints are available for Wallaroo Inference Server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4659e8",
   "metadata": {},
   "source": [
    "### Pipelines Endpoint\n",
    "\n",
    "* Endpoint: HTTPS GET `/pipelines`\n",
    "* Returns:\n",
    "  * List of `pipelines` with the following fields.\n",
    "    * **id** (*String*): The name of the pipeline.\n",
    "    * **status** (*String*): The pipeline status.  `Running` indicates the pipeline is available for inferences.\n",
    "\n",
    "#### Pipeline Endpoint Example\n",
    "\n",
    "The following demonstrates using `curl` to retrieve the Pipelines endpoint.  Replace the HOSTNAME with the address of your Wallaroo Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9267efdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"pipelines\":[{\"id\":\"llama\",\"status\":\"Running\"}]}"
     ]
    }
   ],
   "source": [
    "!curl HOSTNAME:8080/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d04d17c",
   "metadata": {},
   "source": [
    "### Models Endpoint\n",
    "\n",
    "* Endpoint: GET `/models`\n",
    "* Returns:\n",
    "  * List of `models` with the following fields.\n",
    "    * **name** (*String*):  The name of the model.\n",
    "    * **sha** (*String*):  The `sha` hash of the model.\n",
    "    * **status** (*String*):  The model status.  `Running` indicates the models is available for inferences.\n",
    "    * **version** (*String*): The model version in UUID format.\n",
    "\n",
    "#### Models Endpoint Example\n",
    "\n",
    "The following demonstrates using `curl` to retrieve the Models endpoint.  Replace the HOSTNAME with the address of your Wallaroo Inference Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eaa60fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"models\":[{\"name\":\"llama\",\"sha\":\"0bf8b42da8d35dac656048c53230d8d645abdbef281ec5d230fd80aef18aec95\",\"status\":\"Running\",\"version\":\"5291a743-5c38-4448-8122-bd5edec73011\"}]}"
     ]
    }
   ],
   "source": [
    "!curl HOSTNAME:8080/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8e6c2",
   "metadata": {},
   "source": [
    "### Inference Endpoint\n",
    "\n",
    "The following inference endpoint is available from the Wallaroo Server for HuggingFace Summarizer.\n",
    "\n",
    "* Endpoint: HTTPS POST `/pipelines/hf-summarizer-standard`\n",
    "* Headers:\n",
    "  * `Content-Type: application/vnd.apache.arrow.file`: For Apache Arrow tables.\n",
    "  * `Content-Type: application/json; format=pandas-records`: For pandas DataFrame in record format.\n",
    "* Input Parameters: DataFrame in `/pipelines/hf-summarizer-standard` **OR** Apache Arrow table in `application/vnd.apache.arrow.file` with the following inputs:\n",
    "  * **text** (*String* *Required*): The text prompt.\n",
    "* Returns:\n",
    "  * Headers\n",
    "    * `Content-Type: application/json; format=pandas-records`: pandas DataFrame in record format.\n",
    "  * Data\n",
    "    * **check_failures** (*List[Integer]*): Whether any validation checks were triggered.  For more information, see [Wallaroo SDK Essentials Guide: Pipeline Management: Anomaly Testing]({{<ref \"wallaroo-sdk-essentials-pipeline#anomaly-testing\">}}).\n",
    "    * **elapsed** (*List[Integer]*): A list of time in nanoseconds for:\n",
    "    * [0] The time to serialize the input.\n",
    "    * [1...n] How long each step took.\n",
    "    * **model_name** (*String*): The name of the model used.\n",
    "    * **model_version** (*String*): The version of the model in UUID format.\n",
    "    * **original_data**: The original input data.  Returns `null` if the input may be too long for a proper return.\n",
    "    * **outputs** (*List*): The outputs of the inference result separated by data type.\n",
    "    * **String**: The string outputs for the inference.\n",
    "      * **data** (*List[String]*): The generated text from the prompt.\n",
    "        * **dim** (*List[Integer]*): The dimension shape returned, always returned as `[1,1]` for this model deployment.\n",
    "        * **v** (*Integer*): The vector shape of the data, always returned as `1` for this mnodel deployment.\n",
    "    * **pipeline_name**  (*String*): The name of the pipeline.\n",
    "    * **shadow_data**: Any shadow deployed data inferences in the same format as **outputs**.\n",
    "    * **time** (*Integer*): The time since UNIX epoch.\n",
    "\n",
    "### Inference Endpoint Example\n",
    "\n",
    "The following example performs an inference using the pandas record input `./data/test_summarization.df.json` with a text string to summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb23c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST HOSTNAME:8080/pipelines/llama \\\n",
    "    -H \"Content-Type: application/json; format=pandas-records\" \\\n",
    "    -d '[{\"text\":\"What is a number that can divide 0 evenly?\"}]'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
