{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6540f10a-6e1c-44ba-b344-0a476491dce6",
   "metadata": {},
   "source": [
    "# Workshop Notebook 3: Observability Part 1 - Validation Rules\n",
    "\n",
    "In the previous notebooks you uploaded the models and artifacts, then deployed the models to production through provisioning workspaces and pipelines. Now you're ready to put your feet up! But to keep your models operational, your work's not done once the model is in production. You must continue to monitor the behavior and performance of the model to insure that the model provides value to the business.\n",
    "\n",
    "In this notebook, you will learn about adding validation rules to pipelines.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "In the blocks below we will preload some required libraries; we will also redefine some of the convenience functions that you saw in the previous notebooks.\n",
    "\n",
    "After that, you should log into Wallaroo and set your working environment to the workspace that you created for this workshop. Please refer to Notebook 1 to refresh yourself on how to log in and set your working environment to the appropriate workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0f316-7000-467e-b5d2-1a1c4e18d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload needed libraries \n",
    "\n",
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# used to display DataFrame information without truncating\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# used for unique connection names\n",
    "\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c19cb-2b9b-4baf-bbf1-9fcfc47b4339",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convenience functions from the previous notebooks\n",
    "## these functions assume your connection to wallaroo is called wl\n",
    "\n",
    "# return the workspace called <name>, or create it if it does not exist.\n",
    "# this function assumes your connection to wallaroo is called wl\n",
    "def get_workspace(name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "\n",
    "# pull a single datum from a data frame \n",
    "# and convert it to the format the model expects\n",
    "def get_singleton(df, i):\n",
    "    singleton = df.iloc[i,:].to_numpy().tolist()\n",
    "    sdict = {'tensor': [singleton]}\n",
    "    return pd.DataFrame.from_dict(sdict)\n",
    "\n",
    "\n",
    "# pull a batch of data from a data frame\n",
    "# and convert to the format the model expects\n",
    "def get_batch(df, first=0, nrows=1):\n",
    "    last = first + nrows\n",
    "    batch = df.iloc[first:last, :].to_numpy().tolist()\n",
    "    return pd.DataFrame.from_dict({'tensor': batch})\n",
    "\n",
    "\n",
    "# Get the most recent version of a model in the workspace\n",
    "# Assumes that the most recent version is the first in the list of versions.\n",
    "# wl.get_current_workspace().models() returns a list of models in the current workspace\n",
    "\n",
    "def get_model(mname):\n",
    "    modellist = wl.get_current_workspace().models()\n",
    "    model = [m.versions()[0] for m in modellist if m.name() == mname]\n",
    "    if len(model) <= 0:\n",
    "        raise KeyError(f\"model {mname} not found in this workspace\")\n",
    "    return model[0]\n",
    "\n",
    "# get a pipeline by name in the workspace\n",
    "def get_pipeline(pname):\n",
    "    plist = wl.get_current_workspace().pipelines()\n",
    "    pipeline = [p for p in plist if p.name() == pname]\n",
    "    if len(pipeline) <= 0:\n",
    "        raise KeyError(f\"pipeline {pname} not found in this workspace\")\n",
    "    return pipeline[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca3872-038b-4851-bee2-069799ac0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to log in and go to correct workspace\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55ffbcff-cd3c-48b7-9cd5-88119b482f40",
   "metadata": {},
   "source": [
    "## Model Validation Rules\n",
    "\n",
    "A simple way to try to keep your model's behavior up to snuff is to make sure that it receives inputs that it expects, and that its output is something that downstream systems can handle. This can entail specifying rules that document what you expect, and either enforcing these rules (by refusing to make a prediction), or at least logging an alert that the expectations described by your validation rules have been violated. As the developer of the model, the data scientist (along with relevant subject matter experts) will often be the person in the best position to specify appropriate validation rules.\n",
    "\n",
    "In our house price prediction example, suppose you know that house prices in your market are typically in the range $750,000 to $1.5M dollars. Then you might want to set validation rules on your model pipeline to specify that you expect the model's predictions to also be in that range. Then, if the model predicts a value outside that range, the pipeline will log that one of the validation checks has failed; this allows you to investigate that instance further.\n",
    "\n",
    "Note that in this specific example, a model prediction outside the specified range may not necessarily be \"wrong\"; but out-of-range predictions are likely unusual enough that you may want to \"sanity-check\" the model's behavior in these situations.\n",
    "\n",
    "Wallaroo has functionality for specifying simple validation rules on model input and output values.\n",
    "\n",
    "```\n",
    "pipeline.add_validation(<rulename>, <expression>)\n",
    "\n",
    "```\n",
    "Here, `<rulename>` is the name of the rule, and `<expression>` is a simple logical expression that the data scientist expects to be true.\n",
    "\n",
    "More complex validation rules can be implemented via custom pre- or post- processing modules.\n",
    "\n",
    "To add a validation step to a simple one-step pipeline, you need a handle to the pipeline (here called `pipeline`), and a handle to the model in the pipeline (here called `model`).  Then you can specify an expected prediction range as follows:\n",
    "\n",
    "```\n",
    "# get the existing pipeline and undeploy it\n",
    "pipeline = wl.pipelines_by_name(\"pipeline\")[0]\n",
    "pipeline.undeploy()\n",
    "\n",
    "# you also need a handle to the model in this single-step pipeline.\n",
    "# here are two ways to do it:\n",
    "#\n",
    "# (1) If you know the name of the model, you can also just use the get_model() convenience function above.\n",
    "# In this example, the model has been uploaded to wallaroo with the name \"mymodel\"\n",
    "\n",
    "model = get_model(\"mymodel\") \n",
    "\n",
    "# (2) To get the model without knowing its name (for a single-step pipeline)\n",
    "model = pipeline.model_configs()[0].model()\n",
    "\n",
    "\n",
    "# specify the bounds\n",
    "hi_bnd = 1500000.0 # 1.5M\n",
    "\n",
    "#\n",
    "# some examples of validation rules\n",
    "#\n",
    "\n",
    "# (1)  validation rule: prediction should be < 1.5 million>\n",
    "pipeline = pipeline.add_validation(\"less than 1.5m\", model.outputs[0][0] < hi_bnd)\n",
    "\n",
    "pipeline.deploy()\n",
    "\n",
    "```\n",
    "\n",
    "When data is passed to the pipeline for inferences, the pipeline will log a check failure whenever one of the validation expressions evaluates to false. Here are some examples of inference results from a pipeline with the validation rule `model.outputs[0][0] < 35.0`.\n",
    "\n",
    "**TODO: SCREEN SHOT HERE, FROM ANOMALY DETECTION TUTORIAL**\n",
    "\n",
    "\n",
    "You can also find check failures in the logs:\n",
    "\n",
    "```\n",
    "logs = pipeline.logs()\n",
    "logs.loc[logs['check_failures'] > 0]\n",
    "```\n",
    "<hr/>\n",
    "\n",
    "#### Exercise: Add validation rules to your model pipeline\n",
    "\n",
    "Add some simple validation rules to the model pipeline that you created in a previous exercise.\n",
    "\n",
    "* Add an upper bound or a lower bound to the model predictions\n",
    "* Try to create predictions that fall both in and out of the specified range\n",
    "* Look through the logs to find the check failures.\n",
    "\n",
    "**HINT 1**: since the purpose of this exercise is try out validation rules, it might be a good idea to take a small data set and make predictions on that data set first, *then* set the validation rules based on those predictions, so that you can see the check failures trigger.\n",
    "\n",
    "**Don't forget to undeploy your pipeline after you are done**, to free up resources.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b7fb3b-4e56-4ba8-bb39-82ac75512534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## blank space to get your pipeline and run a small batch of data through it to see the range of predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f37a34-6caf-47a8-b569-517c17ba1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# blank space to set a validation rule on the pipeline and check if it triggers as expected\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6179b95-b0bc-4057-80ee-aa8d543f40ac",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "In this workshop you have\n",
    "\n",
    "* Set a validation rule on your house price prediction pipeline.\n",
    "* Detected model predictions that failed the validation rule.\n",
    "\n",
    "In the next notebook, you will learn how to monitor the distribution of model outputs for drift away from expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a5223-b9f6-4891-ab93-00c113b578ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
