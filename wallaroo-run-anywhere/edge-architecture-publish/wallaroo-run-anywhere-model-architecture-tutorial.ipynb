{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.1_tutorials/wallaroo-run-anywhere/edge-architecture-publish).\n",
    "\n",
    "## Run Anywhere for ARM Architecture Tutorial: Preparation\n",
    "\n",
    "Wallaroo Run Anywhere provides model deployment in any device, any cloud, and any architecture.  Models uploaded to Wallaroo are set to their targeted architecture.  By default, this is `X64`.\n",
    "\n",
    "The model's deployment configuration is tied to its architecture settings.  That deployment configuration is carried over to the models publication in Open Container Initiative (OCI) Registries, which allows edge model deployments on X64 and ARM architectures.\n",
    "\n",
    "This tutorial demonstrates deploying a ML model trained to predict house prices to X64 and ARM edge locations through the following steps.\n",
    "\n",
    "* Setting up a workspace, and pipeline.\n",
    "* Upload a model set to the X64 architecture, and the same model set to the ARM architecture.\n",
    "* Performing a sample set of inferences to verify the deployment.\n",
    "* Publish the deployed model to an Open Container Initiative (OCI) Registry for both X64 and ARM deployments.\n",
    "* Deploy the model to both X64 and ARM edge devices.\n",
    "* Perform similar inferences on the edge devices for each architecture and show the results.\n",
    "\n",
    "In this notebook, we use a ONNX model pre-trained to predict house prices for our examples.\n",
    "\n",
    "## Goal\n",
    "\n",
    "Model insights monitors the output of the house price model over a designated time window and compares it to an expected baseline distribution. We measure the performance of model deployments in different locations and compare that to the baseline to detect model drift.\n",
    "\n",
    "### Resources\n",
    "\n",
    "This tutorial provides the following:\n",
    "\n",
    "* Models:\n",
    "  * `models/rf_model.onnx`: The champion model trained to predict house prices.\n",
    "  * Various inputs:\n",
    "    * `smallinputs.df.json`: A set of house inputs that tends to generate low house price values.\n",
    "    * `biginputs.df.json`: A set of house inputs that tends to generate high house price values.\n",
    "    * `normal-inputs.df.json`: A set of house inputs with a range of house values.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* A deployed Wallaroo instance with [Edge Registry Services](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-edge-deployment/#enable-wallaroo-edge-deployment-registry) and [Edge Observability enabled](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-edge-deployment/#set-edge-observability-service).\n",
    "* The following Python libraries installed:\n",
    "  * [`wallaroo`](https://pypi.org/project/wallaroo/): The Wallaroo SDK. Included with the Wallaroo JupyterHub service by default.\n",
    "  * [`pandas`](https://pypi.org/project/pandas/): Pandas, mainly used for Pandas DataFrame\n",
    "  * `json`: Used for format input data for inference requests.\n",
    "* A X64 Docker deployment to deploy the model on an edge location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "* Upload the house price model twice:  The first model version sets the model to the `X64` architecture, the second version is set to the `ARM` architecture.\n",
    "* Create two pipeline versions:\n",
    "  * The first pipeline version uses the model with the `X64` targeted architecture.  This demonstrates that the pipeline deployment configuration inherits the model's `X64` setting.\n",
    "  * The first pipeline version uses the model with the `ARM` targeted architecture.  This demonstrates that the pipeline deployment configuration inherits the model's `ARM` setting.\n",
    "* Publish both pipeline versions to an OCI registry.\n",
    "* Deploy the models from the pipeline versions to their respective architecture.\n",
    "* Perform sample inferences on both deployed model architectures.\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "The first step will be to import our libraries, and set variables used through this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# used to display DataFrame information without truncating\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "workspace_name = f'run-anywhere-house-price-architecture-demonstration-tutorial'\n",
    "main_pipeline_name = f'architecture-demonstration'\n",
    "model_name_control = f'house-price-estimator'\n",
    "model_file_name_control = './models/rf_model.onnx'\n",
    "\n",
    "# ignoring warnings for demonstration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# used to display DataFrame information without truncating\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_workspace(name, client):\n",
    "    workspace = None\n",
    "    for ws in client.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = client.create_workspace(name)\n",
    "    return workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login through local Wallaroo instance\n",
    "\n",
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "We will create a workspace to manage our pipeline and models.  The following variables will set the name of our sample workspace then set it as the current workspace.\n",
    "\n",
    "Workspace, pipeline, and model names should be unique to each user, so we'll add in a randomly generated suffix so multiple people can run this tutorial in a Wallaroo instance without effecting each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'run-anywhere-assay-demonstration-tutorial', 'id': 39, 'archived': False, 'created_by': 'b4a9aa3d-83fc-407a-b4eb-37796e96f1ef', 'created_at': '2024-02-28T17:07:43.828688+00:00', 'models': [], 'pipelines': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = get_workspace(workspace_name, wl)\n",
    "\n",
    "wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload the Model with Targeted Architecture\n",
    "\n",
    "For our example, we will upload the champion model that has been trained to derive house prices from a variety of inputs.  The model file is `rf_model.onnx`, and is uploaded with the name `house-price-estimator`.\n",
    "\n",
    "Models are uploaded to Wallaroo via the [`wallaroo.client.upload_model`](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-reference-guide/client/#Client.upload_model) method which takes the following arguments:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| **path** | *String* (*Required*) | The file path to the model. |\n",
    "| **framework** | *wallaroo.framework.Framework* (*Required*) | The model's framework.  See [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) for supported model frameworks. |\n",
    "| **input_schema** | *pyarrow.lib.Schema* (*Optional*)  | The model's input schema.  **Only required for non-Native Wallaroo frameworks.  See [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) for more details. |\n",
    "| **output_schema** | *pyarrow.lib.Schema* (*Optional*)  | The model's output schema.  **Only required for non-Native Wallaroo frameworks.  See [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) for more details. |\n",
    "| **convert_wait** | *bool* (*Optional*)  | Whether to wait in the SDK session to complete the auto-packaging process for non-native Wallaroo frameworks. |\n",
    "| **arch** | *wallaroo.engine_config.Architecture* (*Optional*)  | The targeted architecture for the model.  Options are <ol><li>`X86` (*Default*)</li><li>`ARM`</li></ol> |\n",
    "\n",
    "We upload two versions of of the model:  the first set to the `X86` architecture, the second to the `ARM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_model_control_x64 = (wl.upload_model(model_name_control, \n",
    "                                        model_file_name_control, \n",
    "                                        framework=Framework.ONNX,\n",
    "                                        arch=Architecture.X86)\n",
    "                                        .configure(tensor_fields=[\"tensor\"])\n",
    "                        )\n",
    "\n",
    "housing_model_control_arm = (wl.upload_model(model_name_control, \n",
    "                                        model_file_name_control, \n",
    "                                        framework=Framework.ONNX,\n",
    "                                        arch=Architecture.ARM)\n",
    "                                        .configure(tensor_fields=[\"tensor\"])\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Pipeline\n",
    "\n",
    "This pipeline is made to be an example of an existing situation where a model is deployed and being used for inferences in a production environment.\n",
    "\n",
    "We will create two versions of the pipeline:\n",
    "\n",
    "* One version with model version set to the `X86` architecture.\n",
    "* The second version with the model version set to the `ARM` architecture.\n",
    "\n",
    "In both cases, the **model's targeted architecture** will be inherited by the pipeline version; no additional pipeline settings are required for that architecture.  Each will have a deployment configuration set to 1 replica, 1 CPU, and 1 Gb of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>assay-demonstration-tutorial</td></tr><tr><th>created</th> <td>2024-02-28 17:09:13.517363+00:00</td></tr><tr><th>last_updated</th> <td>2024-02-28 17:09:14.388029+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>arch</th> <td>None</td></tr><tr><th>accel</th> <td>None</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>84cc722a-4c2a-49fa-ab85-3b9ebd0f12af, a2ec7a67-a55f-4886-84da-9f0f5d074bb4</td></tr><tr><th>steps</th> <td>house-price-estimator</td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'assay-demonstration-tutorial', 'create_time': datetime.datetime(2024, 2, 28, 17, 9, 13, 517363, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'house-price-estimator', 'version': 'fa4be80e-dd8f-4e42-8385-e8eca9fd8500', 'sha': 'e22a0831aafd9917f3cc87a15ed267797f80e2afa12ad7d8810ca58f173b8cc6'}]}}]\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainpipeline = wl.build_pipeline(main_pipeline_name)\n",
    "# clear the steps if used before\n",
    "mainpipeline.clear()\n",
    "\n",
    "# set the model step with the x86 targeted model\n",
    "mainpipeline.add_model_step(housing_model_control_x64)\n",
    "\n",
    "mainpipeline_x86_version = mainpipeline.create_version()\n",
    "\n",
    "# clear the previous steps\n",
    "mainpipeline.clear()\n",
    "\n",
    "# set the model step with the ARM targeted model\n",
    "mainpipeline.add_model_step(housing_model_control_arm)\n",
    "\n",
    "mainpipeline_arm_version = mainpipeline.create_version()\n",
    "\n",
    "#minimum deployment config\n",
    "deploy_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(1).memory(\"1Gi\").build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Deployment\n",
    "\n",
    "We now deploy the pipeline versions to our edge devices.\n",
    "\n",
    "* Publish the pipeline versions:  Publishes the pipeline versions to the OCI registry with the deployment configuration inherited from the pipeline versions, with the architecture settings inherited from the model versions.\n",
    "* Deploy Edge:  Deploy the edge device with the edge location settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Pipeline Versions\n",
    "\n",
    "Publishing the pipeline uses the pipeline `wallaroo.pipeline.publish()` command.  This requires that the Wallaroo Ops instance have [Edge Registry Services](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-edge-deployment/#enable-wallaroo-edge-deployment-registry) enabled.\n",
    "\n",
    "The following publishes the pipeline to the OCI registry and displays the container details.  For more information, see [Wallaroo SDK Essentials Guide: Pipeline Edge Publication](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline-publication/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for pipeline publish... It may take up to 600 sec.\n",
      "Pipeline is Publishing......Published.\n"
     ]
    }
   ],
   "source": [
    "assay_pub_x86 = mainpipeline_x86_version.publish()\n",
    "assay_pub_x86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_pub_arm = mainpipeline_arm_version.publish()\n",
    "assay_pub_arm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DevOps Deployment\n",
    "\n",
    "The edge deployment is performed with `docker run`, `docker compose`, or `helm` installations.  The following command generates the `docker run` command, with the following values provided by the DevOps Engineer:\n",
    "\n",
    "* `$REGISTRYURL`: The URL of the OCI registry service hosting the pipeline publish.\n",
    "* `$OCI_USERNAME`: The username for the OCI registry service.\n",
    "* `$OCI_PASSWORD`: The password or token for the OCI registry service.\n",
    "\n",
    "For more details on model edge deployments with Wallaroo, see [Model Operations: Run Anywhere](https://staging.docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-run-anywhere/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create docker run \n",
    "\n",
    "docker_command = f'''\n",
    "docker run -p 8080:8080 \\\\\n",
    "    -e DEBUG=true \\\\\n",
    "    -e OCI_REGISTRY=$REGISTRYURL \\\\\n",
    "    -e CONFIG_CPUS=1 \\\\\n",
    "    -e OCI_USERNAME=$OCI_USERNAME \\\\\n",
    "    -e OCI_PASSWORD=$OCI_PASSWORD \\\\\n",
    "    -e PIPELINE_URL={assay_pub_x86.pipeline_url} \\\\\n",
    "    {assay_pub_x86.engine_url}\n",
    "'''\n",
    "\n",
    "print(docker_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create docker run \n",
    "\n",
    "docker_command = f'''\n",
    "docker run -p 8080:8080 \\\\\n",
    "    -e DEBUG=true \\\\\n",
    "    -e OCI_REGISTRY=$REGISTRYURL \\\\\n",
    "    -e CONFIG_CPUS=1 \\\\\n",
    "    -e OCI_USERNAME=$OCI_USERNAME \\\\\n",
    "    -e OCI_PASSWORD=$OCI_PASSWORD \\\\\n",
    "    -e PIPELINE_URL={assay_pub_arm.pipeline_url} \\\\\n",
    "    {assay_pub_arm.engine_url}\n",
    "'''\n",
    "\n",
    "print(docker_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Inference Examples\n",
    "\n",
    "The following examples demonstrate performing inferences on each of the model deployments on edge devices in different architectures.  For this example, `HOSTNAME_X86` represents the hostname for the X86 architecture edge deployment, and `HOSTNAME_ARM` is the hostname for the ARM architecture edge deployment.  Update these variables to match your deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME_X86 = 'testboy.local'\n",
    "HOSTNAME_ARM = 'localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a set of normal house values from the x86 deployment\n",
    "!curl -X POST {HOSTNAME_X86}:8080/pipelines/{main_pipeline_name} \\\n",
    "    -H \"Content-Type: Content-Type: application/json; format=pandas-records\" \\\n",
    "    --data @./data/normal-inputs.df.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a set of normal house values from the ARM deployment\n",
    "!curl -X POST {HOSTNAME_ARM}:8080/pipelines/{main_pipeline_name} \\\n",
    "    -H \"Content-Type: Content-Type: application/json; format=pandas-records\" \\\n",
    "    --data @./data/normal-inputs.df.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dda4bf3640b7fafcd1648658b879b4cc9f6ba6084e8fb356fdaaa1a461d1690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
