{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2024.1_tutorials/wallaroo-run-anywhere/edge-architecture-publish-hf-summarization-model).\n",
    "\n",
    "## Run Anywhere for ARM Architecture Tutorial: Hugging Face Summarization Model\n",
    "\n",
    "Wallaroo Run Anywhere provides model deployment in any device, any cloud, and any architecture.  Models uploaded to Wallaroo are set to their targeted architecture.  By default, this is `X64`.\n",
    "\n",
    "The model's deployment configuration is tied to its architecture settings.  That deployment configuration is carried over to the models publication in Open Container Initiative (OCI) Registries, which allows edge model deployments on X64 and ARM architectures.\n",
    "\n",
    "This tutorial demonstrates deploying a ML model trained to predict house prices to X64 and ARM edge locations through the following steps.\n",
    "\n",
    "* Setting up a workspace, and pipeline.\n",
    "* Upload a model set to the ARM architecture.\n",
    "* Deploy the model to the ARM architecture and demonstrate that the deployment configuration inherits its architecture from the model configuration.\n",
    "* Performing a sample set of inferences to verify the deployment.\n",
    "* Publish the deployed model to an Open Container Initiative (OCI) Registry ARM deployments and verify it inherits the edge deployment architecture based on the model configuration.\n",
    "* Deploy the model to an ARM edge device.\n",
    "* Perform similar inferences on the edge device and show the results.\n",
    "\n",
    "In this notebook, we use LLM Hugging Face summarization model.  The sample model is available at the following URL.  **This model should be downloaded and placed into the `./models` folder before beginning this demonstration.**\n",
    "\n",
    "[model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip (1.4 GB)](https://storage.googleapis.com/wallaroo-public-data/llm-models/model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip)\n",
    "\n",
    "## Goal\n",
    "\n",
    "Demonstrate models uploaded to Wallaroo with the targeted architecture set to ARM are deployable on ARM architecture nodepools and ARM edge devices.\n",
    "\n",
    "### Resources\n",
    "\n",
    "This tutorial provides the following:\n",
    "\n",
    "* Models:\n",
    "  * `models/model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip`: **This model should be downloaded and placed into the `./models` folder before beginning this demonstration.** [model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip (1.4 GB)](https://storage.googleapis.com/wallaroo-public-data/llm-models/model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip)\n",
    "  * Various inputs:\n",
    "    * `./data/test_summarization.df.json`: A DataFrame with sample text to summarize.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "* A deployed Wallaroo instance with [Edge Registry Services](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-edge-deployment/#enable-wallaroo-edge-deployment-registry) and [Edge Observability enabled](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-edge-deployment/#set-edge-observability-service).\n",
    "* The following Python libraries installed:\n",
    "  * [`wallaroo`](https://pypi.org/project/wallaroo/): The Wallaroo SDK. Included with the Wallaroo JupyterHub service by default.\n",
    "  * [`pandas`](https://pypi.org/project/pandas/): Pandas, mainly used for Pandas DataFrame\n",
    "  * `json`: Used for format input data for inference requests.\n",
    "* A X64 Docker deployment to deploy the model on an edge location.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "* Upload the model with the targeted architecture set to `ARM`.\n",
    "* Create the pipeline add the model as a model step.\n",
    "* Deploy the model in the targeted architecture and perform sample inferences.\n",
    "* Publish the pipeline an OCI registry.\n",
    "* Deploy the model from the pipeline publish to the edge deployment with ARM architecture.\n",
    "* Perform sample inferences on the ARM edge model deployment.\n",
    "\n",
    "### Import Libraries\n",
    "\n",
    "The first step will be to import our libraries, and set variables used through this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.framework import Framework\n",
    "from wallaroo.engine_config import Architecture\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "# used to display DataFrame information without truncating\n",
    "from IPython.display import display\n",
    "# used for input/output data schemas\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "workspace_name = f'run-anywhere-architecture-hf-summarizer-demonstration-tutorial'\n",
    "arm_pipeline_name = f'architecture-demonstration-arm'\n",
    "model_name_arm = f'hf-summarizer-arm'\n",
    "model_file_name = './models/model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip'\n",
    "\n",
    "# ignoring warnings for demonstration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# used to display DataFrame information without truncating\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log into the following URL in a web browser:\n",
      "\n",
      "\thttps://keycloak.autoscale-uat-gcp.wallaroo.dev/auth/realms/master/device?user_code=CPAZ-MUGU\n",
      "\n",
      "Login successful!\n"
     ]
    }
   ],
   "source": [
    "# Login through local Wallaroo instance\n",
    "\n",
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "We will create a workspace to manage our pipeline and models.  The following variables will set the name of our sample workspace then set it as the current workspace.\n",
    "\n",
    "Workspace, pipeline, and model names should be unique to each user, so we'll add in a randomly generated suffix so multiple people can run this tutorial in a Wallaroo instance without effecting each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'run-anywhere-architecture-hf-summarizer-demonstration-tutorial', 'id': 64, 'archived': False, 'created_by': 'b4a9aa3d-83fc-407a-b4eb-37796e96f1ef', 'created_at': '2024-03-05T16:09:41.98492+00:00', 'models': [], 'pipelines': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = wl.get_workspace(name=workspace_name, create_if_not_exist=True)\n",
    "\n",
    "wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Models and Set Target Architecture to ARM\n",
    "\n",
    "For our example, we will upload the Hugging Face Summarizer model.  The model file is `llm-models/model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip`, and is uploaded with the name `hf-summarizer-arm`.\n",
    "\n",
    "Models are uploaded to Wallaroo via the [`wallaroo.client.upload_model`](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-reference-guide/client/#Client.upload_model) method which takes the following arguments:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| **path** | *String* (*Required*) | The file path to the model. |\n",
    "| **framework** | *wallaroo.framework.Framework* (*Required*) | The model's framework.  See [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) for supported model frameworks. |\n",
    "| **input_schema** | *pyarrow.lib.Schema* (*Optional*)  | The model's input schema.  **Only required for non-Native Wallaroo frameworks.  See [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) for more details. |\n",
    "| **output_schema** | *pyarrow.lib.Schema* (*Optional*)  | The model's output schema.  **Only required for non-Native Wallaroo frameworks.  See [Wallaroo SDK Essentials Guide: Model Uploads and Registrations](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-model-uploads/) for more details. |\n",
    "| **convert_wait** | *bool* (*Optional*)  | Whether to wait in the SDK session to complete the auto-packaging process for non-native Wallaroo frameworks. |\n",
    "| **arch** | *wallaroo.engine_config.Architecture* (*Optional*)  | The targeted architecture for the model.  Options are <ol><li>`X86` (*Default*)</li><li>`ARM`</li></ol> |\n",
    "\n",
    "Verify the ML model is downloaded from [model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip (1.4 GB)](https://storage.googleapis.com/wallaroo-public-data/llm-models/model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip) and placed into the `./models` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_schema = pa.schema([\n",
    "    pa.field('inputs', pa.string()),\n",
    "    pa.field('return_text', pa.bool_()),\n",
    "    pa.field('return_tensors', pa.bool_()),\n",
    "    pa.field('clean_up_tokenization_spaces', pa.bool_()),\n",
    "    # pa.field('generate_kwargs', pa.map_(pa.string(), pa.null())), # dictionaries are not currently supported by the engine\n",
    "])\n",
    "\n",
    "output_schema = pa.schema([\n",
    "    pa.field('summary_text', pa.string()),\n",
    "])\n",
    "\n",
    "model_name_arm = f'hf-summarizer-arm'\n",
    "model_file_name = './models/model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip'\n",
    "\n",
    "model_arm = wl.upload_model(model_name_arm, \n",
    "                        model_file_name, \n",
    "                        framework=wallaroo.framework.Framework.HUGGING_FACE_SUMMARIZATION, \n",
    "                        input_schema=input_schema, \n",
    "                        output_schema=output_schema,\n",
    "                        arch=Architecture.ARM\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "        <tr>\n",
       "          <td>Name</td>\n",
       "          <td>house-price-estimator-arm</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Version</td>\n",
       "          <td>0d7bb3f4-db0f-448d-8100-22c1c49e6a2e</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>File Name</td>\n",
       "          <td>model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>SHA</td>\n",
       "          <td>ee71d066a83708e7ca4a3c07caf33fdc528bb000039b6ca2ef77fa2428dc6268</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Status</td>\n",
       "          <td>ready</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Image Path</td>\n",
       "          <td>proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.1.0-main-4609</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Architecture</td>\n",
       "          <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Acceleration</td>\n",
       "          <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "          <td>Updated At</td>\n",
       "          <td>2024-05-Mar 16:16:49</td>\n",
       "        </tr>\n",
       "      </table>"
      ],
      "text/plain": [
       "{'name': 'house-price-estimator-arm', 'version': '0d7bb3f4-db0f-448d-8100-22c1c49e6a2e', 'file_name': 'model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip', 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.1.0-main-4609', 'arch': None, 'accel': None, 'last_update_time': datetime.datetime(2024, 3, 5, 16, 16, 49, 820523, tzinfo=tzutc())}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_arm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Models to ARM Architecture\n",
    "\n",
    "#### Build the Pipeline\n",
    "\n",
    "This pipeline is used as an example of the edge deployment environment for testing.  We will create a pipeline and add our model set to the `ARM` architecture as a pipeline step.\n",
    "\n",
    "The **model's targeted architecture** is inherited by the pipeline version; no additional pipeline settings are required to set that architecture.  The other settings apply the CPU and memory used by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>architecture-demonstration-arm</td></tr><tr><th>created</th> <td>2024-03-05 16:18:38.768602+00:00</td></tr><tr><th>last_updated</th> <td>2024-03-05 16:18:38.768602+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>arch</th> <td>None</td></tr><tr><th>accel</th> <td>None</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>d033152c-494c-44a6-8981-627c6b6ad72e</td></tr><tr><th>steps</th> <td></td></tr><tr><th>published</th> <td>False</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'architecture-demonstration-arm', 'create_time': datetime.datetime(2024, 3, 5, 16, 18, 38, 768602, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'house-price-estimator-arm', 'version': '0d7bb3f4-db0f-448d-8100-22c1c49e6a2e', 'sha': 'ee71d066a83708e7ca4a3c07caf33fdc528bb000039b6ca2ef77fa2428dc6268'}]}}]\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_arm = wl.build_pipeline(arm_pipeline_name)\n",
    "\n",
    "\n",
    "# clear the steps if used before\n",
    "pipeline_arm.clear()\n",
    "\n",
    "# set the model step with the ARM targeted model\n",
    "pipeline_arm.add_model_step(model_arm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Pipeline\n",
    "\n",
    "We deploy the pipeline, then show that the deployment configuration architecture is set to `ARM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimum deployment config\n",
    "deployment_config = wallaroo.DeploymentConfigBuilder() \\\n",
    "    .cpus(0.25).memory('1Gi') \\\n",
    "    .sidekick_cpus(model_arm, 4) \\\n",
    "    .sidekick_memory(model_arm, \"8Gi\") \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 45s ..........................................."
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>*** An error occurred while deploying your pipeline.</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "WaitForDeployError",
     "evalue": "Deployment failed. See status for details.\nStatus: {'status': 'Error', 'details': [], 'engines': [{'ip': '10.124.0.140', 'name': 'engine-98974ffff-f99tm', 'status': 'Running', 'reason': None, 'details': [], 'pipeline_statuses': {'pipelines': [{'id': 'architecture-demonstration-arm', 'status': 'Running'}]}, 'model_statuses': {'models': [{'config': {'batch_config': None, 'filter_threshold': None, 'id': 82, 'input_schema': '/////xgBAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAQAAAC8AAAAfAAAAEgAAAAEAAAAZP///wAAAQYQAAAAMAAAAAQAAAAAAAAAHAAAAGNsZWFuX3VwX3Rva2VuaXphdGlvbl9zcGFjZXMAAAAAbP///6T///8AAAEGEAAAACAAAAAEAAAAAAAAAA4AAAByZXR1cm5fdGVuc29ycwAAnP///9T///8AAAEGEAAAABwAAAAEAAAAAAAAAAsAAAByZXR1cm5fdGV4dADI////EAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAYAAABpbnB1dHMAAAQABAAEAAAA', 'model_version_id': 47, 'output_schema': '/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAAwAAABzdW1tYXJ5X3RleHQAAAAABAAEAAQAAAA=', 'runtime': 'flight', 'sidekick_uri': None, 'tensor_fields': None}, 'model_version': {'conversion': {'framework': 'hugging-face-summarization', 'python_version': '3.8', 'requirements': []}, 'file_info': {'file_name': 'model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip', 'sha': 'ee71d066a83708e7ca4a3c07caf33fdc528bb000039b6ca2ef77fa2428dc6268', 'version': '0d7bb3f4-db0f-448d-8100-22c1c49e6a2e'}, 'id': 47, 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.1.0-main-4609', 'name': 'house-price-estimator-arm', 'status': 'ready', 'task_id': 'a872f5f5-e971-45dc-8d42-e74b839f9610', 'visibility': 'private', 'workspace_id': 64}, 'status': 'Running'}]}}], 'engine_lbs': [{'ip': '10.124.0.139', 'name': 'engine-lb-d7cc8fc9c-5wxp2', 'status': 'Running', 'reason': None, 'details': []}], 'sidekicks': [{'ip': None, 'name': 'engine-sidekick-house-price-estimator-arm-47-74966db799-wfcnw', 'status': 'Pending', 'reason': '0/4 nodes are available: 1 node(s) had untolerated taint {kubernetes.io/arch: arm64}, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..', 'details': ['0/4 nodes are available: 1 node(s) had untolerated taint {kubernetes.io/arch: arm64}, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..'], 'statuses': None}]}",
     "output_type": "error",
     "traceback": [
      "Deployment failed. See status for details.\nStatus: {'status': 'Error', 'details': [], 'engines': [{'ip': '10.124.0.140', 'name': 'engine-98974ffff-f99tm', 'status': 'Running', 'reason': None, 'details': [], 'pipeline_statuses': {'pipelines': [{'id': 'architecture-demonstration-arm', 'status': 'Running'}]}, 'model_statuses': {'models': [{'config': {'batch_config': None, 'filter_threshold': None, 'id': 82, 'input_schema': '/////xgBAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAQAAAC8AAAAfAAAAEgAAAAEAAAAZP///wAAAQYQAAAAMAAAAAQAAAAAAAAAHAAAAGNsZWFuX3VwX3Rva2VuaXphdGlvbl9zcGFjZXMAAAAAbP///6T///8AAAEGEAAAACAAAAAEAAAAAAAAAA4AAAByZXR1cm5fdGVuc29ycwAAnP///9T///8AAAEGEAAAABwAAAAEAAAAAAAAAAsAAAByZXR1cm5fdGV4dADI////EAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAABwAAAAEAAAAAAAAAAYAAABpbnB1dHMAAAQABAAEAAAA', 'model_version_id': 47, 'output_schema': '/////3gAAAAQAAAAAAAKAAwABgAFAAgACgAAAAABBAAMAAAACAAIAAAABAAIAAAABAAAAAEAAAAUAAAAEAAUAAgABgAHAAwAAAAQABAAAAAAAAEFEAAAACQAAAAEAAAAAAAAAAwAAABzdW1tYXJ5X3RleHQAAAAABAAEAAQAAAA=', 'runtime': 'flight', 'sidekick_uri': None, 'tensor_fields': None}, 'model_version': {'conversion': {'framework': 'hugging-face-summarization', 'python_version': '3.8', 'requirements': []}, 'file_info': {'file_name': 'model-auto-conversion_hugging-face_complex-pipelines_hf-summarisation-bart-large-samsun.zip', 'sha': 'ee71d066a83708e7ca4a3c07caf33fdc528bb000039b6ca2ef77fa2428dc6268', 'version': '0d7bb3f4-db0f-448d-8100-22c1c49e6a2e'}, 'id': 47, 'image_path': 'proxy.replicated.com/proxy/wallaroo/ghcr.io/wallaroolabs/mac-deploy:v2024.1.0-main-4609', 'name': 'house-price-estimator-arm', 'status': 'ready', 'task_id': 'a872f5f5-e971-45dc-8d42-e74b839f9610', 'visibility': 'private', 'workspace_id': 64}, 'status': 'Running'}]}}], 'engine_lbs': [{'ip': '10.124.0.139', 'name': 'engine-lb-d7cc8fc9c-5wxp2', 'status': 'Running', 'reason': None, 'details': []}], 'sidekicks': [{'ip': None, 'name': 'engine-sidekick-house-price-estimator-arm-47-74966db799-wfcnw', 'status': 'Pending', 'reason': '0/4 nodes are available: 1 node(s) had untolerated taint {kubernetes.io/arch: arm64}, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..', 'details': ['0/4 nodes are available: 1 node(s) had untolerated taint {kubernetes.io/arch: arm64}, 3 Insufficient cpu. preemption: 0/4 nodes are available: 1 Preemption is not helpful for scheduling, 3 No preemption victims found for incoming pod..'], 'statuses': None}]}"
     ]
    }
   ],
   "source": [
    "pipeline_arm.deploy(deployment_config=deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_arm.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences on ARM Architecture Model Deployments\n",
    "\n",
    "With the pipeline deployed, we'll perform sample inferences through the pipeline's inference API endpoints.  This will be the same method used for inference requests for model edge deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### inferences on arm model deployments\n",
    "\n",
    "arm_deploy_url = pipeline_arm._deployment._url()\n",
    "\n",
    "# get authorization header\n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# set the content-type and accept headers\n",
    "\n",
    "# set the content type for pandas records\n",
    "headers['Content-Type']= 'application/json; format=pandas-records'\n",
    "\n",
    "# set accept as pandas-records\n",
    "headers['Accept']='application/json; format=pandas-records'\n",
    "\n",
    "!curl -X POST {arm_deploy_url} \\\n",
    "    -H \"Authorization: {headers['Authorization']}\" \\\n",
    "        -H \"Content-Type:{headers['Content-Type']}\" \\\n",
    "            -H \"Accept:{headers['Accept']}\" \\\n",
    "                --data @./data/test_summarization.df.json > ./arm_results.df.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first 5 results from the outputs\n",
    "\n",
    "arm_df = pd.read_json(\"./arm_results.df.json\", orient=\"records\")\n",
    "arm_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undeploy the Pipelines\n",
    "\n",
    "With the inference examples complete, we can undeploy the pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_arm.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Deployment on ARM Architecture\n",
    "\n",
    "We now deploy the pipeline versions to our edge devices.\n",
    "\n",
    "* Publish the pipeline versions:  Publishes the pipeline versions to the OCI registry with the deployment configuration inherited from the pipeline versions, with the architecture settings inherited from the model versions.\n",
    "* Deploy Edge:  Deploy the edge device with the edge location settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish Models with ARM Architecture Deployment Configuration\n",
    "\n",
    "Publishing the pipeline uses the pipeline `wallaroo.pipeline.publish()` command.  This requires that the Wallaroo Ops instance have [Edge Registry Services](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-edge-deployment/#enable-wallaroo-edge-deployment-registry) enabled.\n",
    "\n",
    "The following publishes the pipeline to the OCI registry and displays the container details.  For more information, see [Wallaroo SDK Essentials Guide: Pipeline Edge Publication](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-pipelines/wallaroo-sdk-essentials-pipeline-publication/).\n",
    "\n",
    "Note that `engine_url` field specifies the Wallaroo engine type based on the targeted architecture, which is inherited from the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_pub_arm = pipeline_arm.publish()\n",
    "assay_pub_arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assay_pub_arm.engine_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Models to ARM Architecture Edge Devices\n",
    "\n",
    "The edge deployment is performed with `docker run`, `docker compose`, or `helm` installations.  The following command generates the `docker run` command, with the following values provided by the DevOps Engineer:\n",
    "\n",
    "* `$REGISTRYURL`: The URL of the OCI registry service hosting the pipeline publish.\n",
    "* `$OCI_USERNAME`: The username for the OCI registry service.\n",
    "* `$OCI_PASSWORD`: The password or token for the OCI registry service.\n",
    "\n",
    "For more details on model edge deployments with Wallaroo, see [Model Operations: Run Anywhere](https://staging.docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-run-anywhere/).\n",
    "\n",
    "The edge deployment configuration is taken from the pipeline publish, which shows the ARM engine archicture as part of it's edge deployment configuration that was inherited from the model architecture setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create docker run \n",
    "\n",
    "docker_command = f'''\n",
    "docker run -p 8080:8080 \\\\\n",
    "    -e DEBUG=true \\\\\n",
    "    -e OCI_REGISTRY=$REGISTRYURL \\\\\n",
    "    -e CONFIG_CPUS=1 \\\\\n",
    "    -e OCI_USERNAME=$OCI_USERNAME \\\\\n",
    "    -e OCI_PASSWORD=$OCI_PASSWORD \\\\\n",
    "    -e PIPELINE_URL={assay_pub_x86.pipeline_url} \\\\\n",
    "    {assay_pub_x86.engine_url}\n",
    "'''\n",
    "\n",
    "print(docker_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create docker run \n",
    "\n",
    "docker_command = f'''\n",
    "docker run -p 8080:8080 \\\\\n",
    "    -e DEBUG=true \\\\\n",
    "    -e OCI_REGISTRY=$REGISTRYURL \\\\\n",
    "    -e CONFIG_CPUS=1 \\\\\n",
    "    -e OCI_USERNAME=$OCI_USERNAME \\\\\n",
    "    -e OCI_PASSWORD=$OCI_PASSWORD \\\\\n",
    "    -e PIPELINE_URL={assay_pub_arm.pipeline_url} \\\\\n",
    "    {assay_pub_arm.engine_url}\n",
    "'''\n",
    "\n",
    "print(docker_command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge Inference Examples on ARM Architecture\n",
    "\n",
    "The following examples demonstrate performing inferences on the model deployed on an `ARM` architecture edge device.  `HOSTNAME_ARM` is the hostname for the ARM architecture edge deployment.  Update this variable to match your deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOSTNAME_ARM = 'localhost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the inference through the ARM edge device\n",
    "!curl -X POST {HOSTNAME_ARM}:8080/pipelines/{arm_pipeline_name} \\\n",
    "    -H \"Content-Type: Content-Type: application/json; format=pandas-records\" \\\n",
    "    --data @./data/normal-inputs.df.json > results.df.json\n",
    "\n",
    "# display the first 5 results\n",
    "result_df = pd.from_json('./results.df.json')\n",
    "result_df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dda4bf3640b7fafcd1648658b879b4cc9f6ba6084e8fb356fdaaa1a461d1690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
