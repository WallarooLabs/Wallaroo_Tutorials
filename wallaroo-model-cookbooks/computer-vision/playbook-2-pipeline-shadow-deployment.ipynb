{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Wallaroo Shadow Deployment in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore using Wallaroo's Shadow Deployment Technology to safely improve your Object Detectors in a production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step 3 we learned how to deploy a object detector into a Wallaroo pipeline, run inference, and draw the detected objects, the bounding boxes, the classifications, and the confidence of the classifications on the sample image.\n",
    "\n",
    "Now we will expand on that lesson by using 2 object detectors.  The mobilnet object detector is the control and the faster-rcnn object detector is the challenger.  Lets compare the inferencing results when we feed our shadow deployment pipeline an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import wallaroo\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from CVDemoUtils import CVDemo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control =  wl.upload_model('mobilenet', 'models/mobilenet.pt.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger = wl.upload_model('resnet50', 'models/frcnn-resnet.pt.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(1).memory(\"8Gi\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"shadow-pp\")\n",
    "pipeline.add_shadow_deploy(control, [challenger])\n",
    "pipeline.deploy(deployment_config = deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # needed to allow the pipeline to settle in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the shadow deployment by running inference on a sample image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare input image\n",
    "\n",
    "Next we will load a sample image and resize it to the width and height required for the object detector.\n",
    "\n",
    "We will convert the image to a numpy ndim array and add it do a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#imagePath = 'images/input/example/example_01.jpg'\n",
    "imagePath = 'data/images/input/example/store-front.png'\n",
    "\n",
    "# The image width and height needs to be set to what the model was trained for.  In this case 640x480.\n",
    "cvDemo = CVDemo()\n",
    "\n",
    "# The size the image will be resized to meet the input requirements of the object detector\n",
    "width = 640\n",
    "height = 480\n",
    "tensor, controlImage = cvDemo.loadImageAndResize(imagePath, width, height)\n",
    "challengerImage = controlImage.copy()\n",
    "\n",
    "# get npArray from the tensorFloat\n",
    "npArray = tensor.cpu().numpy()\n",
    "\n",
    "#creates a dictionary with the wallaroo \"tensor\" key and the numpy ndim array representing image as the value.\n",
    "dictData = {\"tensor\": npArray.tolist()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run inference using the Shadow Deployment using the SDK \n",
    "\n",
    "Now lets have the model detect the objects on the image by running inference and extracting the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startTime = time.time()\n",
    "infResults = pipeline.infer(dictData)\n",
    "endTime = time.time()\n",
    "\n",
    "results = infResults[0].raw\n",
    "results['original_data'] = None  # We are removing the input image json.  Not needed\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extracting the Control inference information and putting it in a table\n",
    "\n",
    "We will create a dataframe with columns representing the classification, confidence, and bounding boxes of the objects identified.\n",
    "\n",
    "Lets handle extracting the bounding boxes from the inference results for the objeects detected.  Once extracted from the results we will want to reshape the flattened array into an array with 4 elements (x,y,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['classification','confidence','x','y','width','height'])\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "\n",
    "# Points to where all the inference results are\n",
    "outputs = results['outputs']\n",
    "shadow_data = results['shadow_data']\n",
    "\n",
    "controlBoxes = outputs[0]\n",
    "\n",
    "# reshape this to an array of bounding box coordinates converted to ints\n",
    "boxList = controlBoxes['Float']['data']\n",
    "boxA = np.array(boxList)\n",
    "controlBoxes = boxA.reshape(-1, 4)\n",
    "controlBoxes = controlBoxes.astype(int)\n",
    "\n",
    "df[['x', 'y','width','height']] = pd.DataFrame(controlBoxes)\n",
    "\n",
    "controlClasses = outputs[1]['Int64']['data']\n",
    "controlConfidences = outputs[2]['Float']['data']\n",
    "\n",
    "idx = 0 \n",
    "cocoClasses = cvDemo.getCocoClasses()\n",
    "for idx in range(0,len(controlClasses)):\n",
    "    df['classification'][idx] = cocoClasses[controlClasses[idx]] # Classes contains the 80 different COCO classificaitons\n",
    "    df['confidence'][idx] = controlConfidences[idx]\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display the Control Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we will use the Wallaroo CVDemo helper class to draw the control model results on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {\n",
    "    'model_name' : control.name(),\n",
    "    'pipeline_name' : pipeline.name(),\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'image' : controlImage,\n",
    "    'boxes' : controlBoxes,\n",
    "    'classes' : controlClasses,\n",
    "    'confidences' : controlConfidences,\n",
    "    'confidence-target' : 0.9,\n",
    "    'color':CVDemo.RED, # color to draw bounding boxes and the text in the statistics\n",
    "    'inference-time': (endTime-startTime),\n",
    "    'onnx-time' : 0,                \n",
    "}\n",
    "cvDemo.drawAndDisplayDetectedObjectsWithClassification(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Extracting the Challenger inference information and putting it in a table\n",
    "\n",
    "Lets handle extracting the bounding boxes from the inference results for the objeects detected.  Once extracted from the results we will want to reshape the flattened array into an array with 4 elements (x,y,width,height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challengerBoxes = shadow_data['resnet50'][0]\n",
    "# reshape this to an array of bounding box coordinates converted to ints\n",
    "boxList = challengerBoxes['Float']['data']\n",
    "boxA = np.array(boxList)\n",
    "challengerBoxes = boxA.reshape(-1, 4)\n",
    "challengerBoxes = challengerBoxes.astype(int)\n",
    "\n",
    "challengerDf = pd.DataFrame(columns=['classification','confidence','x','y','width','height'])\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.float_format = '{:.2%}'.format\n",
    "\n",
    "challengerDf[['x', 'y','width','height']] = pd.DataFrame(challengerBoxes)\n",
    "#pd.options.display.float_format = '{:.2%}'.format\n",
    "challengerClasses = shadow_data['resnet50'][1]['Int64']['data']\n",
    "challengerConfidences = shadow_data['resnet50'][2]['Float']['data']\n",
    "\n",
    "idx = 0 \n",
    "for idx in range(0,len(challengerClasses)):\n",
    "    challengerDf['classification'][idx] = cvDemo.CLASSES[challengerClasses[idx]] # Classes contains the 80 different COCO classificaitons\n",
    "    challengerDf['confidence'][idx] = challengerConfidences[idx]\n",
    "challengerDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Display the Challenger Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we will use the Wallaroo CVDemo helper class to draw the challenger model results on the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blue = (255, 0, 0)\n",
    "results = {\n",
    "    'model_name' : challenger.name(),\n",
    "    'pipeline_name' : pipeline.name(),\n",
    "    'width': width,\n",
    "    'height': height,\n",
    "    'image' : challengerImage,\n",
    "    'boxes' : challengerBoxes,\n",
    "    'classes' : challengerClasses,\n",
    "    'confidences' : challengerConfidences,\n",
    "    'confidence-target' : 0.90,\n",
    "    'color':CVDemo.BLUE, # color to draw bounding boxes and the text in the statistics\n",
    "    'inference-time': (endTime-startTime),\n",
    "    'onnx-time' : 0,                \n",
    "}\n",
    "cvDemo.drawAndDisplayDetectedObjectsWithClassification(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................. ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>shadow-pp</td></tr><tr><th>created</th> <td>2022-10-29 15:29:46.904026+00:00</td></tr><tr><th>last_updated</th> <td>2022-12-14 13:00:36.357872+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>tags</th> <td></td></tr><tr><th>steps</th> <td>mobilenet</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'shadow-pp', 'create_time': datetime.datetime(2022, 10, 29, 15, 29, 46, 904026, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'mobilenet', 'version': 'ae96753a-d5bf-48d3-b7dd-c8166692b3ca', 'sha': 'f4c7009e53b679f5e44d70d9612e8dc365565cec88c25b5efa11b903b6b7bdc6'}, {'name': 'resnet50', 'version': '3e7d2577-52b6-44a7-9f95-bd549caf38c3', 'sha': 'ee606dc9776a1029420b3adf59b6d29395c89d1d9460d75045a1f2f152d288e7'}]}}, {'AuditResults': {'from': 1, 'to': None}}, {'MultiOut': {}}]\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()\n",
    "#for d in wl.list_deployments():\n",
    "#    d.undeploy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Notice the difference in the control confidence and the challenger confidence.  <b>Clearly we can see in this example the challenger resnet50 model is performing better than the control mobilenet model</b>.  This is likely due to the fact that frcnn resnet50 model is a 2 stage object detector vs the frcnn mobilenet is a single stage detector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
