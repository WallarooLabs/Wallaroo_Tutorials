{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Store Cameras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore using the mobilenet pipeline we created in step 3 to run inference on the frames in a Video and then draw the identified object's bounding boxes, classification and classification confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv_store.cv_demo_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10064/3999319031.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcv_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_demo_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCVDemo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcv_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv_store.cv_demo_utils'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import wallaroo\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import imutils\n",
    "import cv2\n",
    "from cv_store.cv_demo_utils import CVDemo\n",
    "from cv_store.store import Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "for w in ws:\n",
    "    if w.name() == 'computer-vision':\n",
    "        wl.set_current_workspace(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet'\n",
    "onnx_model_path = 'models/mobilenet.pt.onnx'\n",
    "mobilenet_model = wl.upload_model(model_name, onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(2).memory(\"12Gi\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'mobilenet-pp'\n",
    "pipeline = wl.build_pipeline(pipeline_name) \\\n",
    "            .add_model_step(mobilenet_model) \\\n",
    "            .deploy(deployment_config = deployment_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # needed to allow the pipeline to settle in.\n",
    "pipeline_url = pipeline._deployment._url()\n",
    "print(pipeline_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test the pipeline in a video stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input image\n",
    "\n",
    "Next we will load a sample image and resize it to the width and height required for the object detector.\n",
    "\n",
    "We will convert the image to a numpy ndim array and add it do a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The size the image will be resized to\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "# Only objects that have a confidence > confidence_target will be displayed on the image\n",
    "cvDemo = CVDemo()\n",
    "\n",
    "imagePath = 'images/input/example/example_01.jpg'\n",
    "\n",
    "# The image width and height needs to be set to what the model was trained for.  In this case 640x480.\n",
    "tensor, resizedImage = cvDemo.loadImageAndResize(imagePath, width, height)\n",
    "\n",
    "# get npArray from the tensorFloat\n",
    "npArray = tensor.cpu().numpy()\n",
    "\n",
    "#creates a dictionary with the wallaroo \"tensor\" key and the numpy ndim array representing image as the value.\n",
    "dictData = {\"tensor\": npArray.tolist()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run inference using the SDK \n",
    "\n",
    "Now lets have the model detect the objects on the image by running inference and extracting the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(2)\n",
    "infResults = pipeline.infer(dictData)\n",
    "results = infResults[0].raw\n",
    "results['original_data'] = None  # We are removing the input image json.  Not needed\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.logs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startTime = time.time()\n",
    "infResults = pipeline.infer(dictData)\n",
    "endTime = time.time()\n",
    "\n",
    "results = infResults[0].raw\n",
    "results['original_data'] = None  # We are removing the input image json.  Not needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Detect and Classify Objects in Video using Wallaroo Shadow Deployment\n",
    "\n",
    "Next we will load each frame in the input-video feedubg ut to the pipeline for inferencing.  Then using the results we will draw a bounding box around each identified object, print its classification, and the model's confidence that the prediction is accurate on the frame and save the frame to an output video.\n",
    "\n",
    "As we are executing this notice the time it takes to process each frame.  In the next section we will discuss ways to improve this performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvDemo = CVDemo()\n",
    "\n",
    "store = Store(\"Just Walk Out\")\n",
    "\n",
    "camera = {\n",
    "    'name' : 'grocery',\n",
    "    'src-loc' : 'videos/amazon-fresh-go.mp4',\n",
    "    'dest-loc' : 'videos/grocery-amazon-fresh-go-inferenced.mp4',\n",
    "    'fps' : 15,\n",
    "    'width' :  640,\n",
    "    'height' : 480,\n",
    "    'endpoint-url' : pipeline_url,\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "    'pipeline_name' : pipeline_name,\n",
    "    'pipeline' : pipeline,\n",
    "    'model_name' : model_name,\n",
    "    'confidence-target' : 0.75,\n",
    "    'color': CVDemo.AMBER\n",
    "}\n",
    "store.addCamera(camera)\n",
    "\n",
    "stream = cv2.VideoCapture(camera['src-loc'])\n",
    "#for cnt in range(0,200):\n",
    "#    (grabbed, frame) = stream.read()\n",
    "    \n",
    "store.cameraList[0].runInferenceOnFrame(frame,camera)\n",
    "\n",
    "enabled = True\n",
    "#store.setObjectDetection(enabled)\n",
    "#store.setAnomolyDetection(enabled)\n",
    "#store.setEnableDriftDetection(enabled)\n",
    "#store.setTrack(['person','hand bag'])\n",
    "#store.startCameras()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Notice how simple it is to take the mobilenet object detectors rest api endpoint url and use it in a video stream.  Now its your turn.  Upload a video to this notebook and replace the input-video with the path of the uploaded video.  Update the output video accordingly.\n",
    "\n",
    "See how well it works for you.  Good luck!\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "camera = {\n",
    "    'name' : 'electronics',\n",
    "    'src-loc' : 'videos/amazon-fresh-go.mp4',\n",
    "    'dest-loc' : 'images/out',\n",
    "    'fps' : 15,\n",
    "    'width' :  640,\n",
    "    'height' : 480,\n",
    "    'endpoint-url' : pipeline_url,\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "\n",
    "}\n",
    "store.addCamera(camera)\n",
    "\n",
    "camera = {\n",
    "    'name' : 'Clothing',\n",
    "    'src-loc' : 'videos/amazon-fresh-go.mp4',\n",
    "    'dest-loc' : 'images/out',\n",
    "    'fps' : 15,\n",
    "    'width' :  640,\n",
    "    'height' : 480,\n",
    "    'endpoint-url' : pipeline_url,\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "\n",
    "}\n",
    "store.addCamera(camera)\n",
    "\n",
    "camera = {\n",
    "    'name' : 'Toys',\n",
    "    'src-loc' : 'videos/amazon-fresh-go.mp4',\n",
    "    'dest-loc' : 'images/out',\n",
    "    'fps' : 15,\n",
    "    'width' :  640,\n",
    "    'height' : 480,\n",
    "    'endpoint-url' : pipeline_url,\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "}\n",
    "store.addCamera(camera)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.undeploy()\n",
    "for d in wl.list_deployments():\n",
    "    d.undeploy()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "# The size the image will be resized to\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-inferenced.mp4\"\n",
    "\n",
    "#input_video = \"videos/ww2-warbirds-in-formation.mp4\"\n",
    "#output_video = \"videos/ww2-warbirds-in-formation-inferenced.mp4\"\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 15, # Frames per second\n",
    "    'endpoint-url' : url, # the pipelines rest api endpoint url\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    #'max-frames' : 400, # the # of frames to capture in the output video\n",
    "    'skip-frames' : 225, # the # of frames to capture in the output video\n",
    "    'confidence-target' : 0.90, # only display bounding boxes with confidence > provided #\n",
    "    'color':CVDemo.CYAN, # color to draw bounding boxes and the text in the statistics\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "    'onnx_model_path' : onnx_model_path,\n",
    "    'model_name' : model_name,\n",
    "    'pipeline' : pipeline, # provide this when using inference WALLAROO_SDK \n",
    "    'pipeline_name' : pipeline_name,\n",
    "    'record-start-frame' : 225, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : 275, # the # of frames to capture in the output video  \n",
    "}\n",
    "\n",
    "\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.detectAndClassifyObjectsInVideo(config)\n",
    "print(\"We are done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
