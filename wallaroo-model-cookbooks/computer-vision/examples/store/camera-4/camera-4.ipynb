{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Objects using the Wallaroo mobilenet Pipeline in a Video Stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will explore using the mobilenet pipeline we created in step 3 to run inference on the frames in a Video and then draw the identified object's bounding boxes, classification and classification confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import wallaroo\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import imutils\n",
    "import sys\n",
    " \n",
    "from CVDemoUtils import CVDemo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log into the following URL in a web browser:\n",
      "\n",
      "\thttps://will-cobra-demo.keycloak.wallaroocommunity.ninja/auth/realms/master/device?user_code=BKOK-SZWT\n",
      "\n",
      "Login successful!\n"
     ]
    }
   ],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "for w in ws:\n",
    "    \n",
    "    if w.name() == 'computer-vision-3':\n",
    "        wl.set_current_workspace(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet'\n",
    "onnx_model_path = 'models/mobilenet.pt.onnx'\n",
    "mobilenet_model = wl.upload_model(model_name, onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(1).memory(\"8Gi\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment - this will take up to 45s . ok\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = 'camera-4-pp'\n",
    "pipeline = wl.build_pipeline(pipeline_name) \\\n",
    "            .add_model_step(mobilenet_model) \\\n",
    "            .deploy(deployment_config = deployment_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://engine-lb.camera-4-pp-1425:29502/pipelines/camera-4-pp\n"
     ]
    }
   ],
   "source": [
    "time.sleep(5) # needed to allow the pipeline to settle in.\n",
    "url = pipeline._deployment._url()\n",
    "print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the pipeline in a video stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Detect and Classify Objects in Video using Wallaroo Shadow Deployment\n",
    "\n",
    "Next we will load each frame in the input-video feedubg ut to the pipeline for inferencing.  Then using the results we will draw a bounding box around each identified object, print its classification, and the model's confidence that the prediction is accurate on the frame and save the frame to an output video.\n",
    "\n",
    "As we are executing this notice the time it takes to process each frame.  In the next section we will discuss ways to improve this performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11-22-2022 21:45:09.325447 Start Time:04:45:09 PM\n",
      "11-22-2022 21:45:09.325541 Skipping [225] frames\n",
      "11-22-2022 21:45:09.332233 Video Properties\n",
      "11-22-2022 21:45:09.332284    video input:videos/amazon-fresh-go.mp4\n",
      "11-22-2022 21:45:09.332310    video output:videos/amazon-fresh-go-inferenced.mp4\n",
      "11-22-2022 21:45:09.332338    format:0.0\n",
      "11-22-2022 21:45:09.332356    fourcc:828601953.0\n",
      "11-22-2022 21:45:09.332374    mode:0.0\n",
      "11-22-2022 21:45:09.332389    buffer:0.0\n",
      "11-22-2022 21:45:09.332404    width:1280.0\n",
      "11-22-2022 21:45:09.332418    height:720.0\n",
      "11-22-2022 21:45:09.332432    fps:25.0\n",
      "11-22-2022 21:45:09.332500    frame size:(640, 630)\n",
      "11-22-2022 21:45:09.343689 recordStartFrame:0\n",
      "11-22-2022 21:45:09.343738 recordEndFrame:0\n",
      "11-22-2022 21:45:11.917899 Frame:225 Read: 0.0017 Inf: 1.7805 Onnx: 0.1416 Draw: 0.0129 Total: 1.8581\n",
      "11-22-2022 21:45:12.725353 Inference failed: \n",
      "\tInternalError: There was an error sending data to the pipeline: Received none result for Model Inference\n",
      "11-22-2022 21:45:12.725455 could not read frame\n",
      "11-22-2022 21:45:12.725466 exiting at frame:226\n"
     ]
    },
    {
     "ename": "InferenceError",
     "evalue": "Inference failed: \n\tInternalError: There was an error sending data to the pipeline: Received none result for Model Inference",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4916/1117822034.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m }\n\u001b[1;32m     35\u001b[0m \u001b[0mcvDemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEBUG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mcvDemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectAndClassifyObjectsInVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"We are done.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/demos-v2/computer-vision/cv_store/camera-4/CVDemoUtils.py\u001b[0m in \u001b[0;36mdetectAndClassifyObjectsInVideo\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1414\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"could not read frame\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exiting at frame:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeCnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/demos-v2/computer-vision/cv_store/camera-4/CVDemoUtils.py\u001b[0m in \u001b[0;36mdetectAndClassifyObjectsInVideo\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m   1409\u001b[0m                     \u001b[0;31m# run inference on the frame using width and height object detector is expecting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1411\u001b[0;31m                         \u001b[0minfResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunInferenceOnFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1413\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/demos-v2/computer-vision/cv_store/camera-4/CVDemoUtils.py\u001b[0m in \u001b[0;36mrunInferenceOnFrame\u001b[0;34m(self, frame, config)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0minfResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunInferenceOnFrameUsingApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'endpoint-url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inference'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"WALLAROO_SDK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m              \u001b[0minfResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunInferenceOnFrameUsingSdk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m         \u001b[0mendTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/demos-v2/computer-vision/cv_store/camera-4/CVDemoUtils.py\u001b[0m in \u001b[0;36mrunInferenceOnFrameUsingSdk\u001b[0;34m(self, frame, config)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;31m#with open(\"dictTensor.json\", \"w\") as outfile:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m#    outfile.write(jsonData)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mwalInferResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pipeline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m         \u001b[0minferResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvertWallarooResultToInferenceResultDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalInferResult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'extract-anomalies'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extract-anomalies'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/wallaroo/pipeline.py\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_infer_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/wallaroo/pipeline.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, tensor, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mdeployment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deployment_for_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeployment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdeployment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline {self.name} is not deployed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/wallaroo/deployment.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, tensor, timeout)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inference unable to complete.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mInferenceResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gql_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     def infer_from_file(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/wallaroo/deployment.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Inference unable to complete.\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mInferenceResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gql_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     def infer_from_file(\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/wallaroo/inference_result.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, gql_client, data)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \"\"\"\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInferenceError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_version\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceError\u001b[0m: Inference failed: \n\tInternalError: There was an error sending data to the pipeline: Received none result for Model Inference"
     ]
    }
   ],
   "source": [
    "cvDemo = CVDemo()\n",
    "\n",
    "# The size the image will be resized to\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "#input_video = \"videos/camera2.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-inferenced.mp4\"\n",
    "save_frames_path = \"images/output\"\n",
    "#input_video = \"videos/ww2-warbirds-in-formation.mp4\"\n",
    "#output_video = \"videos/ww2-warbirds-in-formation-inferenced.mp4\"\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'save-frames-path' : save_frames_path, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 15, # Frames per second\n",
    "    'endpoint-url' : url, # the pipelines rest api endpoint url\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    #'max-frames' : 400, # the # of frames to capture in the output video\n",
    "    'skip-frames' : 225, # the # of frames to capture in the output video\n",
    "    'confidence-target' : 0.10, # only display bounding boxes with confidence > provided #\n",
    "    'color':CVDemo.CYAN, # color to draw bounding boxes and the text in the statistics\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "    'onnx_model_path' : onnx_model_path,\n",
    "    'model_name' : model_name,\n",
    "    'pipeline' : pipeline, # provide this when using inference WALLAROO_SDK \n",
    "    'pipeline_name' : pipeline_name,\n",
    "    'skip-frames-list' :[(440,460),(1400,1450)]\n",
    "#    'record-start-frame' : 225, # the # of frames to capture in the output video\n",
    "#    'record-end-frame' : 275, # the # of frames to capture in the output video  \n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.detectAndClassifyObjectsInVideo(config)\n",
    "print(\"We are done.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "Notice how simple it is to take the mobilenet object detectors rest api endpoint url and use it in a video stream.  Now its your turn.  Upload a video to this notebook and replace the input-video with the path of the uploaded video.  Update the output video accordingly.\n",
    "\n",
    "See how well it works for you.  Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for undeployment - this will take up to 45s ................................ ok\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>camera-4-pp</td></tr><tr><th>created</th> <td>2022-11-17 14:36:28.612490+00:00</td></tr><tr><th>last_updated</th> <td>2022-11-20 23:58:05.385558+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>tags</th> <td></td></tr><tr><th>steps</th> <td>mobilenet</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'camera-4-pp', 'create_time': datetime.datetime(2022, 11, 17, 14, 36, 28, 612490, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'mobilenet', 'version': '0d5e4bd9-9509-4e3e-98bf-87b154e87d71', 'sha': '81f00608d80677b4a00e3a7cf438110e1da49c480e1762eded2acf5504179be4'}]}}]\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.undeploy()\n",
    "#for d in wl.list_deployments():\n",
    "#    d.undeploy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
