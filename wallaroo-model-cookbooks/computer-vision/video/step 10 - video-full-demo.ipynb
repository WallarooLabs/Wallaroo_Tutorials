{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Demo Video - Stiching it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will do a review of everything we have learned by building a video that shows how you can use wallaroo to improve your computer vision ai results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import wallaroo\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import imutils\n",
    "\n",
    "from CVDemoUtils import CVDemo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize some Vars\n",
    "\n",
    "Initialize the COCO Classes, meaning the classificaitons found on the images and the default width and height all images are resized to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size the image will be resized to\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "# set the device we will be using to run the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The set of COCO classifications\n",
    "CLASSES = pickle.loads(open(\"models/coco_classes.pickle\", \"rb\").read())\n",
    "\n",
    "# Unique colors for each identified COCO class\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "# The size the image will be resized to\n",
    "width = 640\n",
    "height = 480\n",
    "fps = 25\n",
    "# Only objects that have a confidence > confidence_target will be displayed on the image\n",
    "confidence_target = 0.75\n",
    "\n",
    "cvDemo = CVDemo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Choose the video we want to perform object detection on\n",
    "\n",
    "Next lets choose the video we want to perform object detection on and explain\n",
    "to the user what we are about to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-4-intro-cv.mp4\"\n",
    "\n",
    "\n",
    "startFrame = 0\n",
    "endFrame = 100\n",
    "padding = 10\n",
    "sentPadding = 15\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : fps, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video    'skip-frames' : 75, # the # of frames to capture in the output video\n",
    "    'dashboard-message-list' : [\"Discover how Wallaroo can help\",\"improve your computer vision results.\"],\n",
    "    'dashboard-start-frame' : startFrame + padding,\n",
    "    'dashboard-end-frame' : endFrame - padding,\n",
    "    'color':CVDemo.WHITE,\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.recordVideo(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-4-mobilenet-intro.mp4\"\n",
    "\n",
    "duration = 100\n",
    "startFrame = endFrame\n",
    "endFrame = startFrame + duration\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : fps, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video    'skip-frames' : 75, # the # of frames to capture in the output video\n",
    "    'dashboard-message-list' : [\"Lets start by deploying a pipeline with a\",\"single stage mobilenet object detector.\"],\n",
    "    'dashboard-start-frame' : startFrame + padding,\n",
    "    'dashboard-end-frame' : endFrame - padding,\n",
    "    'color':CVDemo.WHITE,\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.recordVideo(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the mobilenet Pipeline \n",
    "\n",
    "Next we will upload the mobilenet model to wallaroo, tell wallaroo what resources we need, and create the mobilenet pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import wallaroo\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "for w in ws:\n",
    "    if w.name() == 'computer-vision':\n",
    "        wl.set_current_workspace(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet'\n",
    "onnx_model_path = 'models/mobilenet.pt.onnx'\n",
    "mobilenet_model = wl.upload_model(model_name, onnx_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will add our post processing anomoly detection file called post-process-anomoly-detection.py.  Predictions that are lower than 75% we will consider anomalies that need to be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(2).memory(\"12Gi\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'mobilenet-pp'\n",
    "pipeline = wl.build_pipeline(pipeline_name) \\\n",
    "            .add_model_step(mobilenet_model) \\\n",
    "            .deploy(deployment_config = deployment_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # needed to allow the pipeline to settle in.\n",
    "url = pipeline._deployment._url()\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-4-mobilenet-inferenced.mp4\"\n",
    "\n",
    "#startFrame = endFrame\n",
    "startFrame = endFrame\n",
    "\n",
    "noteStart = startFrame + 60\n",
    "endFrame = noteStart + 410\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : fps, # Frames per second\n",
    "    'endpoint-url' : url, # the pipelines rest api endpoint url\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    #'max-frames' : 400, # the # of frames to capture in the output video\n",
    "    'skip-frames' : startFrame, # the # of frames to capture in the output video\n",
    "    'confidence-target' : 0.90, # only display bounding boxes with confidence > provided #\n",
    "    'color':CVDemo.CYAN, # color to draw bounding boxes and the text in the statistics\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "    'onnx_model_path' : onnx_model_path,\n",
    "    'model_name' : model_name,\n",
    "    'pipeline' : pipeline, # provide this when using inference WALLAROO_SDK \n",
    "    'deployment_config' : deployment_config,\n",
    "    'pipeline_name' : 'mobilenet-pp',\n",
    "    'wl' : wl,\n",
    "    'model' : mobilenet_model,\n",
    "    'pipeline_name' : pipeline_name,\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video  \n",
    "    'note-list' : [ { 'note' : ' The Cyan color rectangles are the mobilenet object detections.',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart,\n",
    "                      'end-frame': noteStart + 100},\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 100,\n",
    "                      'end-frame': noteStart + 150  },\n",
    "                      { 'note' : 'The object classifications are located above the bounding box',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 150,\n",
    "                      'end-frame': noteStart + 250  },\n",
    "                     { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 250,\n",
    "                      'end-frame': noteStart + 300  },\n",
    "                       { 'note' : ' and the % confidence in classifications to the right of it.',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 300,\n",
    "                      'end-frame': noteStart + 400 }],\n",
    "    'skip-frames-list' : [ (442,460) ]\n",
    "                      \n",
    "}\n",
    "\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.detectAndClassifyObjectsInVideo(config)\n",
    "print(\"We are done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-4-resnet-intro.mp4\"\n",
    "\n",
    "\n",
    "duration = 100\n",
    "#startFrame = endFrame\n",
    "startFrame = 670\n",
    "\n",
    "endFrame = startFrame + duration\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : fps, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'skip-frames' : startFrame, # the # of frames to capture in the output video    \n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video    'skip-frames' : 75, # the # of frames to capture in the output video\n",
    "    'dashboard-message-list' : [\"Now, lets deploy a challenger. \", \"The resnet50 object detector.\"],\n",
    "    'dashboard-start-frame' : startFrame + padding ,\n",
    "    'dashboard-end-frame' : endFrame - padding ,\n",
    "    'color':CVDemo.WHITE,\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.recordVideo(config)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy the resnet50 Pipeline\n",
    "\n",
    "Next we will upload the mobilenet model to wallaroo, tell wallaroo what resources we need, and create the mobilenet pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import wallaroo\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "for w in ws:\n",
    "    if w.name() == 'computer-vision':\n",
    "        wl.set_current_workspace(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'resnet50'\n",
    "onnx_model_path = 'models/frcnn-resnet.pt.onnx'\n",
    "resnet_model = wl.upload_model(model_name, onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(2).memory(\"12Gi\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'resnet-pp'\n",
    "pipeline = wl.build_pipeline('resnet-pp') \\\n",
    "            .add_model_step(resnet_model) \\\n",
    "            .deploy(deployment_config = deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # needed to allow the pipeline to settle in.\n",
    "url = pipeline._deployment._url()\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.undeploy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-4-resnet-inferenced.mp4\"\n",
    "\n",
    "#startFrame = endFrame\n",
    "startFrame = 670\n",
    "\n",
    "noteStart = startFrame + 60\n",
    "endFrame = noteStart + 410\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : fps, # Frames per second\n",
    "    'endpoint-url' : url, # the pipelines rest api endpoint url\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    #'max-frames' : 400, # the # of frames to capture in the output video\n",
    "    'skip-frames' : startFrame, # the # of frames to capture in the output video\n",
    "    'confidence-target' : 0.90, # only display bounding boxes with confidence > provided #\n",
    "    'color':CVDemo.ORANGE, # color to draw bounding boxes and the text in the statistics\n",
    "    'inference' : 'WALLAROO_SDK', # \"ONNX\" or \"WALLAROO_API\" or \"WALLAROO_SDK\"\n",
    "    'onnx_model_path' : onnx_model_path,\n",
    "    'model_name' : model_name,\n",
    "    'pipeline' : pipeline, # provide this when using inference WALLAROO_SDK \n",
    "    'pipeline_name' : pipeline_name,\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video  \n",
    "    'note-list' : [ { 'note' : ' Notice, the resnet50 model is able to detect more objects. ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart,\n",
    "                      'end-frame': noteStart + 80 },\n",
    "                      { 'note' : 'This is seen in the [Obj] and [Cls] columns.',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 90,\n",
    "                      'end-frame': noteStart + 130 },\n",
    "                       { 'note' : 'but inferences [Inf] are significantly slower. ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 140,\n",
    "                      'end-frame': noteStart + 190 }],\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.detectAndClassifyObjectsInVideo(config)\n",
    "print(\"We are done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Detect and Classify Objects in Video using Wallaroo Shadow Deployment\n",
    "\n",
    "Next we will load each frame in the input-video feedubg ut to the pipeline for inferencing.  Then using the results we will draw a bounding box around each identified object, print its classification, and the model's confidence that the prediction is accurate on the frame and save the frame to an output video.\n",
    "\n",
    "As we are executing this notice the time it takes to process each frame.  In the next section we will discuss ways to improve this performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "for w in ws:\n",
    "    if w.name() == 'computer-vision':\n",
    "        wl.set_current_workspace(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control =  wl.upload_model('mobilenet', 'models/mobilenet.pt.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_list  = [ \n",
    "    wl.upload_model('resnet50', 'models/frcnn-resnet.pt.onnx')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(2).memory(\"12Gi\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = wl.build_pipeline(\"shadow-pp\")\n",
    "pipeline.add_shadow_deploy(control, challenger_list)\n",
    "pipeline.deploy(deployment_config = deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5) # needed to allow the pipeline to settle in.\n",
    "url = pipeline._deployment._url()\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 10\n",
    "duration = 100\n",
    "startFrame = 1139\n",
    "endFrame = startFrame + duration\n",
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-5-shadow-part-1-intro.mp4\"\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 15, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'skip-frames' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video    'skip-frames' : 75, # the # of frames to capture in the output video\n",
    "    'dashboard-message-list' : [\"Lets compare the models using a pipeline\", \n",
    "                                \"that has been configured for shadow deployment.\",\n",
    "                                \"The control is mobilenet, the challenge is resnet50\"],\n",
    "    'dashboard-start-frame' : startFrame + padding ,\n",
    "    'dashboard-end-frame' : endFrame - padding ,\n",
    "    'color':CVDemo.WHITE,\n",
    "}\n",
    "\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.recordVideo(config)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startFrame = endFrame\n",
    "noteStart = startFrame + 60\n",
    "endFrame = noteStart + 475\n",
    "\n",
    "# Only objects that have a confidence > confidence_target will be displayed on the image\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-5-shadow-part-1-inferenced.mp4\"\n",
    "\n",
    "cur_ws = wl.get_current_workspace()\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 25, # Frames per second\n",
    "    'pipeline' : pipeline,\n",
    "    'control-model' : control,\n",
    "    'challenger-model-list' : challenger_list,\n",
    "    'endpoint-url' : url, # the pipelines rest api endpoint url\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'skip-frames' : startFrame, # the # of frames to capture in the output video\n",
    "    'confidence-target' : 0.90, # only display bounding boxes with confidence > provided #\n",
    "    'inference': 'WALLAROO_SDK',\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video  \n",
    "    'note-list' : [ { 'note' : ' The Cyan color rectangles are the mobilenet object detections',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart,\n",
    "                      'end-frame': noteStart + 100},\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 100,\n",
    "                      'end-frame': noteStart + 150  },\n",
    "                     { 'note' : 'The Orange color rectangles are the mobilenet object detections',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 150,\n",
    "                      'end-frame': noteStart + 250 },\n",
    "                     { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 250,\n",
    "                      'end-frame': noteStart + 300  },\n",
    "                      { 'note' : 'A great visulization for the data scientist. ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 300,\n",
    "                      'end-frame': noteStart + 350 },\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 350,\n",
    "                      'end-frame': noteStart + 375  },\n",
    "                    { 'note' : 'Enabling data scientists to determine which model is better',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 375,\n",
    "                      'end-frame': noteStart + 475 }\n",
    "                    ],\n",
    "        'skip-frames-list' : [ (1400,1420) ]                  \n",
    "}\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "cvDemo.detectAndClassifyObjectsInVideoUsingShadowDeployment(config)\n",
    "print(\"We are done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using Shadow Deployment to have the best of both worlds\n",
    "\n",
    "Now we will run inferences again using shadow deployment, but on each frame we will inspect the inferences\n",
    "for both the control and challenger models and choose the one that has the best average score above 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-shadow-part-2-intro.mp4\"\n",
    "\n",
    "duration = 125\n",
    "startFrame = endFrame\n",
    "endFrame = startFrame + duration\n",
    "\n",
    "cvDemo.print(\"startFrame:\"+str(startFrame))\n",
    "cvDemo.print(\"endFrame:\"+str(endFrame))\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 25, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'skip-frames' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video    'skip-frames' : 75, # the # of frames to capture in the output video\n",
    "    'dashboard-message-list' : [\"What if we could have the best of both worlds?\", \n",
    "                                \"The next we will use the shadow deployment pipeline to \", \n",
    "                                \"help us choose the best model.\"],\n",
    "    'dashboard-start-frame' : startFrame + padding ,\n",
    "    'dashboard-end-frame' : endFrame - padding ,\n",
    "    'color':CVDemo.WHITE,\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.recordVideo(config)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startFrame = endFrame\n",
    "noteStart = startFrame + 60\n",
    "endFrame = noteStart + 475\n",
    "\n",
    "# Only objects that have a confidence > confidence_target will be displayed on the image\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-shadow-part-2-inferenced.mp4\"\n",
    "\n",
    "cur_ws = wl.get_current_workspace()\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 25, # Frames per second\n",
    "    'pipeline' : pipeline,\n",
    "    'control-model' : control,\n",
    "    'challenger-model-list' : challenger_list,\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'skip-frames' : 790, # the # of frames to capture in the output video\n",
    "    'confidence-target' : 0.90, # only display bounding boxes with confidence > provided #\n",
    "    'inference': 'WALLAROO_SDK',\n",
    "    'record-start-frame' : 791, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : 890, # the # of frames to capture in the output video  \n",
    "    'note-list' : [ { 'note' : ' Notice ONLY the mobilenet or resnet50 model bounding boxes is displayed',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': noteStart,\n",
    "                      'end-frame': noteStart + 100},\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 100,\n",
    "                      'end-frame': noteStart + 150  },\n",
    "                     { 'note' : 'The model with the best classification confidence %',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': noteStart + 150,\n",
    "                      'end-frame': noteStart + 250 },\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 100,\n",
    "                      'end-frame': noteStart + 150  },\n",
    "                      { 'note' : 'This is reflected in the [Wins] column',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': 807,\n",
    "                      'end-frame': 811 },\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 100,\n",
    "                      'end-frame': noteStart + 150  },\n",
    "                      { 'note' : 'The best of both of worlds!',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': 812,\n",
    "                      'end-frame': 816 }\n",
    "                    ]\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.useBestObjectDetectorInVideoUsingShadowDeployment(config)\n",
    "print(\"We are done.\")\n",
    "\n",
    " 'note-list' : [ { 'note' : ' The Cyan color rectangles are the mobilenet object detections',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart,\n",
    "                      'end-frame': noteStart + 100},\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 100,\n",
    "                      'end-frame': noteStart + 150  },\n",
    "                     { 'note' : 'The Orange color rectangles are the mobilenet object detections',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 150,\n",
    "                      'end-frame': noteStart + 250 },\n",
    "                     { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 250,\n",
    "                      'end-frame': noteStart + 300  },\n",
    "                      { 'note' : 'A great visulization for the data scientist. ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 300,\n",
    "                      'end-frame': noteStart + 350 },\n",
    "                    { 'note' : ' ',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 350,\n",
    "                      'end-frame': noteStart + 375  },\n",
    "                    { 'note' : 'Enabling data scientists to determine which model is better',\n",
    "                      'color' : CVDemo.MAGENTA,\n",
    "                      'start-frame': noteStart + 375,\n",
    "                      'end-frame': noteStart + 475 }\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Insights - Anomalies\n",
    "\n",
    "Wallaroo is a wonderful platform to help you gain insights into the potential anomalies that maybe occuring in your\n",
    "production computer vision models.\n",
    "\n",
    "In the example below we are going to setup a wallaroo custom anomoly that will track and display detected objects with a confidence score below 75%.\n",
    "\n",
    "These are opportunities for the data scientist to inspect each anomoly to determine if the model can be retrained to include or discard the anomoly detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-anomalies-intro.mp4\"\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 15, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'skip-frames' : 890, # the # of frames to capture in the output video\n",
    "    'record-start-frame' : 891, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : 970, # the # of frames to capture in the output video    'skip-frames' : 75, # the # of frames to capture in the output video\n",
    "    'dashboard-message-list' : [\"Wallaroo Anomalies helps data scientists quickly  \", \n",
    "                                \"and easily identify anomalies in object detection models.\", \n",
    "                                \"Let's check it out\"],\n",
    "    'dashboard-start-frame' : 900 ,\n",
    "    'dashboard-end-frame' : 960 ,\n",
    "    'color':CVDemo.WHITE\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.recordVideo(config)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = wl.list_workspaces()\n",
    "for w in ws:\n",
    "    if w.name() == 'computer-vision':\n",
    "        wl.set_current_workspace(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mobilenet'\n",
    "mobilenet_model = wl.upload_model('mobilenet', \"models/mobilenet.pt.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will add our post processing anomoly detection file called post-process-anomoly-detection.py.  Predictions that are lower than 75% we will consider anomalies that need to be inspected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_anomoly_detection = wl.upload_model(\"post-process-anomoly-detection\", \"./post-process-anomoly-detection.py\").configure('python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deploy our custom anomoly detection\n",
    "\n",
    "Next we will deploy our pipeline with the custom anomoly detection logic as a post processing step in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(1).memory(\"12Gi\").build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = 'anomoly-pp'\n",
    "pipeline = wl.build_pipeline(pipeline_name) \\\n",
    "            .add_model_step(mobilenet_model) \\\n",
    "            .add_model_step(module_anomoly_detection)\n",
    "\n",
    "pipeline.deploy(deployment_config = deployment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = pipeline._deployment._url()\n",
    "print(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the pipeline in a video stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Initialize some Vars\n",
    "\n",
    "Initialize the COCO Classes, meaning the classificaitons found on the images and the default width and height all images are resized to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from CVDemoUtils import CVDemo\n",
    "\n",
    "# set the device we will be using to run the model\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# The set of COCO classifications\n",
    "CLASSES = pickle.loads(open(\"models/coco_classes.pickle\", \"rb\").read())\n",
    "\n",
    "# Unique colors for each identified COCO class\n",
    "COLORS = np.random.uniform(0, 255, size=(len(CLASSES), 3))\n",
    "\n",
    "# The size the image will be resized to\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "# Only objects that have a confidence > confidence_target will be displayed on the image\n",
    "confidence_target = 0.75\n",
    "\n",
    "cvDemo = CVDemo(CLASSES,COLORS, DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Detect and Classify Anomalies in the Video Stream\n",
    "\n",
    "Next we will load each frame in the input-video feed itto the pipeline for inferencing.  The pipeline will perform normal inferencing logic and then before returning the results execute our custom anomoly detection.  The anomalies detected, meaning their bounding boxes, their classification and confidence of classifications are included in the inferencing results.\n",
    "\n",
    "We use these results to draw all the anomalies their bounding boxes, each identified object,  its classification, and the model's confidence that the prediction and then save the frame to an output video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The size the image will be resized to\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-anomalies-inferenced.mp4\"\n",
    "\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 15, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    #'max-frames' : 400, # the # of frames to capture in the output video\n",
    "    'skip-frames' : 960, # the # of frames to capture in the output video\n",
    "    'color':CVDemo.ORANGE, # color to draw bounding boxes and the text in the statistics\n",
    "     #'inference' : 'WALLAROO_SDK' is only supported for anomalies right now\n",
    "    'model_name' : model_name,\n",
    "    'pipeline' : pipeline, # provide this when using inference WALLAROO_SDK \n",
    "    'pipeline_name' : pipeline_name,\n",
    "    'record-start-frame' : 962, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : 1060, # the # of frames to capture in the output video  \n",
    "    'note-list' : [ { 'note' : 'Object detections below 75% is an anomaly.',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': 964,\n",
    "                      'end-frame': 968},\n",
    "                     { 'note' : 'Each bounding box identifies an anomoly.',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': 970,\n",
    "                      'end-frame': 975 },\n",
    "                      { 'note' : 'The [Anom] column tells us how many.',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': 978,\n",
    "                      'end-frame': 983 },\n",
    "                     { 'note' : 'Notice the avg conf % [conf] is extremely low.',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': 985,\n",
    "                      'end-frame': 990 }\n",
    "                      { 'note' : 'Great opportunities for model improvement identified.',\n",
    "                      'color' : CVDemo.CYAN,\n",
    "                      'start-frame': 985,\n",
    "                      'end-frame': 990 }\n",
    "                    ]\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.detectAndClassifyAnomaliesInVideo(config)\n",
    "print(\"We are done.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### To learn more message\n",
    "\n",
    "Lets add a learn more message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 100\n",
    "startFrame = endFrame\n",
    "startFrame = 1780\n",
    "endFrame = startFrame + duration\n",
    "\n",
    "\n",
    "input_video = \"videos/amazon-fresh-go.mp4\"\n",
    "output_video = \"videos/amazon-fresh-go-step-9-conclusion.mp4\"\n",
    "\n",
    "config = {\n",
    "    'input-video' : input_video, # source video\n",
    "    'output-video' : output_video, # show the input video with the inferenced results drawn on each frame\n",
    "    'fps' : 15, # Frames per second\n",
    "    'width' : width, # the width of the url\n",
    "    'height' : height, # the height of the url\n",
    "    'skip-frames' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-start-frame' : startFrame, # the # of frames to capture in the output video\n",
    "    'record-end-frame' : endFrame, # the # of frames to capture in the output video    'skip-frames' : 75, # the # of frames to capture in the output video\n",
    "    'dashboard-message-list' : [\"To learn how to do this with your computer vision \",\n",
    "                                \"models checkout the Wallaroo Tutorials.\", \n",
    "                                \"Thanks for watching!\"],\n",
    "    'dashboard-start-frame' : startFrame + padding ,\n",
    "    'dashboard-end-frame' : startFrame - padding ,\n",
    "    'color':CVDemo.WHITE,\n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.recordVideo(config)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conclusion\n",
    "\n",
    "To stitch it all together, we will take each video generated in this tutorial and concatenate them into 1 final video\n",
    "\n",
    "videos/amazon-fresh-go-final.mp4\n",
    "\n",
    "That is it!  You now know how to use wallaroo to improve your computer vision projects.  Good luck with your efforts!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_video = \"\"\n",
    "\n",
    "config = {\n",
    "    'video-list' : ['videos/amazon-fresh-go-step-4-intro-cv.mp4',\n",
    "                    'videos/amazon-fresh-go-step-4-mobilenet-intro.mp4',\n",
    "                    'videos/amazon-fresh-go-step-4-mobilenet-inferenced.mp4',\n",
    "                    'videos/amazon-fresh-go-step-4-resnet-intro.mp4',\n",
    "                    'videos/amazon-fresh-go-step-4-resnet-inferenced.mp4',\n",
    "                    'videos/amazon-fresh-go-step-5-shadow-part-1-intro.mp4',\n",
    "                    'videos/amazon-fresh-go-step-5-shadow-part-1-inferenced.mp4',\n",
    "                    #'videos/amazon-fresh-go-shadow-part-2-intro.mp4',\n",
    "                    #'videos/amazon-fresh-go-shadow-part-2-inferenced.mp4',\n",
    "                    #'videos/amazon-fresh-go-anomalies-intro.mp4', \n",
    "                    #'videos/amazon-fresh-go-anomalies-inferenced.mp4',\n",
    "                    'videos/amazon-fresh-go-step-9-conclusion.mp4'\n",
    "                   ], # source video\n",
    "    'output-video' : 'videos/amazon-fresh-go-step-final.mp4', # record all videos stitched together in this file\n",
    "    'fps': 15,\n",
    "    'width' : width, # the width of the videos\n",
    "    'height' : height, # the height of the videos \n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.stichVideosTogether(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_video = \"\"\n",
    "\n",
    "config = {\n",
    "    'video-list' : [\n",
    "                    'videos/amazon-fresh-go-shadow-part-1-inferenced.mp4',\n",
    "                    'videos/amazon-fresh-go-shadow-part-2-intro.mp4',\n",
    "                  #  'videos/amazon-fresh-go-shadow-part-2-inferenced.mp4',\n",
    "                   ], # source video\n",
    "    'output-video' : 'videos/test-shadow.mp4', # record all videos stitched together in this file\n",
    "    'fps': 15,\n",
    "    'width' : width, # the width of the videos\n",
    "    'height' : height, # the height of the videos \n",
    "}\n",
    "cvDemo.DEBUG = False\n",
    "cvDemo.stichVideosTogether(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in wl.list_deployments():\n",
    "    d.undeploy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
