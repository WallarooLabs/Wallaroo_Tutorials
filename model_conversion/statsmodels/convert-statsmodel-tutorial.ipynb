{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/model_conversion/statsmodels).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Organizations can deploy a Machine Learning (ML) model based on the [statsmodels](https://www.statsmodels.org/stable/index.html) directly into Wallaroo through the following process.  This conversion process transforms the model into an open format that can be run across different frameworks at compiled C-language speeds.\n",
    "\n",
    "This example provides the following:\n",
    "\n",
    "* `train-statsmodel.ipynb`: A sample Jupyter Notebook that trains a sample model.  The model predicts how many bikes will be rented on each of the next 7 days, based on the previous 7 days' bike rentals, temperature, and wind speed.  Additional files to support this example are:\n",
    "  * `day.csv`: Data used to train the sample `statsmodel` example.\n",
    "  * `infer.py`: The inference script that is part of the `statsmodel`.\n",
    "* `convert-statsmodel-tutorial.ipynb`: A sample Jupyter Notebook that demonstrates how to upload, convert, and deploy the `statsmodel` example into a Wallaroo instance.    Additional files to support this example are:\n",
    "  * `bike_day_model.pkl`: A `statsmodel` ML model trained from the `train-statsmodel.ipynb` Notebook.\n",
    "\n",
    "    **IMPORTANT NOTE:** The `statsmodel` ML model is composed of two parts that are contained in the .pkl file:\n",
    "\n",
    "    * The pickled Python runtime expects a dictionary with two keys: `model` and `script`:\n",
    "\n",
    "      * `model`—the pickled model, which will be automatically loaded into the python runtime with the name 'model'\n",
    "      * `script`—the text of the python script to be run, in a format similar to the existing python script steps (i.e. defining a wallaroo_json method which operates on the data).  In this case, the file `infer.py` is the script used.\n",
    "\n",
    "  * `bike_day_eval.json`: Evaluation data used to test the model's performance.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before uploading and running an inference with a MLFlow model in Wallaroo the following will be required:\n",
    "\n",
    "* An installed Wallaroo instance.\n",
    "* The following Python libraries installed:\n",
    "  * `os`\n",
    "  * [`wallaroo`](https://pypi.org/project/wallaroo/): The Wallaroo SDK. Included with the Wallaroo JupyterHub service by default.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "The following steps will perform the following:\n",
    "    \n",
    "1. Upload the `statsmodel` ML model `bike_day_model.pkl` into a Wallaroo.\n",
    "2. Deploy the model into a pipeline.\n",
    "3. Run a test inference.\n",
    "4. Undeploy the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "The first step is to import the libraries that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "\n",
    "import os\n",
    "# Only set the below to make the OS environment ARROW_ENABLED to TRUE.  Otherwise, leave as is.\n",
    "os.environ[\"ARROW_ENABLED\"]=\"True\"\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize connection\n",
    "\n",
    "Start a connect to the Wallaroo instance and save the connection into the variable `wl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login through local Wallaroo instance\n",
    "\n",
    "wl = wallaroo.Client()\n",
    "\n",
    "# SSO login through keycloak\n",
    "\n",
    "wallarooPrefix = \"YOUR PREFIX\"\n",
    "wallarooSuffix = \"YOUR PREFIX\"\n",
    "\n",
    "wallarooPrefix = \"doc-test\"\n",
    "wallarooSuffix = \"wallaroocommunity.ninja\"\n",
    "\n",
    "wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n",
    "                    auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n",
    "                    auth_type=\"sso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Configurations\n",
    "\n",
    "The following will set the workspace, model name, and pipeline that will be used for this example.  If the workspace or pipeline already exist, then they will assigned for use in this example.  If they do not exist, they will be created based on the names listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_name = 'bikedayevalworkspace2'\n",
    "pipeline_name = 'bikedayevalpipeline2'\n",
    "model_name = 'bikedaymodel2'\n",
    "model_file_name = 'bike_day_model.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Workspace and Pipeline\n",
    "\n",
    "This sample code will create or use the existing workspace `bike-day-workspace` as the current workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>bikedayevalpipeline2</td></tr><tr><th>created</th> <td>2023-03-31 13:46:40.869639+00:00</td></tr><tr><th>last_updated</th> <td>2023-03-31 14:16:28.298221+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>77ccc416-9081-4b2e-8ec7-8a01a4a3882c, e856e7fb-8177-450e-81a9-1682a7173228, 79f98d55-5b64-4484-af0e-629021a0af7b, d2b156ed-6faf-4f69-81d7-94befdc400c8, ad996a55-05fa-4868-93b9-5c97c7b0e2b3, c9283735-bce7-4bfc-89f6-fb0a7c4832e5, 2957dd52-7784-467f-967e-b4e3029ff60e</td></tr><tr><th>steps</th> <td>bikedaymodel2</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'bikedayevalpipeline2', 'create_time': datetime.datetime(2023, 3, 31, 13, 46, 40, 869639, tzinfo=tzutc()), 'definition': '[]'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_workspace(name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "def get_pipeline(name):\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(name)\n",
    "    return pipeline\n",
    "\n",
    "workspace = get_workspace(workspace_name)\n",
    "\n",
    "wl.set_current_workspace(workspace)\n",
    "\n",
    "pipeline = get_pipeline(pipeline_name)\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Pickled Package Statsmodel Model\n",
    "\n",
    "Upload the statsmodel stored into the pickled package `bike_day_model.pkl`.  See the Notebook `train-statsmodel.ipynb` for more details on creating this package.\n",
    "\n",
    "Note that this package is being specified as a `python` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_day_model = wl.upload_model(model_name, model_file_name).configure(runtime=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Pipeline\n",
    "\n",
    "We will now add the uploaded model as a step for the pipeline, then deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>bikedayevalpipeline2</td></tr><tr><th>created</th> <td>2023-03-31 13:46:40.869639+00:00</td></tr><tr><th>last_updated</th> <td>2023-03-31 14:16:28.298221+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>77ccc416-9081-4b2e-8ec7-8a01a4a3882c, e856e7fb-8177-450e-81a9-1682a7173228, 79f98d55-5b64-4484-af0e-629021a0af7b, d2b156ed-6faf-4f69-81d7-94befdc400c8, ad996a55-05fa-4868-93b9-5c97c7b0e2b3, c9283735-bce7-4bfc-89f6-fb0a7c4832e5, 2957dd52-7784-467f-967e-b4e3029ff60e</td></tr><tr><th>steps</th> <td>bikedaymodel2</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'bikedayevalpipeline2', 'create_time': datetime.datetime(2023, 3, 31, 13, 46, 40, 869639, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'bikedaymodel2', 'version': '183ce72a-1b88-4fb3-a939-2762ee86a1fe', 'sha': '978f2274c384f4050f2ece7c96a5f9bb5d6f96b387c0c5cbea5d4f36e2fbf18e'}]}}]\"}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.add_model_step(bike_day_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><th>name</th> <td>bikedayevalpipeline2</td></tr><tr><th>created</th> <td>2023-03-31 13:46:40.869639+00:00</td></tr><tr><th>last_updated</th> <td>2023-03-31 14:19:32.701608+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>c9266ee7-fb33-4f02-a97b-86962b686477, 77ccc416-9081-4b2e-8ec7-8a01a4a3882c, e856e7fb-8177-450e-81a9-1682a7173228, 79f98d55-5b64-4484-af0e-629021a0af7b, d2b156ed-6faf-4f69-81d7-94befdc400c8, ad996a55-05fa-4868-93b9-5c97c7b0e2b3, c9283735-bce7-4bfc-89f6-fb0a7c4832e5, 2957dd52-7784-467f-967e-b4e3029ff60e</td></tr><tr><th>steps</th> <td>bikedaymodel2</td></tr></table>"
      ],
      "text/plain": [
       "{'name': 'bikedayevalpipeline2', 'create_time': datetime.datetime(2023, 3, 31, 13, 46, 40, 869639, tzinfo=tzutc()), 'definition': \"[{'ModelInference': {'models': [{'name': 'bikedaymodel2', 'version': '183ce72a-1b88-4fb3-a939-2762ee86a1fe', 'sha': '978f2274c384f4050f2ece7c96a5f9bb5d6f96b387c0c5cbea5d4f36e2fbf18e'}]}}]\"}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'Running',\n",
       " 'details': [],\n",
       " 'engines': [{'ip': '10.244.1.18',\n",
       "   'name': 'engine-5bccd5699d-5gc4x',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': [],\n",
       "   'pipeline_statuses': {'pipelines': [{'id': 'bikedayevalpipeline2',\n",
       "      'status': 'Running'}]},\n",
       "   'model_statuses': {'models': [{'name': 'bikedaymodel2',\n",
       "      'version': '183ce72a-1b88-4fb3-a939-2762ee86a1fe',\n",
       "      'sha': '978f2274c384f4050f2ece7c96a5f9bb5d6f96b387c0c5cbea5d4f36e2fbf18e',\n",
       "      'status': 'Running'}]}}],\n",
       " 'engine_lbs': [{'ip': '10.244.2.13',\n",
       "   'name': 'engine-lb-ddd995646-lkjbt',\n",
       "   'status': 'Running',\n",
       "   'reason': None,\n",
       "   'details': []}],\n",
       " 'sidekicks': []}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Inference\n",
    "\n",
    "Perform an inference from the evaluation data JSON file `bike_day_eval.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "InferenceError",
     "evalue": "Inference failed: \n\tInternalError: Error running inference <class 'AttributeError'>, 'str' object has no attribute 'loc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/arrowtests/lib/python3.8/site-packages/wallaroo/deployment.py\u001b[0m in \u001b[0;36m_make_infer_request\u001b[0;34m(self, data, headers, params, timeout)\u001b[0m\n\u001b[1;32m    589\u001b[0m             )\n\u001b[0;32m--> 590\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/arrowtests/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://doc-test.api.wallaroocommunity.ninja/v1/api/pipelines/infer/bikedayevalpipeline2-79?dataset%5B%5D=%2A&dataset.exclude%5B%5D=metadata&dataset.separator=.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInferenceError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jf/_cj0q9d51s365wksymljdz4h0000gn/T/ipykernel_11362/2423522703.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minferData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./bike_day_eval.df.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minferData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/arrowtests/lib/python3.8/site-packages/wallaroo/pipeline.py\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupdate_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             self._last_infer_time = max(\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/arrowtests/lib/python3.8/site-packages/wallaroo/pipeline.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, tensor, timeout, dataset, dataset_exclude, dataset_separator)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mdeployment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deployment_for_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeployment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             return deployment.infer(\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_exclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_separator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             )\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/arrowtests/lib/python3.8/site-packages/wallaroo/deployment.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, tensor, timeout, dataset, dataset_exclude, dataset_separator)\u001b[0m\n\u001b[1;32m    649\u001b[0m                 )\n\u001b[1;32m    650\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_with_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_with_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/arrowtests/lib/python3.8/site-packages/wallaroo/deployment.py\u001b[0m in \u001b[0;36m_infer_with_pandas\u001b[0;34m(self, tensor, params, timeout)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPANDAS_RECORDS_HEADER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         }\n\u001b[0;32m--> 490\u001b[0;31m         res = self._make_infer_request(\n\u001b[0m\u001b[1;32m    491\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_records\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/envs/arrowtests/lib/python3.8/site-packages/wallaroo/deployment.py\u001b[0m in \u001b[0;36m_make_infer_request\u001b[0;34m(self, data, headers, params, timeout)\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhttp_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInferenceError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         except (\n\u001b[1;32m    594\u001b[0m             \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceError\u001b[0m: Inference failed: \n\tInternalError: Error running inference <class 'AttributeError'>, 'str' object has no attribute 'loc'"
     ]
    }
   ],
   "source": [
    "inferData = pd.read_json('./bike_day_eval.df.json', orient=\"records\")\n",
    "\n",
    "results = pipeline.infer(inferData)\n",
    "\n",
    "\n",
    "# results = pipeline.infer_from_file('bike_day_eval.df.json')\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undeploy the Pipeline\n",
    "\n",
    "Undeploy the pipeline and return the resources back to the Wallaroo instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dda4bf3640b7fafcd1648658b879b4cc9f6ba6084e8fb356fdaaa1a461d1690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
