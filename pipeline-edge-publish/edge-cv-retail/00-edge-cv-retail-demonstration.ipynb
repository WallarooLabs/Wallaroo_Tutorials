{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/wallaroo-model-cookbooks/computer-vision).\n",
    "\n",
    "## Step 00: Introduction and Setup\n",
    "\n",
    "This tutorial demonstrates how to use the Wallaroo to detect objects in images through the following models:\n",
    "\n",
    "* **rnn mobilenet**: A single stage object detector that performs fast inferences.  Mobilenet is typically good at identifying objects at a distance.\n",
    "* **resnet50**:  A dual stage object detector with slower inferencing but but is able to detect objects that are closer to each other.\n",
    "\n",
    "This tutorial series will demonstrate the following:\n",
    "\n",
    "* How to deploy a Wallaroo pipeline with trained rnn mobilenet model and perform sample inferences to detect objects in pictures, then display those objects.\n",
    "* How to deploy a Wallaroo pipeline with a trained resnet50 model and perform sample inferences to detect objects in pictures, then display those objects.\n",
    "* Use the Wallaroo feature shadow deploy to have both models perform inferences, then select the inference result with the higher confidence and show the objects detected.\n",
    "\n",
    "This tutorial assumes that users have installed the [Wallaroo SDK](https://pypi.org/project/wallaroo/) or are running these tutorials from within their Wallaroo instance's JupyterHub service."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This demonstration should be run within a Wallaroo JupyterHub instance for best results.\n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The included OpenCV class is included in this demonstration as `CVDemoUtils.py`, and requires the following dependencies:\n",
    "\n",
    "* ffmpeg\n",
    "* libsm\n",
    "* libxext\n",
    "\n",
    "\n",
    "### Internal JupyterHub Service\n",
    "\n",
    "To install these dependencies in the Wallaroo JupyterHub service, use the following commands from a terminal shell via the following procedure:\n",
    "\n",
    "1. Launch the JupyterHub Service within the Wallaroo install.\n",
    "1. Select **File->New->Terminal**.\n",
    "1. Enter the following:\n",
    "\n",
    "    ```bash\n",
    "    sudo apt-get update\n",
    "    ```\n",
    "\n",
    "    ```bash\n",
    "    sudo apt-get install ffmpeg libsm6 libxext6  -y\n",
    "    ```\n",
    "\n",
    "### External SDK Users\n",
    "\n",
    "For users using the Wallaroo SDK to connect with a remote Wallaroo instance, the following commands will install the required dependancies:\n",
    "\n",
    "For Linux users, this can be installed with:\n",
    "\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install ffmpeg libsm6 libxext6  -y\n",
    "```\n",
    "\n",
    "MacOS users can prepare their environments using a package manager such as [Brew](https://brew.sh/) with the following:\n",
    "\n",
    "```bash\n",
    "brew install ffmpeg libsm libxext\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Dependencies\n",
    "\n",
    "1. This repository may use large file sizes for the models.  If necessary, install [Git Large File Storage (LFS)](https://git-lfs.com) or use the [Wallaroo Tutorials Releases](https://github.com/WallarooLabs/Wallaroo_Tutorials/releases) to download a .zip file of the most recent computer vision tutorial that includes the models.\n",
    "1. Import the following Python libraries into your environment:\n",
    "    1. [torch](https://pypi.org/project/torch/)\n",
    "    1. [wallaroo](https://pypi.org/project/wallaroo/)\n",
    "    1. [torchvision](https://pypi.org/project/torchvision/)\n",
    "    1. [opencv-python](https://pypi.org/project/opencv-python/)\n",
    "    1. [onnx](https://pypi.org/project/onnx/)\n",
    "    1. [onnxruntime](https://pypi.org/project/onnxruntime/)\n",
    "    1. [imutils](https://pypi.org/project/imutils/)\n",
    "    1. [pytz](https://pypi.org/project/pytz/)\n",
    "    1. [ipywidgets](https://pypi.org/project/ipywidgets/)\n",
    "\n",
    "These can be installed by running the command below in the Wallaroo JupyterHub service.  Note the use of `pip install torch --no-cache-dir` for low memory environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install torch --no-cache-dir\n",
    "!pip install opencv-python\n",
    "!pip install onnx\n",
    "!pip install onnxruntime\n",
    "!pip install imutils\n",
    "!pip install pytz\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the tutorials will rely on these libraries and applications, so finish their installation before running the tutorials in this series."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Wallaroo Computer Vision Tutorials\n",
    "\n",
    "In order for the wallaroo tutorial notebooks to run properly, the videos directory must contain these models in the models directory.\n",
    "\n",
    "To download the Wallaroo Computer Vision models:\n",
    "\n",
    "1. Install the Google GCloud terminal application.\n",
    "1. Use the following cmd in a terminal, preferably in the `./models` directory:\n",
    "\n",
    "    ```bash\n",
    "    gcloud storage \"cp gs://wallaroo-model-zoo/open-source/computer-vision/models/*\" .\n",
    "    ```\n",
    "\n",
    "The other source is the Wallaroo Workspaces GitHub repository Release site:\n",
    "\n",
    "[Wallaroo Workshops Releases](https://github.com/WallarooLabs/Workshops/releases/tag/1.0-initial-release)\n",
    "\n",
    "Download the `Computer-Vision-Retail.zip` file - this includes the sample models.\n",
    "\n",
    "### Directory contents\n",
    "\n",
    "* coco_classes.pickle - contain the 80 COCO classifications used by resnet50 and mobilenet object detectors.  \n",
    "* frcnn-resent.pt - PyTorch resnet50 model\n",
    "* frcnn-resnet.pt.onnx - PyTorch resnet50 model converted to onnx\n",
    "* mobilenet.pt - PyTorch mobilenet model\n",
    "* mobilenet.pt.onnx - PyTorch mobilenet model converted to onnx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is available as a demonstration on using the included `CVDemoUtils.py` to convert the pre-trained image recognition models into ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError\n",
    "from wallaroo.framework import Framework\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from CVDemoUtils import CVDemo\n",
    "\n",
    "# used to display dataframe information without truncating\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# used for unique connection names\n",
    "\n",
    "import string\n",
    "import random\n",
    "suffix= ''.join(random.choice(string.ascii_lowercase) for i in range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the model\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "cvDemo = CVDemo()\n",
    "\n",
    "cvDemo.loadPytorchAndConvertToOnnx(pytorchModelPath='./models/mobilenet.pt', \n",
    "                                    sampleImagePath='./data/images/input/example/dairy_bottles.png', \n",
    "                                    width=width, \n",
    "                                    height=height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the model\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "cvDemo = CVDemo()\n",
    "\n",
    "cvDemo.loadPytorchAndConvertToOnnx(pytorchModelPath='./models/frcnn-resnet.pt', \n",
    "                                    sampleImagePath='./data/images/input/example/dairy_bottles.png', \n",
    "                                    width=width, \n",
    "                                    height=height)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computervisiontutorials",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb2d5a211cf7fee4614d0b9047a0972c7d6bf7ecea2ecd0ade6beeb12bbc9c8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
