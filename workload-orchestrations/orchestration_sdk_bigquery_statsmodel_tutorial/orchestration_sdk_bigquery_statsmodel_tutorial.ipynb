{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54d6daff",
   "metadata": {},
   "source": [
    "This can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/workload-orchestrations/orchestration_sdk_bigquery_statsmodel_tutorial).\n",
    "\n",
    "## Wallaroo ML Workload Orchestrations with Statsmodel Tutorial\n",
    "\n",
    "This tutorial provides a quick set of methods and examples regarding Wallaroo Connections and Wallaroo ML Workload Orchestration.  For full details, see the Wallaroo Documentation site.\n",
    "\n",
    "Wallaroo provides Data Connections and ML Workload Orchestrations to provide organizations with a method of creating and managing automated tasks that can either be run on demand or a regular schedule.\n",
    "\n",
    "## Definitions\n",
    "\n",
    "* **Orchestration**: A set of instructions written as a python script with a requirements library.  Orchestrations are uploaded to the Wallaroo instance as a .zip file.\n",
    "* **Task**: An implementation of an orchestration.  Tasks are run either once when requested, on a repeating schedule, or as a service.\n",
    "* **Connection**: Definitions set by MLOps engineers that are used by other Wallaroo users for connection information to a data source.  Usually paired with orchestrations.\n",
    "\n",
    "This tutorial will focus on using Google BigQuery as the data source for supplying the inference data to perform inferences through a Statsmodel ML model.\n",
    "\n",
    "## Tutorial Goals\n",
    "\n",
    "The tutorial will demonstrate the following:\n",
    "\n",
    "1. Create a Wallaroo connection to retrieving information from a Google BigQuery source table.\n",
    "1. Create a Wallaroo connection to store inference results into a Google BigQuery destination table.\n",
    "1. Upload Wallaroo ML Workload Orchestration that supports BigQuery connections with the connection details.\n",
    "1. Run the orchestration once as a Run Once Task and verify that the inference request succeeded and the inference results were saved to the external data store.\n",
    "1. Schedule the orchestration as a Scheduled Task and verify that the inference request succeeded and the inference results were saved to the external data store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67e1bb26",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "* An installed Wallaroo instance.\n",
    "* The following Python libraries installed.  These are included by default in a Wallaroo instance's JupyterHub service.\n",
    "  * `os`\n",
    "  * [`wallaroo`](https://pypi.org/project/wallaroo/): The Wallaroo SDK. Included with the Wallaroo JupyterHub service by default.\n",
    "  * [`pandas`](https://pypi.org/project/pandas/): Pandas, mainly used for Pandas DataFrame\n",
    "  * [`pyarrow`](https://pypi.org/project/pyarrow/): PyArrow for Apache Arrow support\n",
    "* The following Python libraries.  These are **not** included in a Wallaroo instance's JupyterHub service.\n",
    "  * [`google-cloud-bigquery`](https://pypi.org/project/google-cloud-bigquery/): Specifically for its support for Google BigQuery.\n",
    "  * [`google-auth`](https://pypi.org/project/google-auth/): Used to authenticate for bigquery.\n",
    "  * [`db-dtypes`](https://pypi.org/project/db-dtypes/): Converts the BigQuery results to Apache Arrow table or pandas DataFrame.\n",
    "\n",
    "\n",
    "## Tutorial Resources\n",
    "\n",
    "* Models:\n",
    "  * `models/bike_day_model.pkl`: The statsmodel model predicts how many bikes will be rented on each of the next 7 days, based on the previous 7 days' bike rentals, temperature, and wind speed.  This model only accepts shapes (7,4) - 7 rows (representing the last 7 days) and 4 columns (representing the fields `temp`, `holiday`, `workingday`, `windspeed`).  Additional files to support this example are:\n",
    "  * `infer.py`: The inference script that is part of the `statsmodel`.\n",
    "* Data:\n",
    "  * `data/day.csv`: Data used to train the sample `statsmodel` example.\n",
    "  * `data/bike_day_eval.json`: Data used for inferences.  This will be transferred to a BigQuery table as shown in the demonstration.\n",
    "* Resources:\n",
    "  * `resources/bigquery_service_account_input_key.json`: Example service key to authenticate to a Google BigQuery project with the dataset and table used for the inference data.\n",
    "  * `resources/bigquery_service_account_output_key.json`: Example service key to authenticate to a Google BigQuery project with the dataset and table used for the inference result data.\n",
    "  * `resources/statsmodel_forecast_inputs.sql`: SQL script to create the inputs table schema, populated with the values from `./data/day.csv`.\n",
    "  * `resources/statsmodel_forecast_outputs.sql`: SQL script to create the outputs table schema."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2b09330-0408-45eb-b321-b48b65041789",
   "metadata": {},
   "source": [
    "## Initial Steps\n",
    "\n",
    "For this tutorial, we'll create a workspace, upload our sample model and deploy a pipeline.  We'll perform some quick sample inferences to verify that everything it working.\n",
    "\n",
    "### Load Libraries\n",
    "\n",
    "Here we'll import the various libraries we'll use for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee5f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wallaroo\n",
    "from wallaroo.object import EntityNotFoundError, RequiredAttributeMissing\n",
    "\n",
    "# to display dataframe tables\n",
    "from IPython.display import display\n",
    "# used to display dataframe information without truncating\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import pyarrow as pa\n",
    "\n",
    "import time\n",
    "import json\n",
    "\n",
    "# for Big Query connections\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "import db_dtypes\n",
    "\n",
    "import string\n",
    "import random\n",
    "\n",
    "suffix= ''.join(random.choice(string.ascii_lowercase) for i in range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae92d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "wallaroo.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae645fd0",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "The first step is to connect to Wallaroo through the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  If logging in externally, update the `wallarooPrefix` and `wallarooSuffix` variables with the proper DNS information.  For more information on Wallaroo DNS settings, see the [Wallaroo DNS Integration Guide](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-dns-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d19a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login through local Wallaroo instance\n",
    "\n",
    "wl = wallaroo.Client()\n",
    "wallarooPrefix = \"product-uat-ee\"\n",
    "wallarooSuffix = \"wallaroocommunity.ninja\"\n",
    "\n",
    "wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n",
    "                    auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n",
    "                    auth_type=\"sso\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a8340b1",
   "metadata": {},
   "source": [
    "## Variable Declaration\n",
    "\n",
    "The following variables will be used for our big query testing.  \n",
    "\n",
    "We'll use two connections:\n",
    "\n",
    "* bigquery_input_connection: The connection that will draw inference input data from a BigQuery table.\n",
    "* bigquery_output_connection:  The connection that will upload inference results into a BigQuery table.\n",
    "\n",
    "Not that for the connection arguments, we'll retrieve the information from the files `./bigquery_service_account_input_key.json` and `./bigquery_service_account_output_key.json` that include the  [service account key file(SAK)](https://cloud.google.com/bigquery/docs/authentication/service-account-file) information, as well as the dataset and table used.\n",
    "\n",
    "| Field | Included in SAK | \n",
    "|---|---|\n",
    "| type | âˆš | \n",
    "| project_id | âˆš |\n",
    "| private_key_id | âˆš |\n",
    "| private_key | âˆš |\n",
    "| client_email | âˆš |\n",
    "| auth_uri | âˆš |\n",
    "| token_uri | âˆš |\n",
    "| auth_provider_x509_cert_url | âˆš |\n",
    "| client_x509_cert_url | âˆš |\n",
    "| database | ðŸš« |\n",
    "| table | ðŸš« |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a981ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables for later steps\n",
    "\n",
    "workspace_name = f'bigquerystatsmodelworkspace{suffix}'\n",
    "pipeline_name = f'bigquerystatsmodelpipeline{suffix}'\n",
    "model_name = f'bigquerystatsmodelmodel{suffix}'\n",
    "model_file_name = './models/bike_day_model.pkl'\n",
    "\n",
    "bigquery_connection_input_name = f'bigqueryforecastinputs{suffix}'\n",
    "bigquery_connection_input_type = \"BIGQUERY\"\n",
    "bigquery_connection_input_argument = json.load(open('./resources/bigquery_service_account_input_key.json.example'))\n",
    "\n",
    "bigquery_connection_output_name = f'bigqueryforecastoutputs{suffix}'\n",
    "bigquery_connection_output_type = \"BIGQUERY\"\n",
    "bigquery_connection_output_argument = json.load(open('./resources/bigquery_service_account_output_key.json.example'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c46c722",
   "metadata": {},
   "source": [
    "### Helper Methods\n",
    "\n",
    "The following helper methods are used to either create or get workspaces, pipelines, and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0361a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods to retrieve workspaces and pipelines\n",
    "\n",
    "def get_workspace(name):\n",
    "    workspace = None\n",
    "    for ws in wl.list_workspaces():\n",
    "        if ws.name() == name:\n",
    "            workspace= ws\n",
    "    if(workspace == None):\n",
    "        workspace = wl.create_workspace(name)\n",
    "    return workspace\n",
    "\n",
    "def get_pipeline(name):\n",
    "    try:\n",
    "        pipeline = wl.pipelines_by_name(name)[0]\n",
    "    except EntityNotFoundError:\n",
    "        pipeline = wl.build_pipeline(name)\n",
    "    return pipeline\n",
    "\n",
    "def get_connection(name, connection_type, connection_arguments):\n",
    "    try:\n",
    "        connection = wl.get_connection(name)\n",
    "    except RequiredAttributeMissing:\n",
    "        connection =wl.create_connection(name, \n",
    "                  connection_type, \n",
    "                  connection_arguments)\n",
    "    return connection\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98b78cf9",
   "metadata": {},
   "source": [
    "### Create the Workspace and Pipeline\n",
    "\n",
    "We'll now create our workspace and pipeline for the tutorial.  If this tutorial has been run previously, then this will retrieve the existing ones with the assumption they're for us with this tutorial.\n",
    "\n",
    "We'll set the retrieved workspace as the current workspace in the SDK, so all commands will default to that workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace = get_workspace(workspace_name)\n",
    "wl.set_current_workspace(workspace)\n",
    "\n",
    "pipeline = get_pipeline(pipeline_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a9fc274",
   "metadata": {},
   "source": [
    "### Upload the Model and Deploy Pipeline\n",
    "\n",
    "We'll upload our model into our sample workspace, then add it as a pipeline step before deploying the pipeline to it's ready to accept inference requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a69610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the model\n",
    "\n",
    "bike_day_model = wl.upload_model(model_name, model_file_name).configure(runtime=\"python\")\n",
    "\n",
    "# Add the model as a pipeline step\n",
    "\n",
    "pipeline.add_model_step(bike_day_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy the pipeline\n",
    "pipeline.deploy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83e219a1",
   "metadata": {},
   "source": [
    "### Sample Inferences\n",
    "\n",
    "We'll perform some quick sample inferences with the local file data to verity the pipeline deployed and is ready for inferences.  Once done, we'll undeploy the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform inferences\n",
    "\n",
    "results = pipeline.infer_from_file('./data/bike_day_eval.json', data_format=\"custom-json\")\n",
    "print(results)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "573bc347",
   "metadata": {},
   "source": [
    "## Create Connections\n",
    "\n",
    "We will create the data source connection via the Wallaroo client command `create_connection`.\n",
    "\n",
    "Connections are created with the Wallaroo client command [`create_connection`](https://staging.docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-ml-workload-orchestration/#create-orchestration) with the following parameters.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| --- | --- | ---|\n",
    "| **name** | string (Required) | The name of the connection. This must be unique - **if submitting the name of an existing** connection it will return an error. |\n",
    "| **type** | string (Required) | The user defined type of connection. |\n",
    "| **details** | Dict (Required) | User defined configuration details for the data connection.  These can be `{'username':'dataperson', 'password':'datapassword', 'port': 3339}`, or `{'token':'abcde123==', 'host':'example.com', 'port:1234'}`, or other user defined combinations.  |\n",
    "\n",
    "* **IMPORTANT NOTE**:  Data connections names **must** be unique.  Attempting to create a data connection with the same `name` as an existing data connection will result in an error.\n",
    "\n",
    "See the `statsmodel_forecast_inputs` and `statsmodel_forecast_outputs` details listed above for the table schema used for our example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4616c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_input = get_connection(bigquery_connection_input_name, bigquery_connection_input_type, bigquery_connection_input_argument)\n",
    "connection_output = get_connection(bigquery_connection_output_name, bigquery_connection_output_type, bigquery_connection_output_argument)\n",
    "\n",
    "display(connection_input)\n",
    "display(connection_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6caf6e64",
   "metadata": {},
   "source": [
    "### Get Connection by Name\n",
    "\n",
    "The Wallaroo client method `get_connection(name)` retrieves the connection that matches the `name` parameter.  We'll retrieve our connection and store it as `inference_source_connection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc80cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_query_input_connection = wl.get_connection(name=bigquery_connection_input_name)\n",
    "big_query_output_connection = wl.get_connection(name=bigquery_connection_output_name)\n",
    "display(big_query_input_connection)\n",
    "display(big_query_output_connection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "884d3171",
   "metadata": {},
   "source": [
    "### Add Connection to Workspace\n",
    "\n",
    "The method Workspace [`add_connection(connection_name)`](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-dataconnections/#add-data-connection-to-workspace) adds a Data Connection to a workspace, and takes the following parameters.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| --- | --- | ---|\n",
    "| **name** | string (Required) | The name of the Data Connection |\n",
    "\n",
    "We'll add both connections to our sample workspace, then list the connections available to the workspace to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d674d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace.add_connection(bigquery_connection_input_name)\n",
    "workspace.add_connection(bigquery_connection_output_name)\n",
    "\n",
    "\n",
    "workspace.list_connections()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9f60bbf",
   "metadata": {},
   "source": [
    "## Big Query Connection Inference Example\n",
    "\n",
    "We can test the BigQuery connection with a simple inference to our deployed pipeline.  We'll request the data, format the table into a pandas DataFrame, then submit it for an inference request.\n",
    "\n",
    "### Create Google Credentials\n",
    "\n",
    "From our BigQuery request, we'll create the credentials for our BigQuery connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4b88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_input_credentials = service_account.Credentials.from_service_account_info(\n",
    "    big_query_input_connection.details())\n",
    "\n",
    "bigquery_output_credentials = service_account.Credentials.from_service_account_info(\n",
    "    big_query_output_connection.details())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d316530",
   "metadata": {},
   "source": [
    "### Connect to Google BigQuery\n",
    "\n",
    "We can now generate a client from our connection details, specifying the project that was included in the `big_query_connection` details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2e8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigqueryinputclient = bigquery.Client(\n",
    "    credentials=bigquery_input_credentials, \n",
    "    project=big_query_input_connection.details()['project_id']\n",
    ")\n",
    "bigqueryoutputclient = bigquery.Client(\n",
    "    credentials=bigquery_output_credentials, \n",
    "    project=big_query_output_connection.details()['project_id']\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05f91f2b",
   "metadata": {},
   "source": [
    "### Query Data\n",
    "\n",
    "Now we'll create our query and retrieve information from out dataset and table as defined in the file `bigquery_service_account_key.json`.\n",
    "\n",
    "We'll grab the last 7 days of data - with every record assumed to be one day - and use that for our inference request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_dataframe_input = bigqueryinputclient.query(\n",
    "        f\"\"\"\n",
    "        (select dteday, temp, holiday, workingday, windspeed\n",
    "        FROM {big_query_input_connection.details()['dataset']}.{big_query_input_connection.details()['table']}\n",
    "        ORDER BY dteday DESC LIMIT 7)\n",
    "        ORDER BY dteday\n",
    "        \"\"\"\n",
    "    ).to_dataframe().drop(columns=['dteday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a dict, show the first 7 rows\n",
    "display(inference_dataframe_input.to_dict())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1eedb4f",
   "metadata": {},
   "source": [
    "### Sample Inference\n",
    "\n",
    "With our data retrieved, we'll perform an inference and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy the pipeline\n",
    "pipeline.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8ab22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline.infer(inference_dataframe_input.to_dict())\n",
    "display(results[0]['forecast'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7620230",
   "metadata": {},
   "source": [
    "### Upload the Results\n",
    "\n",
    "With the query complete, we'll upload the results back to the BigQuery dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe1115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_table = bigqueryoutputclient.get_table(f\"{big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\")\n",
    "\n",
    "job = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        INSERT {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        VALUES\n",
    "        (current_timestamp(), \"{results[0]['forecast']}\")\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last insert to the output table to verify\n",
    "# wait 10 seconds for the insert to finish\n",
    "time.sleep(10)\n",
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8d44dc7",
   "metadata": {},
   "source": [
    "## Wallaroo ML Workload Orchestration Example\n",
    "\n",
    "With the pipeline deployed and our connections set, we will now generate our ML Workload Orchestration.  See the [Wallaroo ML Workload Orchestrations guide](https://staging.docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-ml-workload-orchestration/) for full details.\n",
    "\n",
    "Orchestrations are uploaded to the Wallaroo instance as a ZIP file with the following requirements:\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| **User Code** | (*Required*) Python script as `.py` files | If `main.py` exists, then that will be used as the task entrypoint. Otherwise, the **first** `main.py` found in any subdirectory will be used as the entrypoint. |\n",
    "| Python Library Requirements | (*Optional*) `requirements.txt` file in the [requirements file format](https://pip.pypa.io/en/stable/reference/requirements-file-format/).  A standard Python requirements.txt for any dependencies to be provided in the task environment. The Wallaroo SDK will already be present and **should not be included in the requirements.txt**. Multiple requirements.txt files are not allowed. |\n",
    "| Other artifacts | &nbsp; | Other artifacts such as files, data, or code to support the orchestration.\n",
    "\n",
    "For our example, our orchestration will:\n",
    "\n",
    "1. Use the `bigquery_remote_inference` to open a connection to the input and output tables.\n",
    "1. Deploy the pipeline.\n",
    "1. Perform an inference with the input data.\n",
    "1. Save the inference results to the output table.\n",
    "1. Undeploy the pipeline.\n",
    "\n",
    "This sample script is stored in `bigquery_statsmodel_remote_inference/main.py` with an `requirements.txt` file having the specific libraries for the Google BigQuery connection., and packaged into the orchestration as `./bigquery_statsmodel_remote_inference/bigquery_statsmodel_remote_inference.zip`.  We'll display the steps in uploading the orchestration to the Wallaroo instance.\n",
    "\n",
    "Note that the orchestration assumes the pipeline is already deployed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c54d9602",
   "metadata": {},
   "source": [
    "### Upload the Orchestration\n",
    "\n",
    "Orchestrations are uploaded with the Wallaroo client `upload_orchestration(path)` method with the following parameters.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| --- | --- | ---|\n",
    "| **path** | string (Required) | The path to the ZIP file to be uploaded. |\n",
    "\n",
    "Once uploaded, the deployment will be prepared and any requirements will be downloaded and installed.\n",
    "\n",
    "\n",
    "For this example, the orchestration `./bigquery_remote_inference/bigquery_remote_inference.zip` will be uploaded and saved to the variable `orchestration`.  Then we will loop until the orchestration status is `ready`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43804b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6e6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestration = wl.upload_orchestration(path=\"./bigquery_remote_inference/bigquery_remote_inference.zip\")\n",
    "\n",
    "while orchestration.status() != 'ready':\n",
    "    print(orchestration.status())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45beade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.list_orchestrations()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf028e54",
   "metadata": {},
   "source": [
    "## Task Management Tutorial\n",
    "\n",
    "Once an Orchestration has the status `ready`, it can be run as a task.  Tasks have three run options.\n",
    "\n",
    "| Type | SDK Call |  How triggered |\n",
    "|---|---|:---|\n",
    "| Once       | `orchestration.run_once(name, json_args, timeout)` | Task runs once and exits.| Single batch, experimentation. |\n",
    "| Scheduled  | `orchestration.run_scheduled(name, schedule, timeout, json_args)` | User provides schedule. Task runs exits whenever schedule dictates. | Recurrent batch. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de1423fb",
   "metadata": {},
   "source": [
    "### Run Task Once\n",
    "\n",
    "We'll do both a Run Once task and generate our Run Once Task from our orchestration.\n",
    "\n",
    "Tasks are generated and run once with the Orchestration `run_once(name, json_args, timeout)` method.  Any arguments for the orchestration are passed in as a `Dict`.  If there are no arguments, then an empty set `{}` is passed.\n",
    "\n",
    "We'll display the last 5 rows of our BigQuery output table, then start the task that will perform the same inference we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c66529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last insert to the output table to verify\n",
    "\n",
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: run once\n",
    "task = orchestration.run_once(name=\"big query statsmodel run once\", json_args={})\n",
    "task"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0dcbc1d4",
   "metadata": {},
   "source": [
    "### Task Status\n",
    "\n",
    "The list of tasks in the Wallaroo instance is retrieves through the Wallaroo Client `list_tasks()` method.  This returns an array list of the following.\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "| --- | --- | ---|\n",
    "| **id** | string | The UUID identifier for the task. |\n",
    "| **last run status** | string | The last reported status the task.  Values are: <br><ul><li>`pending`: The task has not been started.</li><li>`started`: The task has been scheduled to execute.</li><li>`pending_kill`: The task kill command has been issued and the task is scheduled to be stopped.</li></ul> |\n",
    "| **type** | string | The type of the task.  Values are: <br><ul><li>`Temporary Run`: The task runs once then stop.</li><li>`Scheduled Run`: The task repeats on a `cron` like schedule.</li></ul> |\n",
    "| **created at** | DateTime | The date and time the task was started. |\n",
    "| **updated at** | DateTime | The date and time the task was updated. |\n",
    "\n",
    "For this example, the status of the previously created task will be generated, then looped until it has reached status `started`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05a299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while task.status() != \"started\":\n",
    "    display(task.status())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ea0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl.list_tasks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bd6a79a",
   "metadata": {},
   "source": [
    "### Task Results\n",
    "\n",
    "We can view the inferences from our logs and verify that new entries were added from our task.  We'll query the last 5 rows of our inference output table after a wait of 60 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818b5f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)\n",
    "\n",
    "# Get the last insert to the output table to verify\n",
    "\n",
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24d22c9a",
   "metadata": {},
   "source": [
    "## Scheduled Run Task Example\n",
    "\n",
    "The other method of using tasks is as a **scheduled run** through the Orchestration `run_scheduled(name, schedule, timeout, json_args)`.  This sets up a task to run on an regular schedule as defined by the `schedule` parameter in the `cron` service format.  For example:\n",
    "\n",
    "```python\n",
    "schedule={'42 * * * *'}\n",
    "```\n",
    "\n",
    "Runs on the 42nd minute of every hour.\n",
    "\n",
    "The following schedule runs every day at 12 noon from February 1 to February 15 2024 - and then ends.\n",
    "\n",
    "```python\n",
    "schedule={'0 0 12 1-15 2 2024'}\n",
    "```\n",
    "\n",
    "For our example, we will create a scheduled task to run every 1 minute, display the inference results, then use the Orchestration `kill` task to keep the task from running any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab45fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)\n",
    "\n",
    "scheduled_task = orchestration.run_scheduled(name=\"simple_statsmodel_inference_schedule\", schedule=\"*/1 * * * *\", timeout=120, json_args={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efac60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "while scheduled_task.status() != \"started\":\n",
    "    display(scheduled_task.status())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98668880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wait 120 seconds to give the scheduled event time to finish\n",
    "time.sleep(60)\n",
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e957bfc7",
   "metadata": {},
   "source": [
    "### Kill Task\n",
    "\n",
    "With our testing complete, we will kill the scheduled task so it will not run again.  First we'll show all the tasks to verify that our task is there, then issue it the kill command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9eda3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduled_task.kill()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c634fce",
   "metadata": {},
   "source": [
    "### Running Task with Custom Parameters\n",
    "\n",
    "Right now, our task assumes the workspace, pipeline, and connections all have the names we defined above.  For this example, we'll set up a new pipeline with the same pipeline step, but name it `bigquerystatsmodelpipeline02`.\n",
    "\n",
    "When we create our task, we'll add that pipeline name as an argument to our task.  Within our orchestrations `main.py` there is a code block that takes in the task arguments, then sets the pipeline name:\n",
    "\n",
    "```python\n",
    "arguments = wl.task_args()\n",
    "if \"pipeline_name\" in arguments:\n",
    "        pipeline_name = arguments['pipeline_name']\n",
    "    else:\n",
    "        pipeline_name=\"bigquerystatsmodelpipeline\"\n",
    "```\n",
    "\n",
    "We'll pass along our new pipeline name as `{ \"pipeline_name\": \"bigquerystatsmodelpipeline02\" }` and track the task progress as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "newpipeline_name = 'bigquerystatsmodelpipeline02'\n",
    "\n",
    "pipeline02 = get_pipeline(newpipeline_name)\n",
    "# add the model as the pipeline step\n",
    "pipeline02.add_model_step(bike_day_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231c8689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required to set the pipeline steps\n",
    "pipeline02.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ed694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the last insert to the output table to verify\n",
    "\n",
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)\n",
    "\n",
    "# Generate the run once task with the new parameter\n",
    "task = orchestration.run_once(name=\"parameter sample\", json_args={ \"pipeline_name\": newpipeline_name })\n",
    "display(task)\n",
    "\n",
    "# wait for the task to run\n",
    "while task.status() != \"started\":\n",
    "    display(task.status())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait 60 seconds then display the results\n",
    "time.sleep(30)\n",
    "\n",
    "task_inference_results = bigqueryoutputclient.query(\n",
    "        f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {big_query_output_connection.details()['dataset']}.{big_query_output_connection.details()['table']}\n",
    "        ORDER BY date DESC\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "    ).to_dataframe()\n",
    "\n",
    "display(task_inference_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d50453",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "With that, our tutorial is over.  Please feel free to use this tutorial code in your own Wallaroo related projects.  Our last task will be to undeploy our pipelines to restore the resources back to the Wallaroo instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5e12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.undeploy()\n",
    "pipeline02.undeploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f8ccc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
