{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wallaroo Dashboard Metrics Retrieval Tutorial\n",
    "\n",
    "The following tutorial demonstrates using the Wallaroo MLOps API to retrieve Wallaroo metrics data.  These requests are compliant with Prometheus API endpoints.  \n",
    "\n",
    "This tutorial is split into two sections:\n",
    "\n",
    "* Inference Data Generation:  This section creates Wallaroo pipeline and inference requests to generate the log files and other data.\n",
    "* Wallaroo Dashboard Metrics Retrieval via the Wallaroo MLOps API:  Details the Wallaroo MLOps API metrics retrieval endpoints and provides a demonstration of retrieving metrics data.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This tutorial assumes the following:\n",
    "\n",
    "* A Wallaroo Ops environment is installed.\n",
    "* The Wallaroo SDK is installed.  These examples use the Wallaroo SDK to generate the initial inferences information for the metrics requests.\n",
    "\n",
    "## Wallaroo Dashboard Metrics Retrieval via the Wallaroo MLOps API\n",
    "\n",
    "The Wallaroo MLOps API allows for metrics retrieval.  These are used to track:\n",
    "\n",
    "* Inference result performance.\n",
    "* Deployed replicas.\n",
    "* Inference Latency.\n",
    "\n",
    "These inference endpoints are compliant with Prometheus endpoints.\n",
    "\n",
    "### Query Metric Request Endpoints\n",
    "\n",
    "* **Endpoints**: \n",
    "  * `/v1/api/metrics/query` (**GET**)\n",
    "  * `/v1/api/metrics/query` (**POST**)\n",
    "\n",
    "For full details, see the [Wallaroo MLOps API Reference Guide](https://docs.wallarooai/wallaroo-developer-guides/wallaroo-api-guide/wallaroo-mlops-api-reference-guide/#operations-tag-metrics)\n",
    "\n",
    "#### Query Metric Request Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| query | *String* | The Prometheus expression query string. |\n",
    "| time | *String* | The evaluation timestamp in either RFC3339 format or Unix timestamp. |\n",
    "| timeout | *String* | The evaluation timeout in duration format (`5m` for 5 minutes, etc). |\n",
    "\n",
    "#### Query Metric Request Returns\n",
    "\n",
    "| Field | &nbsp; | Type | Description |\n",
    "|---|---|---|---|\n",
    "| **status** | &nbsp; | *String* | The status of the request of either `success` or `error`. |\n",
    "| **data** | &nbsp; | *Dict* | The response data. |\n",
    "| &nbsp; | **data.resultType** | *String* | The type of query result. |\n",
    "| &nbsp; | **data.result** | *String* | DateTime of the model's creation. |\n",
    "| **errorType** | &nbsp; | *String* | The error type if `status` is `error`. |\n",
    "| **errorType** | &nbsp; | *String* | The error messages if `status` is `error`. |\n",
    "| **warnings** | &nbsp; | *Array[String]* | An array of error messages. |\n",
    "\n",
    "### Query Range Metric Endpoints\n",
    "\n",
    "* **Endpoints**\n",
    "  * `/v1/api/metrics/query_range` (**GET**)\n",
    "  * `/v1/api/metrics/query_range` (**POST**)\n",
    "\n",
    "Returns a list of models added to a specific workspace.\n",
    "\n",
    "#### Query Range Metric Request Parameters\n",
    "\n",
    "| Parameter | Type | Description |\n",
    "|---|---|---|\n",
    "| query | *String* | The Prometheus expression query string. |\n",
    "| start | *String* | The starting timestamp in either RFC3339 format or Unix timestamp, inclusive. |\n",
    "| end | *String* | The ending timestamp in either RFC3339 format or Unix timestamp. |\n",
    "| step | *String* | Query resolution step width in either duration format or as a float number of seconds. |\n",
    "| timeout | *String* | The evaluation timeout in duration format (`5m` for 5 minutes, etc). |\n",
    "\n",
    "#### Query Range Metric Request Returns\n",
    "\n",
    "| Field | &nbsp; | Type | Description |\n",
    "|---|---|---|---|\n",
    "| **status** | &nbsp; | *String* | The status of the request of either `success` or `error`. |\n",
    "| **data** | &nbsp; | *Dict* | The response data. |\n",
    "| &nbsp; | **resultType** | *String* | The type of query result. For query range, always `matrix`. |\n",
    "| &nbsp; | **result** | *String* | DateTime of the model's creation. |\n",
    "| **errorType** | &nbsp; | *String* | The error type if `status` is `error`. |\n",
    "| **errorType** | &nbsp; | *String* | The error messages if `status` is `error`. |\n",
    "| **warnings** | &nbsp; | *Array[String]* | An array of error messages. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><h2>Inference Data Generation</h2></summary>\n",
    "\n",
    "This part of the tutorial generates the inference results used for the rest of the tutorial.\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "import wallaroo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is established via the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace\n",
    "\n",
    "Next create the Wallaroo workspace and set it as the default workspace for this session - from this point on, model uploads and other commands will default to this workspace.\n",
    "\n",
    "The workspace id is stored for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'metric-retrieval-tutorial', 'id': 1713, 'archived': False, 'created_by': '7d603858-88e0-472e-8f71-e41094afd7ec', 'created_at': '2025-08-05T18:41:42.646046+00:00', 'models': [], 'pipelines': []}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = wl.get_workspace(name=\"metric-retrieval-tutorial\", create_if_not_exist=True)\n",
    "wl.set_current_workspace(workspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Model\n",
    "\n",
    "For this example, the model `ccfraud.onnx` is used.  This is a credit card fraud model is trained to detect credit card fraud based on a 0 to 1 model:  The closer to 0 the less likely the transactions indicate fraud, while the closer to 1 the more likely the transactions indicate fraud.\n",
    "\n",
    "This model is included in the Wallaroo Native Runtimes, so requires no additional settings at model upload.  For more details on supported models, see [Wallaroo Supported Models](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-deploy/wallaroo-model-operations-upload-register/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ccfraud-model\"\n",
    "model_file_name = \"./models/ccfraud.onnx\"\n",
    "ccfraud_model = (wl.upload_model(name=model_name, \n",
    "                                 path=model_file_name, \n",
    "                                 framework=wallaroo.framework.Framework.ONNX)\n",
    "                                 .configure(tensor_fields=[\"tensor\"])\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model\n",
    "\n",
    "Models are deployed through the following process:\n",
    "\n",
    "* Create a Wallaroo pipeline\n",
    "* Add the model as a pipeline step\n",
    "* Define a deployment configuration.  This defines what resources are allocated for the pipeline's exclusive use.\n",
    "\n",
    "For more details of this process, see [ML Operations: Inference](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-serve/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pipeline\n",
    "pipeline_name = \"metrics-retrieval-tutorial-pipeline\"\n",
    "pipeline = wl.build_pipeline(pipeline_name)\n",
    "\n",
    "# add the model as a pipeline step\n",
    "\n",
    "pipeline.add_model_step(ccfraud_model)\n",
    "\n",
    "# set the deployment configuration for 0.5 cpu, 1 replica, 1 Gi RAM\n",
    "deploy_config = wallaroo.DeploymentConfigBuilder().replica_count(1).cpus(0.5).memory(\"1Gi\").build()\n",
    "\n",
    "# deploy the pipeline\n",
    "pipeline.deploy(deployment_config=deploy_config, wait_for_status=False)\n",
    "# saved for later steps\n",
    "deploy = pipeline._deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for deployment.\n",
      "Waiting for deployment.\n",
      "Waiting for deployment.\n",
      "Waiting for deployment.\n",
      "Waiting for deployment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Running'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wait until deployment is complete before continuing\n",
    "import time\n",
    "time.sleep(15)\n",
    "\n",
    "while pipeline.status()['status'] != 'Running':\n",
    "    time.sleep(15)\n",
    "    print(\"Waiting for deployment.\")\n",
    "    pipeline.status()['status']\n",
    "pipeline.status()['status']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Inferences\n",
    "\n",
    "The following sample inferences are used to generate inference logs records.  Metric retrieval works best with a longer history of inference results;  feel free to rerun this section as needed to create additional records for further testing.\n",
    "\n",
    "The following will run for one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "timeout = time.time() + 60   # 1 minutes from now\n",
    "while True:\n",
    "    if time.time() > timeout:\n",
    "        break\n",
    "    pipeline.infer_from_file(\"./data/cc_data_10k.arrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Pipeline Logs\n",
    "\n",
    "The following retrieves the inference log results for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: There are more logs available. Please set a larger limit or request a file using export_logs."
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>in.tensor</th>\n",
       "      <th>out.dense_1</th>\n",
       "      <th>anomaly.count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[-0.12405868, 0.73698884, 1.0311689, 0.5991753...</td>\n",
       "      <td>[0.0010648072]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[-2.1694233, -3.1647356, 1.2038506, -0.2649221...</td>\n",
       "      <td>[0.00024175644]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[-0.24798988, 0.40499672, 0.49408177, -0.37252...</td>\n",
       "      <td>[0.00150159]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[-0.2260837, 0.12802614, -0.8732004, -2.089788...</td>\n",
       "      <td>[0.00037947297]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[-0.90164274, -0.50116056, 1.2045985, 0.407885...</td>\n",
       "      <td>[0.0001988411]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[-0.1093998, -0.031678658, 0.9885652, -0.68602...</td>\n",
       "      <td>[0.00020942092]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[0.44973943, -0.35288164, 0.5224735, 0.910402,...</td>\n",
       "      <td>[0.00031492114]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[0.82174337, -0.50793207, -1.358988, 0.3713617...</td>\n",
       "      <td>[0.00081187487]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[1.0252348, 0.37717652, -1.4182774, 0.7057443,...</td>\n",
       "      <td>[0.001860708]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>2025-08-05 18:44:22.769</td>\n",
       "      <td>[-0.36498702, 0.11005125, 0.7734325, 1.0163404...</td>\n",
       "      <td>[0.00064843893]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      time                                          in.tensor  \\\n",
       "0  2025-08-05 18:44:22.769  [-0.12405868, 0.73698884, 1.0311689, 0.5991753...   \n",
       "1  2025-08-05 18:44:22.769  [-2.1694233, -3.1647356, 1.2038506, -0.2649221...   \n",
       "2  2025-08-05 18:44:22.769  [-0.24798988, 0.40499672, 0.49408177, -0.37252...   \n",
       "3  2025-08-05 18:44:22.769  [-0.2260837, 0.12802614, -0.8732004, -2.089788...   \n",
       "4  2025-08-05 18:44:22.769  [-0.90164274, -0.50116056, 1.2045985, 0.407885...   \n",
       "..                     ...                                                ...   \n",
       "95 2025-08-05 18:44:22.769  [-0.1093998, -0.031678658, 0.9885652, -0.68602...   \n",
       "96 2025-08-05 18:44:22.769  [0.44973943, -0.35288164, 0.5224735, 0.910402,...   \n",
       "97 2025-08-05 18:44:22.769  [0.82174337, -0.50793207, -1.358988, 0.3713617...   \n",
       "98 2025-08-05 18:44:22.769  [1.0252348, 0.37717652, -1.4182774, 0.7057443,...   \n",
       "99 2025-08-05 18:44:22.769  [-0.36498702, 0.11005125, 0.7734325, 1.0163404...   \n",
       "\n",
       "        out.dense_1  anomaly.count  \n",
       "0    [0.0010648072]              0  \n",
       "1   [0.00024175644]              0  \n",
       "2      [0.00150159]              0  \n",
       "3   [0.00037947297]              0  \n",
       "4    [0.0001988411]              0  \n",
       "..              ...            ...  \n",
       "95  [0.00020942092]              0  \n",
       "96  [0.00031492114]              0  \n",
       "97  [0.00081187487]              0  \n",
       "98    [0.001860708]              0  \n",
       "99  [0.00064843893]              0  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><h2>Wallaroo Dashboard Metrics Retrieval via the Wallaroo MLOps API</h2></summary>\n",
    "\n",
    "The following queries are supported through the Metrics endpoints.  The following references are used here:\n",
    "\n",
    "* `pipelineID`:  The pipeline's string identifier, retrieved from the Wallaroo SDK with `wallaroo.pipeline.Pipeline.name()`.  For example:\n",
    "\n",
    "    ```python\n",
    "    pipeline.name()\n",
    "    ```\n",
    "\n",
    "    ```text\n",
    "    sample-pipeline-name\n",
    "    ```\n",
    "\n",
    "* `deployment_id`: The Kubernetes namespace for the deployment.\n",
    "\n",
    "### Supported Queries\n",
    "\n",
    "Note that each of these queries use the `/v1/metrics/api/v1/query_range` endpoint.  Note as per the section \n",
    "\n",
    "| English Name | Description | Parameterized Query | Example Query | \n",
    "|---|---|---|---|\n",
    "| Requests per second | Number of processed requests per second to a pipeline. |`sum by (pipeline_name) (rate(latency_histogram_ns_count{pipeline_name=\"{pipelineID}\"}[{step}s]))` | `sum by (deploy_id) (rate(latency_histogram_ns_count{deploy_id=\"deployment_id\"}[10s]))` | \n",
    "| Cluster inference rate | Number of inferences processed per second.  This notably differs from requests per second when batch inference requests are made. |`sum by (pipeline_name) (rate(tensor_throughput_batch_count{pipeline_name=\"{pipelineID}\"}[{step}s]))` | `sum by (deploy_id) (rate(tensor_throughput_batch_count{deploy_id=\"deployment_id\"}[10s]))` | \n",
    "| P50 inference latency | Histogram for P90 total inference time spent per message in an engine, includes transport to and from the sidekick in the case there is one. |`histogram_quantile(0.50, sum(rate(latency_histogram_ns_bucket{{deploy_id=\"{deploy_id}\"}}[{step_interval}])) by (le)) / 1e6` | `histogram_quantile(0.50, sum(rate(latency_histogram_ns_bucket{deploy_id=\"deployment_id\"}[10s])) by (le)) / 1e6` | \n",
    "| P95 inference latency | Histogram for P95 total inference time spent per message in an engine, includes transport to and from the sidekick in the case there is one. |`histogram_quantile(0.95, sum(rate(latency_histogram_ns_bucket{{deploy_id=\"{deploy_id}\"}}[{step_interval}])) by (le)) / 1e6` | `histogram_quantile(0.95, sum(rate(latency_histogram_ns_bucket{deploy_id=\"deployment_id\"}[10s])) by (le)) / 1e6` | \n",
    "| P99 inference latency | Histogram for P99 total inference time spent per message in an engine, includes transport to and from the sidekick in the case there is one. |`histogram_quantile(0.99, sum(rate(latency_histogram_ns_bucket{{deploy_id=\"{deploy_id}\"}}[{step_interval}])) by (le)) / 1e6` | `histogram_quantile(0.99, sum(rate(latency_histogram_ns_bucket{deploy_id=\"deployment_id\"}[10s])) by (le)) / 1e6` | \n",
    "| Engine replica count | Number of engine replicas currently running in a pipeline | `count(container_memory_usage_bytes{namespace=\"{pipeline_namespace}\", container=\"engine\"}) or vector(0)` | `count(container_memory_usage_bytes{namespace=\"deployment_id\", container=\"engine\"}) or vector(0)` | \n",
    "| Sidekick replica count | Number of sidekick replicas currently running in a pipeline |`count(container_memory_usage_bytes{namespace=\"{pipeline_namespace}\", container=~\"engine-sidekick-.*\"}) or vector(0)` | `count(container_memory_usage_bytes{namespace=\"deployment_id\", container=~\"engine-sidekick-.*\"}) or vector(0)` | \n",
    "| Output tokens per second (TPS) | LLM output tokens per second: this is the number of tokens generated per second for a LLM deployed in Wallaroo with vLLM | `sum by (kubernetes_namespace) (rate(vllm:generation_tokens_total{kubernetes_namespace=\"{pipeline_namespace}\"}[{step_interval}]))` | `sum by (kubernetes_namespace) (rate(vllm:generation_tokens_total{kubernetes_namespace=\"deployment_id\"}[10s]))` | \n",
    "| P99 Time to first token (TTFT) | P99 time to first token: P99 for time to generate the first token for LLMs deployed in Wallaroo with vLLM | `histogram_quantile(0.99, sum(rate(vllm:time_to_first_token_seconds_bucket{kubernetes_namespace=\"{pipeline_namespace}\"}[{step_interval}])) by (le)) * 1000` | `histogram_quantile(0.99, sum(rate(vllm:time_to_first_token_seconds_bucket{kubernetes_namespace=\"deployment_id\"}[10s])) by (le)) * 1000` |\n",
    "| P95 Time to first token (TTFT) | P95 time to first token: P95 for time to generate the first token for LLMs deployed in Wallaroo with vLLM | `histogram_quantile(0.95, sum(rate(vllm:time_to_first_token_seconds_bucket{kubernetes_namespace=\"{pipeline_namespace}\"}[{step_interval}])) by (le)) * 1000` | `histogram_quantile(0.95, sum(rate(vllm:time_to_first_token_seconds_bucket{kubernetes_namespace=\"deployment_id\"}[10s])) by (le)) * 1000` |\n",
    "| P50 Time to first token (TTFT) | P50 time to first token: P50 for time to generate the first token for LLMs deployed in Wallaroo with vLLM | `histogram_quantile(0.50, sum(rate(vllm:time_to_first_token_seconds_bucket{kubernetes_namespace=\"{pipeline_namespace}\"}[{step_interval}])) by (le)) * 1000` | `histogram_quantile(0.50, sum(rate(vllm:time_to_first_token_seconds_bucket{kubernetes_namespace=\"deployment_id\"}[10s])) by (le)) * 1000` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TTFT Query Example\n",
    "\n",
    "The following example uses the P99 Time to first token (TTFT) query for an LLM [deployed with OpenAI Compatibility in Wallaroo](https://docs.wallarooai/wallaroo-llm/wallaroo-llm-package-deployment/wallaroo-llm-optimizations-openai-compatibility/)\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "# this will also format the timezone in the parsing section\n",
    "timezone = \"US/Central\"\n",
    "\n",
    "selected_timezone = pytz.timezone(timezone)\n",
    "\n",
    "# Define the start and end times of 10:00 to 10:15\n",
    "data_start = selected_timezone.localize(datetime.datetime(2025, 7, 14, 10, 0, 0))\n",
    "data_end = selected_timezone.localize(datetime.datetime(2025, 7, 14, 10, 15, 00))\n",
    "\n",
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query_range\"\n",
    "\n",
    "import time\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Convert to UTC and get the Unix timestamps\n",
    "start_timestamp = int(data_start.astimezone(pytz.UTC).timestamp())\n",
    "end_timestamp = int(data_end.astimezone(pytz.UTC).timestamp())    \n",
    "\n",
    "pipeline_name = \"llama-3-1-8b-pipeline\" # the name of the pipeline\n",
    "deploy_id = 210 # the deployment id\n",
    "step = \"5m\" # the step of the calculation\n",
    "\n",
    "\n",
    "query_ttft = f'histogram_quantile(0.99, sum(rate(vllm:time_to_first_token_seconds_bucket{{kubernetes_namespace=\"{pipeline_name}-{deploy_id}\"}}[{step}])) by (le)) * 1000'\n",
    "print(query_ttft)\n",
    "\n",
    "#request parameters\n",
    "params_ttft = {\n",
    "    'query': query_ttft,\n",
    "    'start': start_timestamp,\n",
    "    'end': end_timestamp,\n",
    "    'step': step\n",
    "}\n",
    "\n",
    "response_rps = requests.get(query_url, headers=headers, params=params_ttft)\n",
    "\n",
    "if response_rps.status_code == 200:\n",
    "    #print(\"Requests Per Second Data:\")\n",
    "    result = response_rps.json()\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"Failed to fetch TTFT data:\", response_rps.status_code, response_rps.text)\n",
    "```\n",
    "\n",
    "```text\n",
    "histogram_quantile(0.99, sum(rate(vllm:time_to_first_token_seconds_bucket{kubernetes_namespace=\"llama-3-1-8b-pipeline-210\"}[5m])) by (le)) * 1000\n",
    "{'status': 'success', 'data': {'resultType': 'matrix', 'result': [{'metric': {}, 'values': [[1752505500, '48.45656000000012'], [1752505800, '39.800000000000004'], [1752506100, 'NaN']]}]}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set prometheus requirements\n",
    "pipeline_id = pipeline_name # the name of the pipeline\n",
    "step = \"1m\" # the step of the calculation\n",
    "\n",
    "# this will also format the timezone in the parsing section\n",
    "timezone = \"US/Central\"\n",
    "\n",
    "selected_timezone = pytz.timezone(timezone)\n",
    "\n",
    "# Define the start and end times\n",
    "data_start = selected_timezone.localize(datetime.datetime(2025, 8, 4, 9, 0, 0))\n",
    "data_end = selected_timezone.localize(datetime.datetime(2025, 8, 6, 9, 59, 59))\n",
    "\n",
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query_range\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requests Per Second Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success', 'data': {'resultType': 'matrix', 'result': []}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Convert to UTC and get the Unix timestamps\n",
    "start_timestamp = int(data_start.astimezone(pytz.UTC).timestamp())\n",
    "end_timestamp = int(data_end.astimezone(pytz.UTC).timestamp())    \n",
    "\n",
    "query_rps = f'sum by (pipeline_name) (rate(latency_histogram_ns_count{{pipeline_name=\"{pipeline_id}\"}}[{step}]))'\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query_rps,\n",
    "    'start': start_timestamp,\n",
    "    'end': end_timestamp,\n",
    "    'step': step\n",
    "}\n",
    "\n",
    "response_rps = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response_rps.status_code == 200:\n",
    "    print(\"Requests Per Second Data:\")\n",
    "    display(response_rps.json())\n",
    "else:\n",
    "    print(\"Failed to fetch RPS data:\", response_rps.status_code, response_rps.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the query inference rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Inference Rate Data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'matrix',\n",
       "  'result': [{'metric': {'pipeline_name': 'metrics-retrieval-tutorial-pipeline'},\n",
       "    'values': [[1754419440, '6274.9353'],\n",
       "     [1754419500, '4474.472727272727'],\n",
       "     ...]}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_inference_rate = f'sum by (pipeline_name) (rate(tensor_throughput_batch_count{{pipeline_name=\"{pipeline_id}\"}}[{step}]))'\n",
    "\n",
    "# inference rte\n",
    "params_inference_rate = {\n",
    "    'query': query_inference_rate,\n",
    "    'start': start_timestamp,\n",
    "    'end': end_timestamp,\n",
    "    'step': step\n",
    "}\n",
    "\n",
    "response_inference_rate = requests.get(query_url, headers=headers, params=params_inference_rate)\n",
    "\n",
    "if response_inference_rate.status_code == 200:\n",
    "    print(\"Cluster Inference Rate Data:\")\n",
    "    display(response_inference_rate.json())\n",
    "else:\n",
    "    print(\"Failed to fetch Inference Rate data:\", response_inference_rate.status_code, response_inference_rate.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wallaroo Dashboard Metrics Retrieval Tutorials\n",
    "\n",
    "The following tutorials demonstrate creating metrics data and retrieving it using the Wallaroo MLOps API.\n",
    "\n",
    "* [Wallaroo Model Observability: Dashboard Metrics for Classification Models](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-observe/wallaroo-model-operations-observability-tutorials/dashboard-metrics-classification/)\n",
    "* [Wallaroo Model Observability: Dashboard Metrics for Summarization Models](https://docs.wallaroo.ai/wallaroo-model-operations/wallaroo-model-operations-observe/wallaroo-model-operations-observability-tutorials/dashboard-metrics-summarization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><h2>Wallaroo Admin Dashboard Metrics Retrieval via the Wallaroo MLOps API</h2></summary>\n",
    "\n",
    "The following queries are available for resource consumption and available through the Admin Dashboard.  Note where each request either uses the `query` endpoint or the `query_range` endpoint.  For examples of these queries, see the [Wallaroo Repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.2_tutorials/development/mlops-api).\n",
    "\n",
    "### Supported Queries\n",
    "\n",
    "Note that each of these queries use the `/v1/metrics/api/v1/query` endpoint.\n",
    "\n",
    "| Query Name | Description | Example Query | \n",
    "|---|---|---|\n",
    "| Total CPU Requested |   Number of CPUs requested in the Wallaroo cluster |`sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})` |\n",
    "| Total CPU allocated | Total number of available CPUs in the Wallaroo cluster |`sum(kube_node_status_capacity{resource=\"cpu\"})` | \n",
    "| Total GPU Requested | Number of GPUs requested in the Wallaroo cluster |`sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})` | \n",
    "| Total GPU Allocated | Total number of available GPUs in the Wallaroo cluster |`sum(kube_node_status_capacity{resource=~\"nvidia_com_gpu\\|qualcomm_com_qaic\"})` | \n",
    "| Total Memory Requested | Amount of memory requested in the Wallaroo cluster. | `sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})` | \n",
    "| Total Memory Allocated | Total amount of memory available in the Wallaroo cluster. | `sum(kube_node_status_capacity{resource=\"memory\"})` |\n",
    "| Total Inference Log Storage used | Amount of inference log storage used. | `kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}` |\n",
    "| Total Inference Log Storage allocated | Total amount of inference log storage available. | `kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}` |\n",
    "| Total Artifact Storage used | Amount of model and orchestration artifact storage used. | `kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"minio\"}` |\n",
    "| Total Artifact Storage allocated | Total amount of model and orchestration artifact storage available. | `kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"minio\"}` |\n",
    "| Average GPU usage over time | Average GPU usage over the defined time range in the Wallaroo cluster. | `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[{duration}] {offset})` |\n",
    "| Average GPU requested over time | Average number of GPU requested over the defined time range in the Wallaroo cluster |  `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[{duration}] {offset})` |\n",
    "| Average CPU usage over time | Average CPU usage over the defined time range in the Wallaroo cluster. | `avg_over_time(sum(wallaroo_kube_pod_resource_usage{resource=”cpu”})[{duration}] {offset})` |\n",
    "| Average CPU requested over time | Average CPU requests over the defined time range in the Wallaroo cluster | `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[{duration}] {offset})` |\n",
    "| Average Memory usage over time | Average memory usage over the defined time range in the Wallaroo cluster. | `avg_over_time(sum(wallaroo_kube_pod_resource_usage{resource=\"memory\"})[{duration}] {offset})` |\n",
    "| Average Memory requests over time | Average memory requests over the defined time range in the Wallaroo cluster. | `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[{duration}] {offset})` |\n",
    "| Average pipelines CPU usage over time | Average CPU usage over the defined time range for an individual Wallaroo pipeline. | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_usage{resource=\"cpu\"})[{duration}] {offset})` |\n",
    "| Average pipelines CPU requested over time | Average number of CPUs requested over the defined time range for an individual Wallaroo pipeline. | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[{duration}] {offset})` |\n",
    "| Average pipelines GPU usage over time | Average GPU usage over the defined time range for an individual Wallaroo pipeline. | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[{duration}] {offset})` |\n",
    "| Average pipelines GPU requested over time | Average number of GPUs requested over the defined time range for an individual Wallaroo pipeline. | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[{duration}] {offset})` |\n",
    "| Average pipelines Mem usage over time | Average memory usage over the defined time range for an individual Wallaroo pipeline. | `avg_over_time(sum by(namespace) (wallaroo_kube_pod_resource_usage{resource=\"memory\"})[{duration}] {offset})` |\n",
    "| Average pipelines Mem requested over time | Average amount of memory requested over the defined time range for an individual Wallaroo pipeline. | `avg_over_time(sum by (namespace)(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[{duration}] {offset})` |\n",
    "| Pipeline inference log storage | Inference log storage used at the end of the defined time range for an individual Wallaroo pipeline |  `sum by(topic) (topic_bytes@{timestamp})` |\n",
    "\n",
    "#### Metrics for a Specified Time Range in the Past Format\n",
    "\n",
    "For queries that retrieve metric data between a range of dates in the past, the following example demonstrates how to use start date, end date, and the offset period.\n",
    "\n",
    "This example uses three variables parameterized and inserted using the Python variable string replacement method:\n",
    "\n",
    "* `date_start`: The date starting the metric analysis period.\n",
    "* `date_end`: The end date of the metric analysis period.\n",
    "* `current_time`: The current time.\n",
    "\n",
    "These values are then converted into the following:\n",
    "\n",
    "* `duration`: The amount of time in seconds between `date_start` and `date_end`.\n",
    "* `offset`: The amount of time in seconds between `date_end` and `current_time`.\n",
    "\n",
    "For example, if the period to measure is between 12/1/2025 12 AM to 12/3/2025 12 AM, and the **current time** is December 15, 2025:\n",
    "\n",
    "* **Duration** is the period from December 1 12:00 AM to December 3 12:00 Am (3 days aka 72 hours aka 259,200 seconds)\n",
    "* **Offset** is the period from December 15, 2025 to December 3, 2025 (12 days).\n",
    "\n",
    "The following example show retrieving the average CPU usage over a period of time for the dates 12/1/2025 to 12/3/2025.\n",
    "\n",
    "```bash\n",
    "# this is the URL to get this metric\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "data_start = selected_timezone.localize(datetime.datetime(2025, 12, 1, 13, 0, 0))\n",
    "data_end = selected_timezone.localize(datetime.datetime(2025, 12, 3, 15, 59, 59))\n",
    "current_time = datetime.datetime.now(selected_timezone)\n",
    "\n",
    "duration = int((data_end-data_start).total_seconds())\n",
    "offset = int((current_time-data_end).total_seconds())\n",
    "\n",
    "\n",
    "query_avg_cpu_usage = f'avg_over_time(sum(wallaroo_kube_pod_resource_usage{{resource=\"cpu\"}})[{duration}s:] offset {offset}s)'\n",
    "\n",
    "#request parameters\n",
    "params_avg_cpu_usage = {\n",
    "    'query': query_avg_cpu_usage\n",
    "}\n",
    "\n",
    "response_avg_cpu_usage = requests.get(query_url, headers=headers, params=params_avg_cpu_usage)\n",
    "\n",
    "\n",
    "if response_avg_cpu_usage.status_code == 200:\n",
    "    print(\"Average CPU usage over time:\")\n",
    "    display(response_avg_cpu_usage.json())\n",
    "else:\n",
    "    print(\"Failed to fetch Avg CPU usage data:\", response_avg_cpu_usage.status_code, response_avg_cpu_usage.text)\n",
    "```\n",
    "\n",
    "### Total CPU Requested Example\n",
    "\n",
    "The following example demonstrates using the Metric Query with the following attributes:\n",
    "\n",
    "* Total CPU Requested\n",
    "* query \n",
    "* `sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})`\n",
    "* Number of CPUs requested in the Wallaroo cluster\n",
    "\n",
    "```python\n",
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "query = 'sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)\n",
    "```\n",
    "\n",
    "Results:\n",
    "\n",
    "```python\n",
    "Query Response:\n",
    "\n",
    "{'status': 'success',\n",
    " 'data': {'resultType': 'vector',\n",
    "  'result': [{'metric': {}, 'value': [1764100020.302, '9.306000000000001']}]}}\n",
    "```\n",
    "\n",
    "### Tutorials\n",
    "\n",
    "* [Wallaroo Admin Dashboard Metrics Retrieval Tutorial](https://github.com/WallarooLabs/Wallaroo_Tutorials/blob/wallaroo2025.2_tutorials/development/mlops-api/Wallaroo-MLOps-Tutorial-Metrics-Retrieval-Examples.ipynb)\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2025.1.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
