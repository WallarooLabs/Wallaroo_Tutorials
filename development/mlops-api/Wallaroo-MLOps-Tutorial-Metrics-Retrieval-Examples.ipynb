{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wallaroo Admin Dashboard Metrics Retrieval Tutorial\n",
    "\n",
    "The following tutorial demonstrates using the Wallaroo MLOps API to retrieve Wallaroo metrics data.  These requests are compliant with Prometheus API endpoints.  \n",
    "\n",
    "This tutorial lists the metrics queries available and demonstrates how to perform each of the queries.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This tutorial assumes the following:\n",
    "\n",
    "* A Wallaroo Ops environment is installed.\n",
    "* The Wallaroo SDK is installed.  These examples use the Wallaroo SDK to generate the initial inferences information for the metrics requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Data Generation\n",
    "\n",
    "This part of the tutorial generates the inference results used for the rest of the tutorial.\n",
    "\n",
    "### Import libraries\n",
    "\n",
    "The first step is to import the libraries required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pytz\n",
    "import datetime\n",
    "\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "import wallaroo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the Wallaroo Instance\n",
    "\n",
    "A connection to Wallaroo is established via the Wallaroo client.  The Python library is included in the Wallaroo install and available through the Jupyter Hub interface provided with your Wallaroo environment.\n",
    "\n",
    "This is accomplished using the `wallaroo.Client()` command, which provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Store the connection into a variable that can be referenced later.\n",
    "\n",
    "If logging into the Wallaroo instance through the internal JupyterHub service, use `wl = wallaroo.Client()`.  For more information on Wallaroo Client settings, see the [Client Connection guide](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/wallaroo-sdk-essentials-client/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = wallaroo.Client(api_endpoint=\"https://autoscale-uat-gcp.wallaroo.dev/\", \n",
    "                     auth_type=\"sso\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ccfraud-model\"\n",
    "model_file_name = \"./models/ccfraud.onnx\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following queries are available for resource consumption.  Note where each request the `/v1/metrics/api/v1/query` endpoint.\n",
    "\n",
    "| Query Name | API Route | Example Query | Description | \n",
    "|---|---|---|---|\n",
    "| Total CPU Requested | query | `sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})` | Number of CPUs requested in the Wallaroo cluster |\n",
    "| Total CPU allocated | query | `sum(kube_node_status_capacity{resource=\"cpu\"})` | Total number of available CPUs in the Wallaroo cluster |\n",
    "| Total GPU Requested | query | `sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})` | Number of GPUs requested in the Wallaroo cluster |\n",
    "| Total GPU Allocated | query | `sum(kube_node_status_capacity{resource=~\"nvidia_com_gpu\\|qualcomm_com_qaic\"})` | Total number of available GPUs in the Wallaroo cluster |\n",
    "| Total Memory Requested | query | `sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})` | Amount of memory requested in the Wallaroo cluster. |\n",
    "| Total Memory Allocated | query | `sum(kube_node_status_capacity{resource=\"memory\"})` | Total amount of memory available in the Wallaroo cluster. |\n",
    "| Total Inference Log Storage used | query | `kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}` | Amount of inference log storage used. |\n",
    "| Total Inference Log Storage allocated | query | `kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}` | Total amount of inference log storage available. |\n",
    "| Total Artifact Storage used | query | `kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"minio\"}` | Amount of model and orchestration artifact storage used. |\n",
    "| Total Artifact Storage allocated | query | `kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"minio\"}` | Total amount of model and orchestration artifact storage available. |\n",
    "| Average GPU usage over time | query | `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[1h:] offset 1h)` | Average GPU usage over the defined time range in the Wallaroo cluster. |\n",
    "| Average GPU requested over time | query | `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[1h:] offset 1h)` | Average number of GPU requested over the defined time range in the Wallaroo cluster | \n",
    "| Average CPU usage over time | query | `avg_over_time(sum(wallaroo_kube_pod_resource_usage{resource=\"cpu\"})[1h:] offset 1h)` | Average CPU usage over the defined time range in the Wallaroo cluster. |\n",
    "|  Average CPU requested over time | query | `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[1h:] offset 1h)` | Average CPU requests over the defined time range in the Wallaroo cluster |\n",
    "| Average Memory usage over time | query | `avg_over_time(sum(wallaroo_kube_pod_resource_usage{resource=\"memory\"})[1h:] offset 1h)` | Average memory usage over the defined time range in the Wallaroo cluster. |\n",
    "| Average Memory requests over time | query | `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[1h:] offset 1h)` | Average memory requests over the defined time range in the Wallaroo cluster. |\n",
    "| Average pipelines CPU usage over time | query | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_usage{resource=\"cpu\"})[1h:] offset 1h)` | Average CPU usage over the defined time range for an individual Wallaroo pipeline. |\n",
    "| Average pipelines CPU requested over time | query | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[1h:] offset 1h)` | Average number of CPUs requested over the defined time range for an individual Wallaroo pipeline. |\n",
    "| Average pipelines GPU usage over time | query | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[1h:] offset 1h)` | Average GPU usage over the defined time range for an individual Wallaroo pipeline. |\n",
    "| Average pipelines GPU requested over time | query | `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[1h:] offset 1h)` | Average number of GPUs requested over the defined time range for an individual Wallaroo pipeline. |\n",
    "| Average pipelines Mem usage over time | query | `avg_over_time(sum by(namespace) (wallaroo_kube_pod_resource_usage{resource=\"memory\"})[1h:] offset 1h)` | Average memory usage over the defined time range for an individual Wallaroo pipeline. |\n",
    "| Average pipelines Mem requested over time | query | `avg_over_time(sum by (namespace)(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[1h:] offset 1h)` | Average amount of memory requested over the defined time range for an individual Wallaroo pipeline. |\n",
    "| Pipeline inference log storage | query | `avg_over_time(sum by(topic) (topic_bytes)[1h:] offset 1h)` | Average inference log storage used over the defined time range for an individual Wallaroo pipeline |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total CPU Requested\n",
    "\n",
    "* Total CPU Requested\n",
    "* query \n",
    "* `sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})`\n",
    "* Number of CPUs requested in the Wallaroo cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866061.844, '14.406']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total CPU allocated\n",
    "\n",
    "* Total CPU allocated\n",
    "* query\n",
    "* `sum(kube_node_status_capacity{resource=\"cpu\"})`\n",
    "* Total number of available CPUs in the Wallaroo cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866062.086, '48']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'sum(kube_node_status_capacity{resource=\"cpu\"})'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total GPU Requested\n",
    "\n",
    "* Total GPU Requested\n",
    "* query\n",
    "* `sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})`\n",
    "* Number of GPUs requested in the Wallaroo cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866062.314, '2']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total GPU Allocated\n",
    "\n",
    "* Total GPU Allocated\n",
    "* query\n",
    "* `sum(kube_node_status_capacity{resource=~\"nvidia_com_gpu|qualcomm_com_qaic\"})` \n",
    "* Total number of available GPUs in the Wallaroo cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866062.545, '5']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'sum(kube_node_status_capacity{resource=~\"nvidia_com_gpu|qualcomm_com_qaic\"})'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Memory Requested\n",
    "\n",
    "* Total Memory Requested\n",
    "* query\n",
    "* `sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})`\n",
    "* Amount of memory requested in the Wallaroo cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866062.778, '32220643328']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Memory Allocated\n",
    "\n",
    "* Total Memory Allocated\n",
    "* query\n",
    "* `sum(kube_node_status_capacity{resource=\"memory\"})`\n",
    "* Total amount of memory available in the Wallaroo cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866062.982, '197850009600']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'sum(kube_node_status_capacity{resource=\"memory\"})'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Inference Log Storage used\n",
    "\n",
    "* Total Inference Log Storage used\n",
    "* query\n",
    "* `kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}`\n",
    "* Amount of inference log storage used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'__name__': 'kubelet_volume_stats_used_bytes',\n",
       "     'beta_kubernetes_io_arch': 'amd64',\n",
       "     'beta_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'beta_kubernetes_io_os': 'linux',\n",
       "     'cloud_google_com_gke_boot_disk': 'pd-balanced',\n",
       "     'cloud_google_com_gke_container_runtime': 'containerd',\n",
       "     'cloud_google_com_gke_cpu_scaling_level': '8',\n",
       "     'cloud_google_com_gke_logging_variant': 'DEFAULT',\n",
       "     'cloud_google_com_gke_max_pods_per_node': '110',\n",
       "     'cloud_google_com_gke_memory_gb_scaling_level': '32',\n",
       "     'cloud_google_com_gke_nodepool': 'persistent',\n",
       "     'cloud_google_com_gke_os_distribution': 'cos',\n",
       "     'cloud_google_com_gke_provisioning': 'standard',\n",
       "     'cloud_google_com_gke_stack_type': 'IPV4',\n",
       "     'cloud_google_com_machine_family': 'e2',\n",
       "     'cloud_google_com_private_node': 'false',\n",
       "     'failure_domain_beta_kubernetes_io_region': 'us-central1',\n",
       "     'failure_domain_beta_kubernetes_io_zone': 'us-central1-c',\n",
       "     'instance': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'job': 'kubernetes-nodes',\n",
       "     'kubernetes_io_arch': 'amd64',\n",
       "     'kubernetes_io_hostname': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'kubernetes_io_os': 'linux',\n",
       "     'namespace': 'wallaroo',\n",
       "     'node_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'persistentvolumeclaim': 'plateau-managed-disk',\n",
       "     'topology_gke_io_zone': 'us-central1-c',\n",
       "     'topology_kubernetes_io_region': 'us-central1',\n",
       "     'topology_kubernetes_io_zone': 'us-central1-c',\n",
       "     'wallaroo_ai_node_purpose': 'persistent'},\n",
       "    'value': [1764866063.272, '23963860992']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Inference Log Storage allocated\n",
    "\n",
    "* Total Inference Log Storage allocated\n",
    "* query\n",
    "* `kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}`\n",
    "* Total amount of inference log storage available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'__name__': 'kubelet_volume_stats_capacity_bytes',\n",
       "     'beta_kubernetes_io_arch': 'amd64',\n",
       "     'beta_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'beta_kubernetes_io_os': 'linux',\n",
       "     'cloud_google_com_gke_boot_disk': 'pd-balanced',\n",
       "     'cloud_google_com_gke_container_runtime': 'containerd',\n",
       "     'cloud_google_com_gke_cpu_scaling_level': '8',\n",
       "     'cloud_google_com_gke_logging_variant': 'DEFAULT',\n",
       "     'cloud_google_com_gke_max_pods_per_node': '110',\n",
       "     'cloud_google_com_gke_memory_gb_scaling_level': '32',\n",
       "     'cloud_google_com_gke_nodepool': 'persistent',\n",
       "     'cloud_google_com_gke_os_distribution': 'cos',\n",
       "     'cloud_google_com_gke_provisioning': 'standard',\n",
       "     'cloud_google_com_gke_stack_type': 'IPV4',\n",
       "     'cloud_google_com_machine_family': 'e2',\n",
       "     'cloud_google_com_private_node': 'false',\n",
       "     'failure_domain_beta_kubernetes_io_region': 'us-central1',\n",
       "     'failure_domain_beta_kubernetes_io_zone': 'us-central1-c',\n",
       "     'instance': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'job': 'kubernetes-nodes',\n",
       "     'kubernetes_io_arch': 'amd64',\n",
       "     'kubernetes_io_hostname': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'kubernetes_io_os': 'linux',\n",
       "     'namespace': 'wallaroo',\n",
       "     'node_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'persistentvolumeclaim': 'plateau-managed-disk',\n",
       "     'topology_gke_io_zone': 'us-central1-c',\n",
       "     'topology_kubernetes_io_region': 'us-central1',\n",
       "     'topology_kubernetes_io_zone': 'us-central1-c',\n",
       "     'wallaroo_ai_node_purpose': 'persistent'},\n",
       "    'value': [1764866063.494, '210779168768']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"plateau-managed-disk\"}'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Artifact Storage used\n",
    "\n",
    "* Total Artifact Storage used\n",
    "* query\n",
    "* `kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"minio\"}`\n",
    "* Amount of model and orchestration artifact storage used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'__name__': 'kubelet_volume_stats_capacity_bytes',\n",
       "     'beta_kubernetes_io_arch': 'amd64',\n",
       "     'beta_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'beta_kubernetes_io_os': 'linux',\n",
       "     'cloud_google_com_gke_boot_disk': 'pd-balanced',\n",
       "     'cloud_google_com_gke_container_runtime': 'containerd',\n",
       "     'cloud_google_com_gke_cpu_scaling_level': '8',\n",
       "     'cloud_google_com_gke_logging_variant': 'DEFAULT',\n",
       "     'cloud_google_com_gke_max_pods_per_node': '110',\n",
       "     'cloud_google_com_gke_memory_gb_scaling_level': '32',\n",
       "     'cloud_google_com_gke_nodepool': 'persistent',\n",
       "     'cloud_google_com_gke_os_distribution': 'cos',\n",
       "     'cloud_google_com_gke_provisioning': 'standard',\n",
       "     'cloud_google_com_gke_stack_type': 'IPV4',\n",
       "     'cloud_google_com_machine_family': 'e2',\n",
       "     'cloud_google_com_private_node': 'false',\n",
       "     'failure_domain_beta_kubernetes_io_region': 'us-central1',\n",
       "     'failure_domain_beta_kubernetes_io_zone': 'us-central1-c',\n",
       "     'instance': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'job': 'kubernetes-nodes',\n",
       "     'kubernetes_io_arch': 'amd64',\n",
       "     'kubernetes_io_hostname': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'kubernetes_io_os': 'linux',\n",
       "     'namespace': 'wallaroo',\n",
       "     'node_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'persistentvolumeclaim': 'minio',\n",
       "     'topology_gke_io_zone': 'us-central1-c',\n",
       "     'topology_kubernetes_io_region': 'us-central1',\n",
       "     'topology_kubernetes_io_zone': 'us-central1-c',\n",
       "     'wallaroo_ai_node_purpose': 'persistent'},\n",
       "    'value': [1764866063.689, '791522189312']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=\"minio\"}'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Artifact Storage allocated\n",
    "\n",
    "* Total Artifact Storage allocated\n",
    "* query\n",
    "* `kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"minio\"}`\n",
    "* Total amount of model and orchestration artifact storage available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'__name__': 'kubelet_volume_stats_used_bytes',\n",
       "     'beta_kubernetes_io_arch': 'amd64',\n",
       "     'beta_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'beta_kubernetes_io_os': 'linux',\n",
       "     'cloud_google_com_gke_boot_disk': 'pd-balanced',\n",
       "     'cloud_google_com_gke_container_runtime': 'containerd',\n",
       "     'cloud_google_com_gke_cpu_scaling_level': '8',\n",
       "     'cloud_google_com_gke_logging_variant': 'DEFAULT',\n",
       "     'cloud_google_com_gke_max_pods_per_node': '110',\n",
       "     'cloud_google_com_gke_memory_gb_scaling_level': '32',\n",
       "     'cloud_google_com_gke_nodepool': 'persistent',\n",
       "     'cloud_google_com_gke_os_distribution': 'cos',\n",
       "     'cloud_google_com_gke_provisioning': 'standard',\n",
       "     'cloud_google_com_gke_stack_type': 'IPV4',\n",
       "     'cloud_google_com_machine_family': 'e2',\n",
       "     'cloud_google_com_private_node': 'false',\n",
       "     'failure_domain_beta_kubernetes_io_region': 'us-central1',\n",
       "     'failure_domain_beta_kubernetes_io_zone': 'us-central1-c',\n",
       "     'instance': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'job': 'kubernetes-nodes',\n",
       "     'kubernetes_io_arch': 'amd64',\n",
       "     'kubernetes_io_hostname': 'gke-autoscale-uat-gcp-persistent-e78fe29f-osra',\n",
       "     'kubernetes_io_os': 'linux',\n",
       "     'namespace': 'wallaroo',\n",
       "     'node_kubernetes_io_instance_type': 'e2-standard-8',\n",
       "     'persistentvolumeclaim': 'minio',\n",
       "     'topology_gke_io_zone': 'us-central1-c',\n",
       "     'topology_kubernetes_io_region': 'us-central1',\n",
       "     'topology_kubernetes_io_zone': 'us-central1-c',\n",
       "     'wallaroo_ai_node_purpose': 'persistent'},\n",
       "    'value': [1764866063.926, '623442173952']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'kubelet_volume_stats_used_bytes{persistentvolumeclaim=\"minio\"}'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average GPU usage over time\n",
    "\n",
    "* Average GPU usage over time\n",
    "* Endpoint: `query`\n",
    "* `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})[1h:] offset 1h)`\n",
    "* Average GPU usage over the defined time range in the Wallaroo cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866064.146, '2']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})[1h:] offset 1h)'\n",
    "\n",
    "# this will also format the timezone in the parsing section\n",
    "timezone = \"US/Mountain\"\n",
    "selected_timezone = pytz.timezone(timezone)\n",
    "\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average GPU requested over time\n",
    "\n",
    "* Average GPU requested over time\n",
    "* query\n",
    "* `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})[1h:] offset 1h)`\n",
    "* Average number of GPU requested over the defined time range in the Wallaroo cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866064.405, '2']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "step = \"5m\" # the step of the calculation\n",
    "# this will also format the timezone in the parsing section\n",
    "timezone = \"US/Mountain\"\n",
    "selected_timezone = pytz.timezone(timezone)\n",
    "# Define the start and end times\n",
    "\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CPU usage over time\n",
    "\n",
    "* Average CPU usage over time\n",
    "* query\n",
    "* `avg_over_time(sum(wallaroo_kube_pod_resource_usage{resource=\"cpu\"})[1h:] offset 1h)`\n",
    "* Average CPU usage over the defined time range in the Wallaroo cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866064.635, '0.11641219185']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum(wallaroo_kube_pod_resource_usage{resource=\"cpu\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average CPU requested over time\n",
    "\n",
    "* Average CPU requested over time\n",
    "* query\n",
    "* `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[1h:] offset 1h)`\n",
    "* Average CPU requests over the defined time range in the Wallaroo cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866064.905, '7.306']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Average Memory usage over time\n",
    "\n",
    "* Average Memory usage over time\n",
    "* query\n",
    "* `avg_over_time(sum(wallaroo_kube_pod_resource_usage{resource=\"memory\"})[1h:] offset 1h)`\n",
    "* Average memory usage over the defined time range in the Wallaroo cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866065.138, '7.306']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Memory requests over time\n",
    "\n",
    "* Average Memory requests over time\n",
    "* query\n",
    "* `avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[1h:] offset 1h)`\n",
    "* Average memory requests over the defined time range in the Wallaroo cluster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {}, 'value': [1764866065.477, '23496491008']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pipelines CPU usage over time\n",
    "\n",
    "* Average pipelines CPU usage over time\n",
    "* query\n",
    "* `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_usage{resource=\"cpu\"})[1h:] offset 1h)`\n",
    "* Average CPU usage over the defined time range for an individual Wallaroo pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'namespace': 'wallaroo'},\n",
       "    'value': [1764866065.745, '0.08219504493472223']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-replicatest-53'},\n",
       "    'value': [1764866065.745, '0.008602407261111111']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-jcw-48'},\n",
       "    'value': [1764866065.745, '0.010516425854166667']},\n",
       "   {'metric': {'namespace': 'tinyllama-openai-414'},\n",
       "    'value': [1764866065.745, '0.015087655079166666']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_usage{resource=\"cpu\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pipelines CPU requested over time\n",
    "\n",
    "* Average pipelines CPU requested over time\n",
    "* query\n",
    "* `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[1h:] offset 1h)`\n",
    "* Average number of CPUs requested over the defined time range for an individual Wallaroo pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'namespace': 'wallaroo'},\n",
       "    'value': [1764866065.967, '1.256']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-replicatest-53'},\n",
       "    'value': [1764866065.967, '0.1']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-jcw-48'},\n",
       "    'value': [1764866065.967, '4.35']},\n",
       "   {'metric': {'namespace': 'tinyllama-openai-414'},\n",
       "    'value': [1764866065.967, '1.6']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=\"cpu\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pipelines GPU usage over time\n",
    "\n",
    "* Average pipelines GPU usage over time\n",
    "* query\n",
    "* `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu\\|qualcomm.com/qaic\"})[1h:] offset 1h)`\n",
    "* Average GPU usage over the defined time range for an individual Wallaroo pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'namespace': 'tinyllama-openai-414'},\n",
       "    'value': [1764866066.254, '1']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-jcw-48'},\n",
       "    'value': [1764866066.254, '1']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pipelines GPU requested over time\n",
    "\n",
    "* Average pipelines GPU requested over time\n",
    "* query\n",
    "* `avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})[1h:] offset 1h)`\n",
    "* Average number of GPUs requested over the defined time range for an individual Wallaroo pipeline. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'namespace': 'tinyllama-openai-414'},\n",
       "    'value': [1764866066.542, '1']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-jcw-48'},\n",
       "    'value': [1764866066.542, '1']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum by(namespace)(wallaroo_kube_pod_resource_requests{resource=~\"nvidia.com/gpu|qualcomm.com/qaic\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pipelines Mem usage over time\n",
    "\n",
    "* Average pipelines Mem usage over time\n",
    "* query\n",
    "* `avg_over_time(sum by(namespace) (wallaroo_kube_pod_resource_usage{resource=\"memory\"})[1h:] offset 1h)`\n",
    "* Average memory usage over the defined time range for an individual Wallaroo pipeline. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'namespace': 'wallaroo'},\n",
       "    'value': [1764866066.769, '4420674309.688889']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-jcw-48'},\n",
       "    'value': [1764866066.769, '921146823.1111112']},\n",
       "   {'metric': {'namespace': 'tinyllama-openai-414'},\n",
       "    'value': [1764866066.769, '7787871556.266666']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-replicatest-53'},\n",
       "    'value': [1764866066.769, '15813700.266666668']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum by(namespace) (wallaroo_kube_pod_resource_usage{resource=\"memory\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average pipelines Mem requested over time\n",
    "\n",
    "* Average pipelines Mem requested over time\n",
    "* query\n",
    "* `avg_over_time(sum by (namespace)(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[1h:] offset 1h)`\n",
    "* Average amount of memory requested over the defined time range for an individual Wallaroo pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'namespace': 'wallaroo'},\n",
       "    'value': [1764866067.012, '3766484992']},\n",
       "   {'metric': {'namespace': 'tinyllama-openai-414'},\n",
       "    'value': [1764866067.012, '9797894144']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-replicatest-53'},\n",
       "    'value': [1764866067.012, '134217728']},\n",
       "   {'metric': {'namespace': 'whisper-hf-byop-jcw-48'},\n",
       "    'value': [1764866067.012, '9797894144']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum by (namespace)(wallaroo_kube_pod_resource_requests{resource=\"memory\"})[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline inference log storage\n",
    "\n",
    "* Pipeline inference log storage\n",
    "* query\n",
    "* `avg_over_time(sum by(topic) (topic_bytes)[1h:] offset 1h)`\n",
    "* Average inference log storage used over the defined time range for an individual Wallaroo pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Response:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'data': {'resultType': 'vector',\n",
       "  'result': [{'metric': {'topic': 'workspace-1526-pipeline-ma-consumptionchanges-stage-inference'},\n",
       "    'value': [1764866067.27, '1148670']},\n",
       "   {'metric': {'topic': 'workspace-71-pipeline-assay-demonstration-tutorial-jcw-inference'},\n",
       "    'value': [1764866067.27, '60325542']},\n",
       "   {'metric': {'topic': 'workspace-1529-pipeline-rum-assay-nan-jcw-inference'},\n",
       "    'value': [1764866067.27, '2041174']},\n",
       "   {'metric': {'topic': 'workspace-1444-pipeline-dlrm-click-prediction-inference'},\n",
       "    'value': [1764866067.27, '4282479']},\n",
       "   {'metric': {'topic': 'workspace-42-pipeline-retail-inv-tracker-edge-obs-inference'},\n",
       "    'value': [1764866067.27, '3500604']},\n",
       "   {'metric': {'topic': 'workspace-86-pipeline-house-price-predictor-drift-inference'},\n",
       "    'value': [1764866067.27, '77898428']},\n",
       "   {'metric': {'topic': 'workspace-1786-pipeline-ai-screening-inference'},\n",
       "    'value': [1764866067.27, '2817923']}]}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this is the URL to get prometheus metrics\n",
    "query_url = f\"{wl.api_endpoint}/v1/metrics/api/v1/query\"\n",
    "\n",
    "# Retrieve the token \n",
    "headers = wl.auth.auth_header()\n",
    "\n",
    "# Get the total size of all the pipeline files by pipeline AND pipeline_version\n",
    "query = 'avg_over_time(sum by(topic) (topic_bytes)[1h:] offset 1h)'\n",
    "\n",
    "#request parameters\n",
    "params_rps = {\n",
    "    'query': query,\n",
    "}\n",
    "\n",
    "response = requests.get(query_url, headers=headers, params=params_rps)\n",
    "\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Query Response:\")\n",
    "    display(response.json())\n",
    "else:\n",
    "    print(\"Failed to fetch query response:\", response.status_code, response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wallaroosdk2025.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
