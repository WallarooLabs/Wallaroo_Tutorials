{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4e1f9428-6405-4c6f-a26c-747f30522825","showTitle":false,"title":""}},"source":["This tutorial and the assets can be downloaded as part of the [Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/development/sdk-install-guides/databricks-azure-sdk-install).\n","\n","## Installing the Wallaroo SDK into Databricks Azure Workspace\n","\n","Organizations that use Databricks Azure for model training and development can deploy models to Wallaroo through the [Wallaroo SDK](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/).  The following guide is created to assist users with installing the Wallaroo SDK, setting up authentication through Databricks Azure, and making a standard connection to a Wallaroo instance through Databricks Azure Workspace.\n","\n","These instructions are based on the on the [Wallaroo SSO for Microsoft Azure](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-sso-authentication/wallaroo-sso-azure/) and the [Connect to Wallaroo](https://docs.wallaroo.ai/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-essentials-guide/) guides.\n","\n","This tutorial provides the following:\n","\n","* `aloha-cnn-lstm.zip`: A pre-trained open source model that uses an [Aloha CNN LSTM model](https://www.researchgate.net/publication/348920204_Using_Auxiliary_Inputs_in_Deep_Learning_Models_for_Detecting_DGA-based_Domain_Names) for classifying Domain names as being either legitimate or being used for nefarious purposes such as malware distribution.\n","* `data-1.json`, `data-1k.json`, `data-25k.json`: Data files with 1, 1,000, and 25,000 records for testing.\n","\n","To use the Wallaroo SDK within Databricks Azure Workspace, a virtual environment will be used.  This will set the necessary libraries and specific Python version required.\n","\n","## Prerequisites\n","\n","The following is required for this tutorial:\n","\n","* A Wallaroo instance version 2022.4 or later.\n","* An Azure Databricks workspace with a cluster\n","\n","## General Steps\n","\n","For our example, we will perform the following:\n","\n","* Wallaroo SDK Install\n","  * Install the Wallaroo SDK into the Databricks Azure cluster.\n","  * Install the Wallaroo Python SDK.\n","  * Connect to a remote Wallaroo instance.  This instance is configured to use the standard Keycloak service.\n","* Wallaroo SDK from Databricks Azure Workspace (Optional)\n","  * The following steps are used to demonstrate using the Wallaroo SDK in an Databricks Azure Workspace environment.  The entire tutorial can be found on the [Wallaroo Tutorials repository]([Wallaroo Tutorials repository](https://github.com/WallarooLabs/Wallaroo_Tutorials/tree/main/sdk-install-guides/azure-ml-sdk-install)).\n","    * Create a workspace for our work.\n","    * Upload the CCFraud model.\n","    * Create a pipeline that can ingest our submitted data, submit it to the model, and export the results\n","    * Run a sample inference through our pipeline by loading a file\n","    * Undeploy the pipeline and return resources back to the Wallaroo instance's Kubernetes environment."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"81ad1d51-2f30-401b-8ff2-8b5fafac6ec2","showTitle":false,"title":""}},"source":["## Install Wallaroo SDK\n","\n","### Add Wallaroo SDK to Cluster\n","\n","To install the Wallaroo SDK in a Databricks Azure environment:\n","\n","1. From the Databricks Azure dashboard, select **Computer**, then the cluster to use.\n","1. Select **Libraries**.\n","1. Select **Install new**.\n","1. Select **PyPI**.  In the **Package** field, enter the current version of the [Wallaroo SDK](https://pypi.org/project/wallaroo/).  It is recommended to specify the version, which as of this writing is `wallaroo==2022.4.0`.\n","1. Select **Install**.\n","\n","Once the **Status** shows **Installed**, it will be available in Azure Databricks notebooks and other tools that use the cluster."]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"e650ae9f-36c9-4a11-8567-f7789751e026","showTitle":false,"title":""}},"source":["Once the Wallaroo SDK is installed, it can be used in a new or imported workbook.\n","\n","To use a new Notebook:\n","\n","1. From the left navigation panel, select **+ New**.\n","1. Select **Notebook**.\n","\n","To upload an existing notebook:\n","\n","1. From the left navigation panel, select **Workspace**, the workspace to use, then the dropdown icon and select **Import**.\n","1. Select the Jupyter Notebook to import.\n","\n","## Sample Wallaroo Connection\n","\n","With the Wallaroo Python SDK installed, remote commands and inferences can be performed through the following steps.\n","\n","### Open a Connection to Wallaroo\n","\n","The first step is to connect to Wallaroo through the Wallaroo client.\n","\n","This is accomplished using the `wallaroo.Client(api_endpoint, auth_endpoint, auth_type command)` command that connects to the Wallaroo instance services.  For more information on the DNS names of Wallaroo services, see the [DNS Integration Guide](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-dns-guide/).\n","\n","The `Client` method takes the following parameters:\n","\n","* **api_endpoint** (*String*): The URL to the Wallaroo instance API service.\n","* **auth_endpoint** (*String*): The URL to the Wallaroo instance Keycloak service.\n","* **auth_type command** (*String*): The authorization type.  In this case, `SSO`.\n","\n","Once run, the `wallaroo.Client` command provides a URL to grant the SDK permission to your specific Wallaroo environment.  When displayed, enter the URL into a browser and confirm permissions.  Depending on the configuration of the Wallaroo instance, the user will either be presented with a login request to the Wallaroo instance or be authenticated through a broker such as Google, Github, etc.  To use the broker, select it from the list under the username/password login forms.  For more information on Wallaroo authentication configurations, see the [Wallaroo Authentication Configuration Guides](https://docs.wallaroo.ai/wallaroo-operations-guide/wallaroo-configuration/wallaroo-sso-authentication/).\n","\n","![Wallaroo Login](./images/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-install-guides/databricks-azure-sdk-guide/azure-initial-login.png)\n","\n","Once authenticated, the user will verify adding the device the user is establishing the connection from.  Once both steps are complete, then the connection is granted.\n","\n","![Device Registration](./images/wallaroo-developer-guides/wallaroo-sdk-guides/wallaroo-sdk-install-guides/wallaroo-device-access.png)\n","\n","The connection is stored in the variable `wl` for use in all other Wallaroo calls."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"05b165e9-14b3-4ee2-a0e4-cc2263bb0355","showTitle":false,"title":""}},"outputs":[],"source":["import wallaroo\n","from wallaroo.object import EntityNotFoundError"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"bd223ab8-524a-4207-a48a-71949c3c2be9","showTitle":false,"title":""}},"outputs":[],"source":["# SSO login through keycloak\n","\n","wallarooPrefix = \"YOUR PREFIX\"\n","wallarooSuffix = \"YOUR SUFFIX\"\n","\n","wl = wallaroo.Client(api_endpoint=f\"https://{wallarooPrefix}.api.{wallarooSuffix}\", \n","                    auth_endpoint=f\"https://{wallarooPrefix}.keycloak.{wallarooSuffix}\", \n","                    auth_type=\"sso\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"eeb03ff3-8bb6-47ab-8b61-3e3b4a16bdd1","showTitle":false,"title":""}},"source":["### Create the Workspace\n","\n","We will create a workspace to work in and call it the `databricksazuresdkworkspace`, then set it as current workspace environment.  We'll also create our pipeline in advance as `azuremlsdkpipeline`.\n","\n","* **IMPORTANT NOTE**:  For this example, the Aloha model is stored in the file `alohacnnlstm.zip`.  When using tensor based models, the zip file **must** match the name of the tensor directory.  For example, if the tensor directory is `alohacnnlstm`, then the .zip file must be named `alohacnnlstm.zip`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"cc697145-548d-45ea-9ea5-bd838d32d4ec","showTitle":false,"title":""}},"outputs":[],"source":["workspace_name = 'databricksazuresdkworkspace'\n","pipeline_name = 'databricksazuresdkpipeline'\n","model_name = 'alohamodel'\n","model_file_name = '/dbfs/FileStore/YOUR PATH/alohacnnlstm.zip'"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ae994f7f-47e1-4133-a0b2-61c35781c6e3","showTitle":false,"title":""}},"outputs":[],"source":["def get_workspace(name):\n","    workspace = None\n","    for ws in wl.list_workspaces():\n","        if ws.name() == name:\n","            workspace= ws\n","    if(workspace == None):\n","        workspace = wl.create_workspace(name)\n","    return workspace\n","\n","def get_pipeline(name):\n","    try:\n","        pipeline = wl.pipelines_by_name(pipeline_name)[0]\n","    except EntityNotFoundError:\n","        pipeline = wl.build_pipeline(pipeline_name)\n","    return pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a7134ac0-f770-48e6-926e-3ed0cae2efbc","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td></td></tr></table>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td></td></tr></table>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["workspace = get_workspace(workspace_name)\n","\n","wl.set_current_workspace(workspace)\n","\n","pipeline = get_pipeline(pipeline_name)\n","pipeline"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"f50ab5c4-9524-4b20-a75e-85e2990eb744","showTitle":false,"title":""}},"source":["We can verify the workspace is created the current default workspace with the `get_current_workspace()` command."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5b54aa8e-b864-44bc-94fe-4da8232d7e5e","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[6]: {'name': 'databricksazuresdkworkspace', 'id': 20, 'archived': False, 'created_by': '59e7c5e0-64da-424f-8b5e-de53348f3347', 'created_at': '2023-01-13T17:28:30.374032+00:00', 'models': [], 'pipelines': [{'name': 'databricksazuresdkpipeline', 'create_time': datetime.datetime(2023, 1, 13, 17, 28, 30, 723301, tzinfo=tzutc()), 'definition': '[]'}]}"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[6]: {'name': 'databricksazuresdkworkspace', 'id': 20, 'archived': False, 'created_by': '59e7c5e0-64da-424f-8b5e-de53348f3347', 'created_at': '2023-01-13T17:28:30.374032+00:00', 'models': [], 'pipelines': [{'name': 'databricksazuresdkpipeline', 'create_time': datetime.datetime(2023, 1, 13, 17, 28, 30, 723301, tzinfo=tzutc()), 'definition': '[]'}]}","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["wl.get_current_workspace()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7653bb43-c455-4ec5-812a-9fcf29e48d53","showTitle":false,"title":""}},"source":["### Upload the Models\n","\n","Now we will upload our model.\n","\n","**IMPORTANT NOTE**:  Use the local file path format such as `/dbfs/FileStore/shared_uploads/YOURWORKSPACE/file` format rather than the `dbfs:` format."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"19e3432d-f23b-4fc8-aff1-57bb7a8943d2","showTitle":false,"title":""}},"outputs":[],"source":["#model = wl.upload_model(model_name, model_file_name).configure()\n","model = wl.upload_model(model_name, model_file_name).configure(\"tensorflow\")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"b3f85c27-c930-4ca6-839f-688d0d3b6c4b","showTitle":false,"title":""}},"source":["### Deploy a Model\n","\n","Now that we have a model that we want to use we will create a deployment for it. \n","\n","To do this, we'll create our pipeline that can ingest the data, pass the data to our CCFraud model, and give us a final output.  We'll call our pipeline `databricksazuresdkpipeline`, then deploy it so it's ready to receive data.  The deployment process usually takes about 45 seconds."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"7c833e81-ecf6-46a8-a0f2-e5eaf995e00f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td></td></tr></table>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>deployed</th> <td>(none)</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td></td></tr></table>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["pipeline.add_model_step(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"a559306c-ed8b-4b62-8995-e8dbcf014fae","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:35:01.706224+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>dd9801b9-aeb1-4801-beb8-f0b993e30b81, 6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td>alohamodel</td></tr></table>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:35:01.706224+00:00</td></tr><tr><th>deployed</th> <td>True</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>dd9801b9-aeb1-4801-beb8-f0b993e30b81, 6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td>alohamodel</td></tr></table>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["pipeline.deploy()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4ce11e36-42fd-419e-9090-f5d8ec62639d","showTitle":false,"title":""}},"source":["We can verify that the pipeline is running and list what models are associated with it."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"69b6d055-1454-4f50-aa1c-feb9f7539c85","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[16]: {'status': 'Running',\n"," 'details': [],\n"," 'engines': [{'ip': '10.244.2.58',\n","   'name': 'engine-76ff897498-87pnm',\n","   'status': 'Running',\n","   'reason': None,\n","   'details': [],\n","   'pipeline_statuses': {'pipelines': [{'id': 'databricksazuresdkpipeline',\n","      'status': 'Running'}]},\n","   'model_statuses': {'models': [{'name': 'alohamodel',\n","      'version': '025a778b-7718-4c1f-9ec6-d51135ce8896',\n","      'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8',\n","      'status': 'Running'}]}}],\n"," 'engine_lbs': [{'ip': '10.244.3.11',\n","   'name': 'engine-lb-55dcdff64c-mf6c4',\n","   'status': 'Running',\n","   'reason': None,\n","   'details': []}],\n"," 'sidekicks': []}"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[16]: {'status': 'Running',\n 'details': [],\n 'engines': [{'ip': '10.244.2.58',\n   'name': 'engine-76ff897498-87pnm',\n   'status': 'Running',\n   'reason': None,\n   'details': [],\n   'pipeline_statuses': {'pipelines': [{'id': 'databricksazuresdkpipeline',\n      'status': 'Running'}]},\n   'model_statuses': {'models': [{'name': 'alohamodel',\n      'version': '025a778b-7718-4c1f-9ec6-d51135ce8896',\n      'sha': 'd71d9ffc61aaac58c2b1ed70a2db13d1416fb9d3f5b891e5e4e2e97180fe22f8',\n      'status': 'Running'}]}}],\n 'engine_lbs': [{'ip': '10.244.3.11',\n   'name': 'engine-lb-55dcdff64c-mf6c4',\n   'status': 'Running',\n   'reason': None,\n   'details': []}],\n 'sidekicks': []}","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["pipeline.status()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"649a9776-7b5a-4c54-86ab-38c64243d0ce","showTitle":false,"title":""}},"source":["## Interferences\n","\n","### Infer 1 row\n","\n","Now that the pipeline is deployed and our CCfraud model is in place, we'll perform a smoke test to verify the pipeline is up and running properly.  We'll use the `infer_from_file` command to load a single transaction and determine if it is flagged for fraud.  If it returns correctly, a small valud should be returned indicating a low likelihood that the transaction was fraudulent."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"5de8a006-02a4-4c1b-a4ed-1f77e4a127e5","showTitle":false,"title":""}},"outputs":[],"source":["result = pipeline.infer_from_file(\"/dbfs/FileStore/YOUR PATH/data_1.json\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"d35cf9fd-bbcb-4b6d-b275-e7122bb3f1f5","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["Out[18]: [array([[0.00151959]]),\n"," array([[0.98291481]]),\n"," array([[0.01209957]]),\n"," array([[4.75912966e-05]]),\n"," array([[2.02893716e-05]]),\n"," array([[0.00031977]]),\n"," array([[0.01102928]]),\n"," array([[0.99756402]]),\n"," array([[0.01034162]]),\n"," array([[0.00803896]]),\n"," array([[0.01615506]]),\n"," array([[0.00623623]]),\n"," array([[0.00099858]]),\n"," array([[1.79337805e-26]]),\n"," array([[1.38899512e-27]])]"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"Out[18]: [array([[0.00151959]]),\n array([[0.98291481]]),\n array([[0.01209957]]),\n array([[4.75912966e-05]]),\n array([[2.02893716e-05]]),\n array([[0.00031977]]),\n array([[0.01102928]]),\n array([[0.99756402]]),\n array([[0.01034162]]),\n array([[0.00803896]]),\n array([[0.01615506]]),\n array([[0.00623623]]),\n array([[0.00099858]]),\n array([[1.79337805e-26]]),\n array([[1.38899512e-27]])]","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"ansi"}},"output_type":"display_data"}],"source":["result[0].data()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"0f35e2de-6d3b-4391-99a8-db51716a8fab","showTitle":false,"title":""}},"source":["### Batch Inference\n","\n","Now that our smoke test is successful, let's really give it some data.  We'll use the `cc_data_1k.json` file that contains 1,000 inferences to be performed."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"861a3ef3-c662-491c-a977-ca10709784a3","showTitle":false,"title":""}},"outputs":[],"source":["result = pipeline.infer_from_file(\"/dbfs/FileStore/YOUR PATH/data_1k.json\")\n","result"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"ce6232e6-13c0-4ff2-a2c5-be8f2dbefcce","showTitle":false,"title":""}},"source":["## Undeploy Pipeline\n","\n","When finished with our tests, we will undeploy the pipeline so we have the Kubernetes resources back for other tasks.  Note that if the deployment variable is unchanged pipeline.deploy() will restart the inference engine in the same configuration as before."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"29260771-cb42-4dc4-866f-180183bb2f3c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:35:01.706224+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>dd9801b9-aeb1-4801-beb8-f0b993e30b81, 6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td>alohamodel</td></tr></table>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<table><tr><th>name</th> <td>databricksazuresdkpipeline</td></tr><tr><th>created</th> <td>2023-01-13 17:28:30.723301+00:00</td></tr><tr><th>last_updated</th> <td>2023-01-13 17:35:01.706224+00:00</td></tr><tr><th>deployed</th> <td>False</td></tr><tr><th>tags</th> <td></td></tr><tr><th>versions</th> <td>dd9801b9-aeb1-4801-beb8-f0b993e30b81, 6042c356-5b6c-42ea-8024-b4bc313b101d</td></tr><tr><th>steps</th> <td>alohamodel</td></tr></table>","datasetInfos":[],"metadata":{},"removedWidgets":[],"textData":null,"type":"htmlSandbox"}},"output_type":"display_data"}],"source":["pipeline.undeploy()"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"c904c4a3-b7c3-48f9-9572-3bd2d602f59d","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"install-wallaroo-sdk-databricks-azure-guide","notebookOrigID":3942848317515628,"widgets":{}},"environment":{"kernel":"wallaroosdk","name":"common-cpu.m100","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/base-cpu:m100"},"kernelspec":{"display_name":"wallaroosdk","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15 (default, Nov 24 2022, 08:57:44) \n[Clang 14.0.6 ]"},"vscode":{"interpreter":{"hash":"e951b583e3fbbcadfc15df61f9ed97fb9a76f59b2130f2cdf4d0033aa05e7e64"}}},"nbformat":4,"nbformat_minor":0}
